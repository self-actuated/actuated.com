<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="author" content="OpenFaaS Ltd"/><meta name="twitter:card" content="summary_large_image"/><title>Run AI models with ollama in CI with GitHub Actions</title><meta name="description" content="With the new GPU support for actuated, we&#x27;ve been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards."/><meta property="twitter:title" content="Run AI models with ollama in CI with GitHub Actions"/><meta property="twitter:description" content="With the new GPU support for actuated, we&#x27;ve been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards."/><meta property="og:title" content="Run AI models with ollama in CI with GitHub Actions"/><meta property="og:description" content="With the new GPU support for actuated, we&#x27;ve been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards."/><meta name="twitter:image:src" content="https://actuated.com/images/2024-04-ollama-in-ci/background.png"/><meta property="og:image" content="https://actuated.com/images/2024-04-ollama-in-ci/background.png"/><meta name="next-head-count" content="13"/><meta charSet="utf-8"/><link rel="icon" type="image/png" href="/images/actuated.png"/><link rel="stylesheet" href="https://rsms.me/inter/inter.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css"/><link rel="manifest" href="/manifest.json"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M5YNDNX7VT"></script><script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-M5YNDNX7VT');
          </script><link rel="apple-touch-icon" href="/images/actuated.png"/><link rel="preload" href="/_next/static/css/931f0fc3815fa31d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/931f0fc3815fa31d.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-114634acb84f8baa.js" defer=""></script><script src="/_next/static/chunks/main-34e2100d07ae5757.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a058602dc4f1b3be.js" defer=""></script><script src="/_next/static/chunks/552-542f150c917a7625.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-386521db085c1ba3.js" defer=""></script><script src="/_next/static/qP6XrePfh_ktdNhbSnok_/_buildManifest.js" defer=""></script><script src="/_next/static/qP6XrePfh_ktdNhbSnok_/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="bg-white"><header><div class="relative bg-white" data-headlessui-state=""><div class="mx-auto flex max-w-7xl items-center justify-between px-4 py-6 sm:px-6 md:justify-start md:space-x-10 lg:px-8"><div class="flex justify-start lg:w-0 lg:flex-1"><a href="/"><span class="sr-only">Actuated</span><img class="h-8 w-auto sm:h-10" src="/images/actuated.png" alt="Actuated logo"/></a></div><div class="-my-2 -mr-2 md:hidden"><button class="inline-flex items-center justify-center rounded-md bg-white p-2 text-gray-400 hover:bg-gray-100 hover:text-gray-500 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-indigo-500" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open menu</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><nav class="hidden space-x-10 md:flex"><div class="relative" data-headlessui-state=""><button class="text-gray-500 group inline-flex items-center rounded-md bg-white text-base font-medium hover:text-gray-900 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2" type="button" aria-expanded="false" data-headlessui-state=""><span>Solutions</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="text-gray-400 ml-2 h-5 w-5 group-hover:text-gray-500"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z" clip-rule="evenodd"></path></svg></button></div><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="/blog">Blog</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="/blog/blazing-fast-ci-with-microvms">Announcement</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="/pricing">Pricing</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="https://docs.actuated.dev/">Docs</a></nav><div class="hidden items-center justify-end md:flex md:flex-1 lg:w-0"><a href="https://dashboard.actuated.dev" class="whitespace-nowrap text-base font-medium text-gray-500 hover:text-gray-900">Sign in</a><a href="/pricing" class="ml-8 inline-flex items-center justify-center whitespace-nowrap rounded-md border border-transparent bg-indigo-600 px-4 py-2 text-base font-medium text-white shadow-sm hover:bg-indigo-700">Sign-up</a></div></div></div></header><main><div class="container mx-auto max-w-4xl bg-white mt-4 px-4 sm:px-6"><h1 id="post_title" class="text-3xl mb-3 leading-8 font-extrabold tracking-tight text-gray-900 sm:text-4xl sm:leading-10 text-center">Run AI models with ollama in CI with GitHub Actions</h1></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-4xl"><div class="border-b border-gray-200 py-4 flex items-center text-gray-500 mx-auto"><div><img class="h-10 w-10 rounded-full" src="/images/alex.jpg" alt=""/></div><div class="ml-3"><p id="post_author" class="text-sm leading-5 font-medium text-gray-900"></p><div class="flex text-sm leading-5 text-gray-500"><time dateTime="2024-04-25" id="post_date">April 25, 2024</time><span class="mx-1"></span></div></div></div><div class="mt-6 prose sm:prose-lg max-w-none"><p id="post_description" class="mb-4">With the new GPU support for actuated, we&#x27;ve been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards.</p></div><div id="post_content" class="mt-6 prose sm:prose-lg max-w-none"><p>That means you can run real end to end tests in CI with the same models you may use in dev and production. And if you use OpenAI or AWS SageMaker extensively, you could perhaps swap out what can be a very expensive API endpoint for your CI or testing environments to save money.</p>
<p>If you'd like to learn more about how and why you'd want access to GPUs in CI, read my past update: <a href="/blog/gpus-for-github-actions">Accelerate GitHub Actions with dedicated GPUs</a>.</p>
<p>We'll first cover what ollama is, why it's so popular, how to get it, what kinds of fun things you can do with it, then how to access it from actuated using a real GPU.</p>
<p><img src="/images/2024-04-ollama-in-ci/logos.png" alt="ollama can run in CI with isolated GPU acceleration using actuated"></p>
<blockquote>
<p>ollama can now run in CI with isolated GPU acceleration using actuated</p>
</blockquote>
<h2>What's ollama?</h2>
<p><a href="https://ollama.com/">ollama</a> is an open source project that aims to do for AI models, what Docker did for Linux containers. Whilst Docker created a user experience to share and run containers using container images in the Open Container Initiative (OCI) format, ollama bundles well-known AI models and makes it easy to run them without having to think about Python versions or Nvidia CUDA libraries.</p>
<p>The project packages and runs various models, but seems to take its name from Meta's popular <a href="https://llama.meta.com/">llama2 model</a>, which whilst <a href="https://llama.meta.com/faq">not released under an open source license</a>, allows for a generous amount of free usage for most types of users.</p>
<p>The ollama project can be run directly on a Linux, MacOS or Windows host, or within a container. There's a server component, and a CLI that acts as a client to pre-trained models. The main use-case today is that of inference - exercising the model with input data. A more recent feature means that you can create embeddings, if you pull a model that supports them.</p>
<p>On Linux, ollama can be installed using a utility script:</p>
<pre><code class="hljs language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<p>This provides the <code>ollama</code> CLI command.</p>
<h2>A quick tour of ollama</h2>
<p>After the initial installation, you can start a server:</p>
<pre><code class="hljs language-bash">ollama serve
</code></pre>
<p>By default, its REST API will listen on port <code>11434</code> on 127.0.0.1.</p>
<p>You can find the reference for ollama's REST API here: <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">API endpoints</a> - which includes things like: creating a chat completion, pulling a model, or generating embeddings.</p>
<p>You can then browse <a href="https://ollama.com/library">available models on the official website</a>, which resembles the Docker Hub. This set currently includes: gemma (built upon Google's DeepMind), mistral (an LLM), codellama (for generating Code), phi (from Microsoft research), vicuna (for chat, based upon llama2), llava (a vision encoder), and many more.</p>
<p>Most models will download with a default parameter size that's small enough to run on most CPUs or GPUs, but if you need to access it, there are larger models for higher accuracy.</p>
<p>For instance, the <a href="https://ollama.com/library/llama2">llama2</a> model by Meta will default to the 7b model which needs around 8GB of RAM.</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Pull the default model size:</span>

ollama pull llama2

<span class="hljs-comment"># Override the parameter size</span>

ollama pull llama2:13b
</code></pre>
<p>Once you have a model, you can then either "run" it, where you'll be able to ask it questions and interact with it like you would with ChatGPT, or you can send it API requests from your own applications using REST and HTTP.</p>
<p>For an interactive prompt, give no parameters:</p>
<pre><code class="hljs language-bash">ollama run llama2
</code></pre>
<p>To get an immediate response for use in i.e. scripts:</p>
<pre><code class="hljs language-bash">ollama run llama2 <span class="hljs-string">"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?"</span>
</code></pre>
<p>And you can use the REST API via <code>curl</code>, or your own codebase:</p>
<pre><code class="hljs language-bash">curl -s http://localhost:11434/api/generate -d <span class="hljs-string">'{
    "model": "llama2",
    "stream": false,
    "prompt":"What are the risks of running privileged Docker containers for CI workloads?"
}'</span> | jq
</code></pre>
<p>We are just scratching the surface with what ollama can do, with a focus on testing and pulling pre-built models, but you can also create and share models using a <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfile</a>, which is another homage to the Docker experience by the ollama developers.</p>
<h3>Access ollama from Python code</h3>
<p>Here's how to access the API via Python, the <code>stream</code> parameter will emit JSON progressively when set to True, block until done if set to False. With Node.js, Python, Java, C#, etc the code will be very similar, but using your own preferred HTTP client. For Golang (Go) users, ollama founder <a href="https://twitter.com/jmorgan">Jeffrey Morgan</a> maintains a <a href="https://pkg.go.dev/github.com/jmorganca/ollama/api">higher-level Go SDK</a>.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> json

url = <span class="hljs-string">"http://localhost:11434/api/generate"</span>
payload = {
    <span class="hljs-string">"model"</span>: <span class="hljs-string">"llama2"</span>,
    <span class="hljs-string">"stream"</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">"prompt"</span>: <span class="hljs-string">"What are the risks of running privileged Docker containers for CI workloads?"</span>
}
headers = {
    <span class="hljs-string">"Content-Type"</span>: <span class="hljs-string">"application/json"</span>
}

response = requests.post(url, data=json.dumps(payload), headers=headers)

<span class="hljs-comment"># Parse the JSON response</span>
response_json = response.json()

<span class="hljs-comment"># Pretty print the JSON response</span>
<span class="hljs-built_in">print</span>(json.dumps(response_json, indent=<span class="hljs-number">4</span>))
</code></pre>
<p>When you're constructing a request by API, make sure you include any tags in the <code>model</code> name, if you've used one. I.e. <code>"model": "llama2:13b"</code>.</p>
<p>I hear from so many organisations who have gone to lengths to get SOC2 compliance, doing CVE scanning, or who are running Open Policy Agent or Kyverno to enforce strict Pod admission policies in Kubernetes, but then are happy to run their CI in Pods in privileged mode. So I asked the model why that may not be a smart idea. You can run the sample for yourself or <a href="https://gist.githubusercontent.com/alexellis/4c85e5927d251153c41379c4cac1a6c8/raw/257a02df0e01dff966c68509baf299bc32d3a11e/security.txt">see the response here</a>. We also go into detail in the <a href="https://docs.actuated.dev/faq/">actuated FAQ</a>, the security situation around self-hosted runners and containers is the main reason we built the solution.</p>
<h3>Putting it together for a GitHub Action</h3>
<p>The following GitHub Action will run on for customers who are enrolled for GPU support for actuated. If you'd like to gain access, contact us via the form on the <a href="https://actuated.com/pricing">Pricing page</a>.</p>
<p>The <code>self-actuated/nvidia-run</code> installs either the consumer or datacenter driver for Nvidia, depending on what you have in your system. This only takes about 30 seconds and could be cached if you like. The ollama models could also be <a href="https://docs.actuated.dev/tasks/local-github-cache/">cached using a local S3 bucket</a>.</p>
<p>Then, we simply run the equivalent bash commands from the previous section to:</p>
<ul>
<li>Install ollama</li>
<li>Start serving the REST API</li>
<li>Pull the llama2 model from Meta</li>
<li>Run an inference via CLI</li>
<li>Run an inference via REST API using curl</li>
</ul>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">ollama-e2e</span>

<span class="hljs-attr">on:</span>
    <span class="hljs-attr">workflow_dispatch:</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">ollama-e2e:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">ollama-e2e</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/nvidia-run@master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">Ollama</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            curl -fsSL https://ollama.com/install.sh | sudo -E sh
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Start</span> <span class="hljs-string">serving</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              # Run the background, there is no way to daemonise at the moment
              ollama serve &#x26;
</span>
              <span class="hljs-comment"># A short pause is required before the HTTP port is opened</span>
              <span class="hljs-string">sleep</span> <span class="hljs-number">5</span>

              <span class="hljs-comment"># This endpoint blocks until ready</span>
              <span class="hljs-string">time</span> <span class="hljs-string">curl</span> <span class="hljs-string">-i</span> <span class="hljs-string">http://localhost:11434</span>

        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Pull</span> <span class="hljs-string">llama2</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              ollama pull llama2
</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Invoke</span> <span class="hljs-string">via</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              ollama run llama2 "What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?"
</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Invoke</span> <span class="hljs-string">via</span> <span class="hljs-string">API</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            curl -s http://localhost:11434/api/generate -d '{
              "model": "llama2",
              "stream": false,
              "prompt":"What are the risks of running privileged Docker containers for CI workloads?"
            }' | jq
</span></code></pre>
<p>There is no built-in way to daemonise the ollama server, so for now we run it in the background using bash. The readiness endpoint can then be accessed which blocks until the server has completed its initialisation.</p>
<h3>Interactive access with SSH</h3>
<p>By modifying your CI job, you can drop into a remote SSH session and run interactive commands at any point in the workflow.</p>
<p>That's how I came up with the commands for the Nvidia driver installation, and for the various ollama commands I shared.</p>
<p>Find out more about SSH for GitHub Actions <a href="https://docs.actuated.dev/tasks/debug-ssh/">in the actuated docs</a>.</p>
<p><img src="/images/2024-04-ollama-in-ci/ssh.png" alt="Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM"></p>
<blockquote>
<p>Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM</p>
</blockquote>
<h2>Wrapping up</h2>
<p>Within a very short period of time ollama helped us pull a popular AI model that can be used for chat and completions. We were then able to take what we learned and run it on a GPU at an accelerated speed and accuracy by using actuated's <a href="/blog/gpus-for-github-actions">new GPU support for GitHub Actions and GitLab CI</a>. Most hosted CI systems provide a relatively small amount of disk space for jobs, with actuated you can customise this and that may be important if you're going to be downloading large AI models. You can also easily customise the amount of RAM and vCPU using the <code>runs-on</code> label to any combination you need.</p>
<p>ollama isn't the only way to find, download and run AI models, just like Docker wasn't the only way to download and install Nginx or Postgresql, but it provides a useful and convenient interface for those of us who are still learning about AI, and are not as concerned with the internal workings of the models.</p>
<p>Over on the OpenFaaS blog, in the tutorial <a href="https://www.openfaas.com/blog/openai-streaming-responses/">Stream OpenAI responses from functions using Server Sent Events</a>, we covered how to stream a response from a model to a function, and then back to a user. There, we used the <a href="https://github.com/c0sogi/llama-api">llama-api</a> open source project, which is a single-purpose HTTP API for simulating llama2.</p>
<p>One of the benefits of ollama is <a href="https://github.com/ollama/ollama/tree/main/examples">the detailed range of examples</a> in the docs, and the ability to run other models that may include computer vision such as with the <a href="https://llava-vl.github.io/">LLaVA: Large Language and Vision Assistant model</a> or generating code with <a href="https://codellama.dev/about">Code Llama</a>.</p>
<p>Right now, many of us are running and tuning models in development, some of us are using OpenAI's API or self-hosted models in production, but there's very little talk about doing thorough end to end testing or exercising models in CI. That's where actuated can help.</p>
<p>Feel free to <a href="/pricing">reach out for early access, or to see if we can help your team with your CI needs</a>.</p></div></div></main><footer class="bg-white"><div class="mx-auto max-w-7xl py-12 px-4 sm:px-6 md:flex md:items-center md:justify-between lg:px-8"><div class="flex justify-center space-x-6 md:order-2"><a class="text-gray-400 hover:text-gray-500" href="https://twitter.com/selfactuated"><span class="sr-only">Twitter</span><svg fill="currentColor" viewBox="0 0 24 24" class="h-6 w-6" aria-hidden="true"><path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84"></path></svg></a><a class="text-gray-400 hover:text-gray-500" href="https://github.com/self-actuated"><span class="sr-only">GitHub</span><svg fill="currentColor" viewBox="0 0 24 24" class="h-6 w-6" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path></svg></a><a class="text-gray-400 hover:text-gray-500" href="/rss.xml"><span class="sr-only">RSS</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M12.75 19.5v-.75a7.5 7.5 0 00-7.5-7.5H4.5m0-6.75h.75c7.87 0 14.25 6.38 14.25 14.25v.75M6 18.75a.75.75 0 11-1.5 0 .75.75 0 011.5 0z"></path></svg></a></div><div class="mt-8 md:order-1 md:mt-0"><p class="text-center text-base text-gray-400">Â© 2024 <a href="https://openfaas.com">OpenFaaS Ltd</a>. All rights reserved. <a class="hover:underline" href="/terms">Terms &amp; Conditions.</a></p></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"ollama-in-github-actions","fileName":"2024-03-25-ollama-in-github-actions.md","contentHtml":"\u003cp\u003eThat means you can run real end to end tests in CI with the same models you may use in dev and production. And if you use OpenAI or AWS SageMaker extensively, you could perhaps swap out what can be a very expensive API endpoint for your CI or testing environments to save money.\u003c/p\u003e\n\u003cp\u003eIf you'd like to learn more about how and why you'd want access to GPUs in CI, read my past update: \u003ca href=\"/blog/gpus-for-github-actions\"\u003eAccelerate GitHub Actions with dedicated GPUs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe'll first cover what ollama is, why it's so popular, how to get it, what kinds of fun things you can do with it, then how to access it from actuated using a real GPU.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2024-04-ollama-in-ci/logos.png\" alt=\"ollama can run in CI with isolated GPU acceleration using actuated\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eollama can now run in CI with isolated GPU acceleration using actuated\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eWhat's ollama?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/\"\u003eollama\u003c/a\u003e is an open source project that aims to do for AI models, what Docker did for Linux containers. Whilst Docker created a user experience to share and run containers using container images in the Open Container Initiative (OCI) format, ollama bundles well-known AI models and makes it easy to run them without having to think about Python versions or Nvidia CUDA libraries.\u003c/p\u003e\n\u003cp\u003eThe project packages and runs various models, but seems to take its name from Meta's popular \u003ca href=\"https://llama.meta.com/\"\u003ellama2 model\u003c/a\u003e, which whilst \u003ca href=\"https://llama.meta.com/faq\"\u003enot released under an open source license\u003c/a\u003e, allows for a generous amount of free usage for most types of users.\u003c/p\u003e\n\u003cp\u003eThe ollama project can be run directly on a Linux, MacOS or Windows host, or within a container. There's a server component, and a CLI that acts as a client to pre-trained models. The main use-case today is that of inference - exercising the model with input data. A more recent feature means that you can create embeddings, if you pull a model that supports them.\u003c/p\u003e\n\u003cp\u003eOn Linux, ollama can be installed using a utility script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003ecurl -fsSL https://ollama.com/install.sh | sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis provides the \u003ccode\u003eollama\u003c/code\u003e CLI command.\u003c/p\u003e\n\u003ch2\u003eA quick tour of ollama\u003c/h2\u003e\n\u003cp\u003eAfter the initial installation, you can start a server:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eollama serve\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, its REST API will listen on port \u003ccode\u003e11434\u003c/code\u003e on 127.0.0.1.\u003c/p\u003e\n\u003cp\u003eYou can find the reference for ollama's REST API here: \u003ca href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\"\u003eAPI endpoints\u003c/a\u003e - which includes things like: creating a chat completion, pulling a model, or generating embeddings.\u003c/p\u003e\n\u003cp\u003eYou can then browse \u003ca href=\"https://ollama.com/library\"\u003eavailable models on the official website\u003c/a\u003e, which resembles the Docker Hub. This set currently includes: gemma (built upon Google's DeepMind), mistral (an LLM), codellama (for generating Code), phi (from Microsoft research), vicuna (for chat, based upon llama2), llava (a vision encoder), and many more.\u003c/p\u003e\n\u003cp\u003eMost models will download with a default parameter size that's small enough to run on most CPUs or GPUs, but if you need to access it, there are larger models for higher accuracy.\u003c/p\u003e\n\u003cp\u003eFor instance, the \u003ca href=\"https://ollama.com/library/llama2\"\u003ellama2\u003c/a\u003e model by Meta will default to the 7b model which needs around 8GB of RAM.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Pull the default model size:\u003c/span\u003e\n\nollama pull llama2\n\n\u003cspan class=\"hljs-comment\"\u003e# Override the parameter size\u003c/span\u003e\n\nollama pull llama2:13b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you have a model, you can then either \"run\" it, where you'll be able to ask it questions and interact with it like you would with ChatGPT, or you can send it API requests from your own applications using REST and HTTP.\u003c/p\u003e\n\u003cp\u003eFor an interactive prompt, give no parameters:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eollama run llama2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo get an immediate response for use in i.e. scripts:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eollama run llama2 \u003cspan class=\"hljs-string\"\u003e\"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd you can use the REST API via \u003ccode\u003ecurl\u003c/code\u003e, or your own codebase:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003ecurl -s http://localhost:11434/api/generate -d \u003cspan class=\"hljs-string\"\u003e'{\n    \"model\": \"llama2\",\n    \"stream\": false,\n    \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n}'\u003c/span\u003e | jq\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe are just scratching the surface with what ollama can do, with a focus on testing and pulling pre-built models, but you can also create and share models using a \u003ca href=\"https://github.com/ollama/ollama/blob/main/docs/modelfile.md\"\u003eModelfile\u003c/a\u003e, which is another homage to the Docker experience by the ollama developers.\u003c/p\u003e\n\u003ch3\u003eAccess ollama from Python code\u003c/h3\u003e\n\u003cp\u003eHere's how to access the API via Python, the \u003ccode\u003estream\u003c/code\u003e parameter will emit JSON progressively when set to True, block until done if set to False. With Node.js, Python, Java, C#, etc the code will be very similar, but using your own preferred HTTP client. For Golang (Go) users, ollama founder \u003ca href=\"https://twitter.com/jmorgan\"\u003eJeffrey Morgan\u003c/a\u003e maintains a \u003ca href=\"https://pkg.go.dev/github.com/jmorganca/ollama/api\"\u003ehigher-level Go SDK\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\nurl = \u003cspan class=\"hljs-string\"\u003e\"http://localhost:11434/api/generate\"\u003c/span\u003e\npayload = {\n    \u003cspan class=\"hljs-string\"\u003e\"model\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama2\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e\"stream\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e\"prompt\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"What are the risks of running privileged Docker containers for CI workloads?\"\u003c/span\u003e\n}\nheaders = {\n    \u003cspan class=\"hljs-string\"\u003e\"Content-Type\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"application/json\"\u003c/span\u003e\n}\n\nresponse = requests.post(url, data=json.dumps(payload), headers=headers)\n\n\u003cspan class=\"hljs-comment\"\u003e# Parse the JSON response\u003c/span\u003e\nresponse_json = response.json()\n\n\u003cspan class=\"hljs-comment\"\u003e# Pretty print the JSON response\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(json.dumps(response_json, indent=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen you're constructing a request by API, make sure you include any tags in the \u003ccode\u003emodel\u003c/code\u003e name, if you've used one. I.e. \u003ccode\u003e\"model\": \"llama2:13b\"\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eI hear from so many organisations who have gone to lengths to get SOC2 compliance, doing CVE scanning, or who are running Open Policy Agent or Kyverno to enforce strict Pod admission policies in Kubernetes, but then are happy to run their CI in Pods in privileged mode. So I asked the model why that may not be a smart idea. You can run the sample for yourself or \u003ca href=\"https://gist.githubusercontent.com/alexellis/4c85e5927d251153c41379c4cac1a6c8/raw/257a02df0e01dff966c68509baf299bc32d3a11e/security.txt\"\u003esee the response here\u003c/a\u003e. We also go into detail in the \u003ca href=\"https://docs.actuated.dev/faq/\"\u003eactuated FAQ\u003c/a\u003e, the security situation around self-hosted runners and containers is the main reason we built the solution.\u003c/p\u003e\n\u003ch3\u003ePutting it together for a GitHub Action\u003c/h3\u003e\n\u003cp\u003eThe following GitHub Action will run on for customers who are enrolled for GPU support for actuated. If you'd like to gain access, contact us via the form on the \u003ca href=\"https://actuated.com/pricing\"\u003ePricing page\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eself-actuated/nvidia-run\u003c/code\u003e installs either the consumer or datacenter driver for Nvidia, depending on what you have in your system. This only takes about 30 seconds and could be cached if you like. The ollama models could also be \u003ca href=\"https://docs.actuated.dev/tasks/local-github-cache/\"\u003ecached using a local S3 bucket\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThen, we simply run the equivalent bash commands from the previous section to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstall ollama\u003c/li\u003e\n\u003cli\u003eStart serving the REST API\u003c/li\u003e\n\u003cli\u003ePull the llama2 model from Meta\u003c/li\u003e\n\u003cli\u003eRun an inference via CLI\u003c/li\u003e\n\u003cli\u003eRun an inference via REST API using curl\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-yaml\"\u003e\u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eollama-e2e\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eon:\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eworkflow_dispatch:\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003ejobs:\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eollama-e2e:\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eollama-e2e\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003eruns-on:\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003eactuated-8cpu-16gb\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003egpu\u003c/span\u003e]\n        \u003cspan class=\"hljs-attr\"\u003esteps:\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003euses:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eactions/checkout@v1\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003euses:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eself-actuated/nvidia-run@master\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eInstall\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eOllama\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n            curl -fsSL https://ollama.com/install.sh | sudo -E sh\n\u003c/span\u003e        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eStart\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eserving\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n              # Run the background, there is no way to daemonise at the moment\n              ollama serve \u0026#x26;\n\u003c/span\u003e\n              \u003cspan class=\"hljs-comment\"\u003e# A short pause is required before the HTTP port is opened\u003c/span\u003e\n              \u003cspan class=\"hljs-string\"\u003esleep\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e\n\n              \u003cspan class=\"hljs-comment\"\u003e# This endpoint blocks until ready\u003c/span\u003e\n              \u003cspan class=\"hljs-string\"\u003etime\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ecurl\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-i\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ehttp://localhost:11434\u003c/span\u003e\n\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ePull\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ellama2\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n              ollama pull llama2\n\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eInvoke\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003evia\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ethe\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eCLI\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n              ollama run llama2 \"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"\n\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eInvoke\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003evia\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eAPI\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n            curl -s http://localhost:11434/api/generate -d '{\n              \"model\": \"llama2\",\n              \"stream\": false,\n              \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n            }' | jq\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere is no built-in way to daemonise the ollama server, so for now we run it in the background using bash. The readiness endpoint can then be accessed which blocks until the server has completed its initialisation.\u003c/p\u003e\n\u003ch3\u003eInteractive access with SSH\u003c/h3\u003e\n\u003cp\u003eBy modifying your CI job, you can drop into a remote SSH session and run interactive commands at any point in the workflow.\u003c/p\u003e\n\u003cp\u003eThat's how I came up with the commands for the Nvidia driver installation, and for the various ollama commands I shared.\u003c/p\u003e\n\u003cp\u003eFind out more about SSH for GitHub Actions \u003ca href=\"https://docs.actuated.dev/tasks/debug-ssh/\"\u003ein the actuated docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2024-04-ollama-in-ci/ssh.png\" alt=\"Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eWithin a very short period of time ollama helped us pull a popular AI model that can be used for chat and completions. We were then able to take what we learned and run it on a GPU at an accelerated speed and accuracy by using actuated's \u003ca href=\"/blog/gpus-for-github-actions\"\u003enew GPU support for GitHub Actions and GitLab CI\u003c/a\u003e. Most hosted CI systems provide a relatively small amount of disk space for jobs, with actuated you can customise this and that may be important if you're going to be downloading large AI models. You can also easily customise the amount of RAM and vCPU using the \u003ccode\u003eruns-on\u003c/code\u003e label to any combination you need.\u003c/p\u003e\n\u003cp\u003eollama isn't the only way to find, download and run AI models, just like Docker wasn't the only way to download and install Nginx or Postgresql, but it provides a useful and convenient interface for those of us who are still learning about AI, and are not as concerned with the internal workings of the models.\u003c/p\u003e\n\u003cp\u003eOver on the OpenFaaS blog, in the tutorial \u003ca href=\"https://www.openfaas.com/blog/openai-streaming-responses/\"\u003eStream OpenAI responses from functions using Server Sent Events\u003c/a\u003e, we covered how to stream a response from a model to a function, and then back to a user. There, we used the \u003ca href=\"https://github.com/c0sogi/llama-api\"\u003ellama-api\u003c/a\u003e open source project, which is a single-purpose HTTP API for simulating llama2.\u003c/p\u003e\n\u003cp\u003eOne of the benefits of ollama is \u003ca href=\"https://github.com/ollama/ollama/tree/main/examples\"\u003ethe detailed range of examples\u003c/a\u003e in the docs, and the ability to run other models that may include computer vision such as with the \u003ca href=\"https://llava-vl.github.io/\"\u003eLLaVA: Large Language and Vision Assistant model\u003c/a\u003e or generating code with \u003ca href=\"https://codellama.dev/about\"\u003eCode Llama\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eRight now, many of us are running and tuning models in development, some of us are using OpenAI's API or self-hosted models in production, but there's very little talk about doing thorough end to end testing or exercising models in CI. That's where actuated can help.\u003c/p\u003e\n\u003cp\u003eFeel free to \u003ca href=\"/pricing\"\u003ereach out for early access, or to see if we can help your team with your CI needs\u003c/a\u003e.\u003c/p\u003e","title":"Run AI models with ollama in CI with GitHub Actions","description":"With the new GPU support for actuated, we've been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards.","tags":["ai","ollama","ml","localmodels","githubactions","openai","llama","machinelearning"],"author_img":"alex","image":"/images/2024-04-ollama-in-ci/background.png","date":"2024-04-25"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"ollama-in-github-actions"},"buildId":"qP6XrePfh_ktdNhbSnok_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>