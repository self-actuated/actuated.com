<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="author" content="OpenFaaS Ltd"/><meta name="twitter:card" content="summary_large_image"/><title>Accelerate GitHub Actions with dedicated GPUs</title><meta name="description" content="You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases."/><meta property="twitter:title" content="Accelerate GitHub Actions with dedicated GPUs"/><meta property="twitter:description" content="You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases."/><meta property="og:title" content="Accelerate GitHub Actions with dedicated GPUs"/><meta property="og:description" content="You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases."/><meta name="twitter:image:src" content="https://actuated.dev/images/2024-03-gpus/background.png"/><meta property="og:image" content="https://actuated.dev/images/2024-03-gpus/background.png"/><meta name="next-head-count" content="13"/><meta charSet="utf-8"/><link rel="icon" type="image/png" href="/images/actuated.png"/><link rel="stylesheet" href="https://rsms.me/inter/inter.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css"/><link rel="manifest" href="/manifest.json"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M5YNDNX7VT"></script><script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-M5YNDNX7VT');
          </script><link rel="apple-touch-icon" href="/images/actuated.png"/><link rel="preload" href="/_next/static/css/931f0fc3815fa31d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/931f0fc3815fa31d.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-114634acb84f8baa.js" defer=""></script><script src="/_next/static/chunks/main-34e2100d07ae5757.js" defer=""></script><script src="/_next/static/chunks/pages/_app-ba4f0f0b110ca3d9.js" defer=""></script><script src="/_next/static/chunks/552-542f150c917a7625.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-109b0d2eddac75b5.js" defer=""></script><script src="/_next/static/rbUDR1uvu6_LnDVo8sHCt/_buildManifest.js" defer=""></script><script src="/_next/static/rbUDR1uvu6_LnDVo8sHCt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="bg-white"><header><div class="relative bg-white" data-headlessui-state=""><div class="mx-auto flex max-w-7xl items-center justify-between px-4 py-6 sm:px-6 md:justify-start md:space-x-10 lg:px-8"><div class="flex justify-start lg:w-0 lg:flex-1"><a href="/"><span class="sr-only">Actuated</span><img class="h-8 w-auto sm:h-10" src="/images/actuated.png" alt="Actuated logo"/></a></div><div class="-my-2 -mr-2 md:hidden"><button class="inline-flex items-center justify-center rounded-md bg-white p-2 text-gray-400 hover:bg-gray-100 hover:text-gray-500 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-indigo-500" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open menu</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div><nav class="hidden space-x-10 md:flex"><div class="relative" data-headlessui-state=""><button class="text-gray-500 group inline-flex items-center rounded-md bg-white text-base font-medium hover:text-gray-900 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2" type="button" aria-expanded="false" data-headlessui-state=""><span>Solutions</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="text-gray-400 ml-2 h-5 w-5 group-hover:text-gray-500"><path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z" clip-rule="evenodd"></path></svg></button></div><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="/blog">Blog</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="https://actuated.dev/blog/blazing-fast-ci-with-microvms">Announcement</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="/pricing">Pricing</a><a class="text-base font-medium text-gray-500 hover:text-gray-900" href="https://docs.actuated.dev/">Docs</a></nav><div class="hidden items-center justify-end md:flex md:flex-1 lg:w-0"><a href="https://dashboard.actuated.dev" class="whitespace-nowrap text-base font-medium text-gray-500 hover:text-gray-900">Sign in</a><a href="/pricing" class="ml-8 inline-flex items-center justify-center whitespace-nowrap rounded-md border border-transparent bg-indigo-600 px-4 py-2 text-base font-medium text-white shadow-sm hover:bg-indigo-700">Sign-up</a></div></div></div></header><main><div class="container mx-auto max-w-4xl bg-white mt-4 px-4 sm:px-6"><h1 id="post_title" class="text-3xl mb-3 leading-8 font-extrabold tracking-tight text-gray-900 sm:text-4xl sm:leading-10 text-center">Accelerate GitHub Actions with dedicated GPUs</h1></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-4xl"><div class="border-b border-gray-200 py-4 flex items-center text-gray-500 mx-auto"><div><img class="h-10 w-10 rounded-full" src="/images/alex.jpg" alt=""/></div><div class="ml-3"><p id="post_author" class="text-sm leading-5 font-medium text-gray-900"></p><div class="flex text-sm leading-5 text-gray-500"><time dateTime="2024-03-12" id="post_date">March 12, 2024</time><span class="mx-1"></span></div></div></div><div class="mt-6 prose sm:prose-lg max-w-none"><p id="post_description" class="mb-4">You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases.</p></div><div id="post_content" class="mt-6 prose sm:prose-lg max-w-none"><p>With the surge of interest in AI and machine learning models, it's not hard to think of reasons why people want GPUs in their workstations and production environments. They make building &#x26; training, fine-tuning and serving (inference) from a machine learning model just that much quicker than running with a CPU alone.</p>
<p>So if you build and test code for CPUs in CI pipelines like GitHub Actions, why wouldn't you do the same with code built for GPUs? Why exercise only a portion of your codebase?</p>
<h2>GPUs for GitHub Actions</h2>
<p>One of our earliest customers moved all their GitHub Actions to actuated for a team of around 30 people, but since Firecracker has no support for GPUs, they had to keep a few self-hosted runners around for testing their models. Their second hand Dell servers were racked in their own datacentre, with 8x 3090 GPUs in each machine.</p>
<p>Their request for GPU support in actuated predated the hype around OpenAI, and was the catalyst for us doing this work.</p>
<p>They told us how many issues they had keeping drivers in sync when trying to use self-hosted runners, and the security issues they ran into with mounting Docker sockets or running privileged containers in Kubernetes.</p>
<p>With a microVM and actuated, you'll be able to test out different versions of drivers as you see fit, and know there will never be side-effects between builds. You can read more in <a href="https://docs.actuated.dev/faq/">our FAQ</a> on how actuated differs from other solutions which rely on the poor isolation afforded by containers. Actuated is the closest you can get to a hosted runner, whilst having full access to your own hardware.</p>
<p>I'll tell you a bit more about it, how to build your own workstation with commodity hardware, or where to rent a powerful bare-metal host with a capable GPU for less than 200 USD / mo that you can use with actuated.</p>
<h3>Available in early access</h3>
<p>So today, we're announcing early access to actuated for GPUs. Whether your machine has one GPU, two, or ten, you can allocate them directly to a microVM for a CI job, giving strong isolation, and the same ephemeral environment that you're used to with GitHub's hosted runners.</p>
<p><img src="/images/2024-03-gpus/3060.jpg" alt="Our test rig with 2x 3060s"></p>
<blockquote>
<p>Our test rig has 2x Nvidia 3060 GPUs and is available for customer demos and early testing.</p>
</blockquote>
<p>We've compiled a list of vendors that provide access to fast, bare-metal compute, but at the moment, there are only a few options for bare-metal with GPUs.</p>
<ul>
<li>Self-build a workstation, you'll recover the total cost in &#x3C; 1 month vs hosted</li>
<li>Use a European bare-metal host like Hetzner or OVH Cloud</li>
</ul>
<p>We have a full bill of materials available for anyone who wants to build a workstation with 2x Nvidia 3060 graphics cards, giving 24GB of usage RAM at a relatively low maximum power consumption of 170W. It's ideal for CI and end to end testing.</p>
<p>If you'd like to go even more premium, the Nvidia RTX 4000 card comes with 20GB of RAM, so two of those would give you 40GB of RAM available for Large Language Models (LLMs).</p>
<p>For Hetzner, you can get started with an i5 bare-metal host with 14 cores, 64GB RAM and a dedicated Nvidia RTX 4000 for around 184 EUR / mo (less than 200 USD / mo). If that sounds like ridiculously good value, it's because it is.</p>
<h2>What does the build look like?</h2>
<p>Once you've installed the actuated agent, it's the same process as a regular bare-metal host.</p>
<p>It'll show up on your actuated dashboard, and you can start sending jobs to it immediately.</p>
<p><img src="/images/2024-03-gpus/runner.png" alt="The server with 2x GPUs showing up in the dashboard"></p>
<blockquote>
<p>The server with 2x GPUs showing up in the dashboard</p>
</blockquote>
<p>Here's how we install the Nvidia driver for a consumer-grade card. The process is very similar for the datacenter range of GPUs found in enterprise servers.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">nvidia-smi</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">nvidia-smi:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">nvidia-smi</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Download</span> <span class="hljs-string">Nvidia</span> <span class="hljs-string">install</span> <span class="hljs-string">package</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
           curl -s -S -L -O https://us.download.nvidia.com/XFree86/Linux-x86_64/525.60.11/NVIDIA-Linux-x86_64-525.60.11.run \
              &#x26;&#x26; chmod +x ./NVIDIA-Linux-x86_64-525.60.11.run
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">Nvidia</span> <span class="hljs-string">driver</span> <span class="hljs-string">and</span> <span class="hljs-string">Kernel</span> <span class="hljs-string">module</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              sudo ./NVIDIA-Linux-x86_64-525.60.11.run \
                  --accept-license \
                  --ui=none \
                  --no-questions \
                  --no-x-check \
                  --no-check-for-alternate-installs \
                  --no-nouveau-check
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">nvidia-smi</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            nvidia-smi
</span></code></pre>
<p>This is a very similar approach to installing a driver on your own machine, just without any interactive prompts. It took around 38s which is not very long considering how much time AI and ML operations can run for when doing end to end testing. The process installs some binaries like <code>nvidia-smi</code> and compiles a Kernel module to load the graphics driver, these could easily be cached with GitHub Action's built-in caching mechanism.</p>
<p>For convenience, we created a composite action that reduces the duplication if you have lots of workflows with the Nvidia driver installed.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">gpu-job</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">gpu-job:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">gpu-job</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/nvidia-run@master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">nvidia-smi</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            nvidia-smi
</span></code></pre>
<p>Of course, if you have an AMD graphics card, or even an ML accelerator like a PCIe Google Corale, that can also be passed through into a VM in a dedicated way.</p>
<p>The mechanism being used is called VFIO, and allows a VM to take full, dedicated, isolated control over a PCI device.</p>
<h2>A quick example to get started</h2>
<p>To show the difference between using a GPU and CPU, I ran <a href="https://github.com/openai/whisper">OpenAI's Whisper project</a>, which transcribes audio or video to a text file.</p>
<p>With the <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">following demo video of Actuated's SSH gateway</a>, running with the tiny model.</p>
<ul>
<li>A AMD Ryzen 9 5950X 16-Core Processor took 39 seconds</li>
<li>The same host with a 3060 GPU mounted via actuated took 16 seconds</li>
</ul>
<p>That's over 2x quicker, for a 5:34 minute video. If you process a lot of clips, or much longer clips then the difference may be even more marked.</p>
<p>The tiny model is really designed for demos, and in production you'd use the medium or large model which is much more resource intensive.</p>
<p>Here's a screenshot showing what this looks like with the medium model, which is much larger and more accurate:</p>
<p><a href="/images/2024-03-gpus/gpu-medium.png">Medium model running on a GPU via actuated</a></p>
<blockquote>
<p>Medium model running on a GPU via actuated</p>
</blockquote>
<p>With a CPU, even with 16 vCPU, all of them get pinned at 100%, and then it takes a significantly longer time to process.</p>
<p><a href="https://twitter.com/alexellisuk/status/1767252171978871195/"><img src="/images/2024-03-gpus/cpu-medium.png" alt="You can run the medium model on CPU, but would you want to?"></a></p>
<blockquote>
<p>You can run the medium model on CPU, but would you want to?</p>
</blockquote>
<p>With the medium model:</p>
<ul>
<li>The CPU took 8 minutes 54 seconds, pinning 16 cores at 100%</li>
<li>The GPU took only 55s with very little CPU consumption</li>
</ul>
<p>The GPU increased the speed by 9x, imagine how much quicker it'd be if you used an Nvidia 3090, 4090, or even an RTX 4000.</p>
<p>If you want to just explore the system, and run commands interactively, you can use <a href="https://docs.actuated.dev/tasks/debug-ssh/">actuated's SSH feature</a> to get a shell. Once you know the commands you want to run, you can copy them into your workflow YAML file for GitHub Actions.</p>
<p>We took the SSH debug session for a test-drive. We installed <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">the NVIDIA Container Toolkit</a>, then ran the ollama tool to test out some Large Language Models (LLMs).</p>
<p><a href="https://ollama.com/">Ollama</a> is an open source tool for downloading and testing prepackaged models like Mistral or Llama2.</p>
<p><a href="https://twitter.com/alexellisuk/status/1767899463471796278/"><img src="https://pbs.twimg.com/media/GIjXEXZWkAAiuKc?format=png&#x26;name=medium" alt="Our experiment with ollama within a GitHub Actions runner"></a></p>
<blockquote>
<p>Our experiment with ollama within a GitHub Actions runner</p>
</blockquote>
<h2>The technical details</h2>
<p>Since launch, actuated powered by Firecracker has securely isolated over 220k CI jobs for GitHub Actions users. Whilst it's a complex project to integrate, it has been very reliable in production.</p>
<p>Now in order to bring GPUs to actuated, we needed to add support for a second Virtual Machine Manager (VMM), and we picked cloud-hypervisor.</p>
<p>cloud-hypervisor was originally a fork from Firecracker and shares a significant amount of code. One place it diverged was adding support for PCI devices, such as GPUs. Through VFIO, cloud-hypervisor allows for a GPU to be passed through to a VM in a dedicated way, so it can be used in isolation.</p>
<p>Here's the first demo that I ran when we had everything working, showing the output from <code>nvidia-smi</code>:</p>
<p><a href="https://twitter.com/alexellisuk/status/1765706313068130695/photo/1"><img src="https://pbs.twimg.com/media/GIEMJZUWMAAS0A8?format=jpg&#x26;name=large" alt="The first run of nvidia-smi"></a></p>
<blockquote>
<p>The first run of nvidia-smi</p>
</blockquote>
<h2>Reach out for more</h2>
<p>In a relatively short period of time, we were able to update our codebase to support both Firecracker and cloud-hypervisor, and to enable consumer-grade GPUs to be passed through to VMs in isolation.</p>
<p>You can rent a really powerful and capable machine from Hetzner for under 200 USD / mo, or build your own workstation with dual graphics cards like our demo rig, for less than 2000 USD and then you own that and can use it as much as you want, plugged in under your desk or left in a cabinet in your office.</p>
<p><strong>A quick recap on use-cases</strong></p>
<p>Let's say you want to run end to end tests for an application that uses a GPU? Perhaps it runs on Kubernetes? You can do that.</p>
<p>Do you want to fine-tune, train, or run a batch of inferences on a model? You can do that. GitHub Actions has a 6 hour timeout, which is plenty for many tasks.</p>
<p>Would it make sense to run Stable Diffusion in the background, with different versions, different inputs, across a matrix? GitHub Actions makes that easy, and actuated can manage the GPU allocations for you.</p>
<p>Do you run inference from OpenFaaS functions? We have a tutorial on <a href="https://www.openfaas.com/blog/transcribe-audio-with-openai-whisper/">OpenAI Whisper within a function with GPU acceleration here</a> and a separate one on how to <a href="https://www.openfaas.com/blog/openai-streaming-responses/">serve Server Sent Events (SSE) from OpenAI or self-hosted models</a>, which is popular for chat-style interfaces to AI models.</p>
<p>If you're interested in GPU support for GitHub Actions, then reach out to talk to us with <a href="https://actuated.dev/pricing">this form</a>.</p></div></div></main><footer class="bg-white"><div class="mx-auto max-w-7xl py-12 px-4 sm:px-6 md:flex md:items-center md:justify-between lg:px-8"><div class="flex justify-center space-x-6 md:order-2"><a class="text-gray-400 hover:text-gray-500" href="https://twitter.com/selfactuated"><span class="sr-only">Twitter</span><svg fill="currentColor" viewBox="0 0 24 24" class="h-6 w-6" aria-hidden="true"><path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84"></path></svg></a><a class="text-gray-400 hover:text-gray-500" href="https://github.com/self-actuated"><span class="sr-only">GitHub</span><svg fill="currentColor" viewBox="0 0 24 24" class="h-6 w-6" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path></svg></a><a class="text-gray-400 hover:text-gray-500" href="/rss.xml"><span class="sr-only">RSS</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M12.75 19.5v-.75a7.5 7.5 0 00-7.5-7.5H4.5m0-6.75h.75c7.87 0 14.25 6.38 14.25 14.25v.75M6 18.75a.75.75 0 11-1.5 0 .75.75 0 011.5 0z"></path></svg></a></div><div class="mt-8 md:order-1 md:mt-0"><p class="text-center text-base text-gray-400">© 2022 <a href="https://openfaas.com">OpenFaaS Ltd</a>, Inc. All rights reserved.</p></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"gpus-for-github-actions","fileName":"2024-03-12-gpus-for-github-actions.md","contentHtml":"\u003cp\u003eWith the surge of interest in AI and machine learning models, it's not hard to think of reasons why people want GPUs in their workstations and production environments. They make building \u0026#x26; training, fine-tuning and serving (inference) from a machine learning model just that much quicker than running with a CPU alone.\u003c/p\u003e\n\u003cp\u003eSo if you build and test code for CPUs in CI pipelines like GitHub Actions, why wouldn't you do the same with code built for GPUs? Why exercise only a portion of your codebase?\u003c/p\u003e\n\u003ch2\u003eGPUs for GitHub Actions\u003c/h2\u003e\n\u003cp\u003eOne of our earliest customers moved all their GitHub Actions to actuated for a team of around 30 people, but since Firecracker has no support for GPUs, they had to keep a few self-hosted runners around for testing their models. Their second hand Dell servers were racked in their own datacentre, with 8x 3090 GPUs in each machine.\u003c/p\u003e\n\u003cp\u003eTheir request for GPU support in actuated predated the hype around OpenAI, and was the catalyst for us doing this work.\u003c/p\u003e\n\u003cp\u003eThey told us how many issues they had keeping drivers in sync when trying to use self-hosted runners, and the security issues they ran into with mounting Docker sockets or running privileged containers in Kubernetes.\u003c/p\u003e\n\u003cp\u003eWith a microVM and actuated, you'll be able to test out different versions of drivers as you see fit, and know there will never be side-effects between builds. You can read more in \u003ca href=\"https://docs.actuated.dev/faq/\"\u003eour FAQ\u003c/a\u003e on how actuated differs from other solutions which rely on the poor isolation afforded by containers. Actuated is the closest you can get to a hosted runner, whilst having full access to your own hardware.\u003c/p\u003e\n\u003cp\u003eI'll tell you a bit more about it, how to build your own workstation with commodity hardware, or where to rent a powerful bare-metal host with a capable GPU for less than 200 USD / mo that you can use with actuated.\u003c/p\u003e\n\u003ch3\u003eAvailable in early access\u003c/h3\u003e\n\u003cp\u003eSo today, we're announcing early access to actuated for GPUs. Whether your machine has one GPU, two, or ten, you can allocate them directly to a microVM for a CI job, giving strong isolation, and the same ephemeral environment that you're used to with GitHub's hosted runners.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2024-03-gpus/3060.jpg\" alt=\"Our test rig with 2x 3060s\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOur test rig has 2x Nvidia 3060 GPUs and is available for customer demos and early testing.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe've compiled a list of vendors that provide access to fast, bare-metal compute, but at the moment, there are only a few options for bare-metal with GPUs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSelf-build a workstation, you'll recover the total cost in \u0026#x3C; 1 month vs hosted\u003c/li\u003e\n\u003cli\u003eUse a European bare-metal host like Hetzner or OVH Cloud\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe have a full bill of materials available for anyone who wants to build a workstation with 2x Nvidia 3060 graphics cards, giving 24GB of usage RAM at a relatively low maximum power consumption of 170W. It's ideal for CI and end to end testing.\u003c/p\u003e\n\u003cp\u003eIf you'd like to go even more premium, the Nvidia RTX 4000 card comes with 20GB of RAM, so two of those would give you 40GB of RAM available for Large Language Models (LLMs).\u003c/p\u003e\n\u003cp\u003eFor Hetzner, you can get started with an i5 bare-metal host with 14 cores, 64GB RAM and a dedicated Nvidia RTX 4000 for around 184 EUR / mo (less than 200 USD / mo). If that sounds like ridiculously good value, it's because it is.\u003c/p\u003e\n\u003ch2\u003eWhat does the build look like?\u003c/h2\u003e\n\u003cp\u003eOnce you've installed the actuated agent, it's the same process as a regular bare-metal host.\u003c/p\u003e\n\u003cp\u003eIt'll show up on your actuated dashboard, and you can start sending jobs to it immediately.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2024-03-gpus/runner.png\" alt=\"The server with 2x GPUs showing up in the dashboard\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe server with 2x GPUs showing up in the dashboard\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere's how we install the Nvidia driver for a consumer-grade card. The process is very similar for the datacenter range of GPUs found in enterprise servers.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-yaml\"\u003e\u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003envidia-smi\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003ejobs:\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003envidia-smi:\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003envidia-smi\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003eruns-on:\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003eactuated-8cpu-16gb\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003egpu\u003c/span\u003e]\n        \u003cspan class=\"hljs-attr\"\u003esteps:\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003euses:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eactions/checkout@v1\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eDownload\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eNvidia\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003einstall\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003epackage\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n           curl -s -S -L -O https://us.download.nvidia.com/XFree86/Linux-x86_64/525.60.11/NVIDIA-Linux-x86_64-525.60.11.run \\\n              \u0026#x26;\u0026#x26; chmod +x ./NVIDIA-Linux-x86_64-525.60.11.run\n\u003c/span\u003e        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eInstall\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eNvidia\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003edriver\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eand\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eKernel\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003emodule\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n              sudo ./NVIDIA-Linux-x86_64-525.60.11.run \\\n                  --accept-license \\\n                  --ui=none \\\n                  --no-questions \\\n                  --no-x-check \\\n                  --no-check-for-alternate-installs \\\n                  --no-nouveau-check\n\u003c/span\u003e        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eRun\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003envidia-smi\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n            nvidia-smi\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a very similar approach to installing a driver on your own machine, just without any interactive prompts. It took around 38s which is not very long considering how much time AI and ML operations can run for when doing end to end testing. The process installs some binaries like \u003ccode\u003envidia-smi\u003c/code\u003e and compiles a Kernel module to load the graphics driver, these could easily be cached with GitHub Action's built-in caching mechanism.\u003c/p\u003e\n\u003cp\u003eFor convenience, we created a composite action that reduces the duplication if you have lots of workflows with the Nvidia driver installed.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-yaml\"\u003e\u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003egpu-job\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003ejobs:\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003egpu-job:\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003egpu-job\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003eruns-on:\u003c/span\u003e [\u003cspan class=\"hljs-string\"\u003eactuated-8cpu-16gb\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003egpu\u003c/span\u003e]\n        \u003cspan class=\"hljs-attr\"\u003esteps:\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003euses:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eactions/checkout@v1\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003euses:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eself-actuated/nvidia-run@master\u003c/span\u003e\n        \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eRun\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003envidia-smi\u003c/span\u003e\n          \u003cspan class=\"hljs-attr\"\u003erun:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e|\n            nvidia-smi\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOf course, if you have an AMD graphics card, or even an ML accelerator like a PCIe Google Corale, that can also be passed through into a VM in a dedicated way.\u003c/p\u003e\n\u003cp\u003eThe mechanism being used is called VFIO, and allows a VM to take full, dedicated, isolated control over a PCI device.\u003c/p\u003e\n\u003ch2\u003eA quick example to get started\u003c/h2\u003e\n\u003cp\u003eTo show the difference between using a GPU and CPU, I ran \u003ca href=\"https://github.com/openai/whisper\"\u003eOpenAI's Whisper project\u003c/a\u003e, which transcribes audio or video to a text file.\u003c/p\u003e\n\u003cp\u003eWith the \u003ca href=\"https://www.youtube.com/watch?v=l9VuQZ4a5pc\"\u003efollowing demo video of Actuated's SSH gateway\u003c/a\u003e, running with the tiny model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA AMD Ryzen 9 5950X 16-Core Processor took 39 seconds\u003c/li\u003e\n\u003cli\u003eThe same host with a 3060 GPU mounted via actuated took 16 seconds\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat's over 2x quicker, for a 5:34 minute video. If you process a lot of clips, or much longer clips then the difference may be even more marked.\u003c/p\u003e\n\u003cp\u003eThe tiny model is really designed for demos, and in production you'd use the medium or large model which is much more resource intensive.\u003c/p\u003e\n\u003cp\u003eHere's a screenshot showing what this looks like with the medium model, which is much larger and more accurate:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/images/2024-03-gpus/gpu-medium.png\"\u003eMedium model running on a GPU via actuated\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMedium model running on a GPU via actuated\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith a CPU, even with 16 vCPU, all of them get pinned at 100%, and then it takes a significantly longer time to process.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/alexellisuk/status/1767252171978871195/\"\u003e\u003cimg src=\"/images/2024-03-gpus/cpu-medium.png\" alt=\"You can run the medium model on CPU, but would you want to?\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYou can run the medium model on CPU, but would you want to?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith the medium model:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe CPU took 8 minutes 54 seconds, pinning 16 cores at 100%\u003c/li\u003e\n\u003cli\u003eThe GPU took only 55s with very little CPU consumption\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe GPU increased the speed by 9x, imagine how much quicker it'd be if you used an Nvidia 3090, 4090, or even an RTX 4000.\u003c/p\u003e\n\u003cp\u003eIf you want to just explore the system, and run commands interactively, you can use \u003ca href=\"https://docs.actuated.dev/tasks/debug-ssh/\"\u003eactuated's SSH feature\u003c/a\u003e to get a shell. Once you know the commands you want to run, you can copy them into your workflow YAML file for GitHub Actions.\u003c/p\u003e\n\u003cp\u003eWe took the SSH debug session for a test-drive. We installed \u003ca href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\"\u003ethe NVIDIA Container Toolkit\u003c/a\u003e, then ran the ollama tool to test out some Large Language Models (LLMs).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/\"\u003eOllama\u003c/a\u003e is an open source tool for downloading and testing prepackaged models like Mistral or Llama2.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/alexellisuk/status/1767899463471796278/\"\u003e\u003cimg src=\"https://pbs.twimg.com/media/GIjXEXZWkAAiuKc?format=png\u0026#x26;name=medium\" alt=\"Our experiment with ollama within a GitHub Actions runner\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOur experiment with ollama within a GitHub Actions runner\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eThe technical details\u003c/h2\u003e\n\u003cp\u003eSince launch, actuated powered by Firecracker has securely isolated over 220k CI jobs for GitHub Actions users. Whilst it's a complex project to integrate, it has been very reliable in production.\u003c/p\u003e\n\u003cp\u003eNow in order to bring GPUs to actuated, we needed to add support for a second Virtual Machine Manager (VMM), and we picked cloud-hypervisor.\u003c/p\u003e\n\u003cp\u003ecloud-hypervisor was originally a fork from Firecracker and shares a significant amount of code. One place it diverged was adding support for PCI devices, such as GPUs. Through VFIO, cloud-hypervisor allows for a GPU to be passed through to a VM in a dedicated way, so it can be used in isolation.\u003c/p\u003e\n\u003cp\u003eHere's the first demo that I ran when we had everything working, showing the output from \u003ccode\u003envidia-smi\u003c/code\u003e:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/alexellisuk/status/1765706313068130695/photo/1\"\u003e\u003cimg src=\"https://pbs.twimg.com/media/GIEMJZUWMAAS0A8?format=jpg\u0026#x26;name=large\" alt=\"The first run of nvidia-smi\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe first run of nvidia-smi\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eReach out for more\u003c/h2\u003e\n\u003cp\u003eIn a relatively short period of time, we were able to update our codebase to support both Firecracker and cloud-hypervisor, and to enable consumer-grade GPUs to be passed through to VMs in isolation.\u003c/p\u003e\n\u003cp\u003eYou can rent a really powerful and capable machine from Hetzner for under 200 USD / mo, or build your own workstation with dual graphics cards like our demo rig, for less than 2000 USD and then you own that and can use it as much as you want, plugged in under your desk or left in a cabinet in your office.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA quick recap on use-cases\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet's say you want to run end to end tests for an application that uses a GPU? Perhaps it runs on Kubernetes? You can do that.\u003c/p\u003e\n\u003cp\u003eDo you want to fine-tune, train, or run a batch of inferences on a model? You can do that. GitHub Actions has a 6 hour timeout, which is plenty for many tasks.\u003c/p\u003e\n\u003cp\u003eWould it make sense to run Stable Diffusion in the background, with different versions, different inputs, across a matrix? GitHub Actions makes that easy, and actuated can manage the GPU allocations for you.\u003c/p\u003e\n\u003cp\u003eDo you run inference from OpenFaaS functions? We have a tutorial on \u003ca href=\"https://www.openfaas.com/blog/transcribe-audio-with-openai-whisper/\"\u003eOpenAI Whisper within a function with GPU acceleration here\u003c/a\u003e and a separate one on how to \u003ca href=\"https://www.openfaas.com/blog/openai-streaming-responses/\"\u003eserve Server Sent Events (SSE) from OpenAI or self-hosted models\u003c/a\u003e, which is popular for chat-style interfaces to AI models.\u003c/p\u003e\n\u003cp\u003eIf you're interested in GPU support for GitHub Actions, then reach out to talk to us with \u003ca href=\"https://actuated.dev/pricing\"\u003ethis form\u003c/a\u003e.\u003c/p\u003e","title":"Accelerate GitHub Actions with dedicated GPUs","description":"You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases.","tags":["ai","ml","githubactions","openai","transcription","machinelearning"],"author_img":"alex","image":"/images/2024-03-gpus/background.png","date":"2024-03-12"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"gpus-for-github-actions"},"buildId":"rbUDR1uvu6_LnDVo8sHCt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>