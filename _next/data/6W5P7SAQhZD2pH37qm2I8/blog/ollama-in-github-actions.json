{"pageProps":{"post":{"slug":"ollama-in-github-actions","fileName":"2024-03-25-ollama-in-github-actions.md","contentHtml":"<p>That means you can run real end to end tests in CI with the same models you may use in dev and production. And if you use OpenAI or AWS SageMaker extensively, you could perhaps swap out what can be a very expensive API endpoint for your CI or testing environments to save money.</p>\n<p>If you'd like to learn more about how and why you'd want access to GPUs in CI, read my past update: <a href=\"/blog/gpus-for-github-actions\">Accelerate GitHub Actions with dedicated GPUs</a>.</p>\n<p>We'll first cover what ollama is, why it's so popular, how to get it, what kinds of fun things you can do with it, then how to access it from actuated using a real GPU.</p>\n<p><img src=\"/images/2024-04-ollama-in-ci/logos.png\" alt=\"ollama can run in CI with isolated GPU acceleration using actuated\"></p>\n<blockquote>\n<p>ollama can now run in CI with isolated GPU acceleration using actuated</p>\n</blockquote>\n<h2>What's ollama?</h2>\n<p><a href=\"https://ollama.com/\">ollama</a> is an open source project that aims to do for AI models, what Docker did for Linux containers. Whilst Docker created a user experience to share and run containers using container images in the Open Container Initiative (OCI) format, ollama bundles well-known AI models and makes it easy to run them without having to think about Python versions or cuda libraries.</p>\n<p>The ollama project can be run directly on a Linux, MacOS or Windows host, or within a container. There's a server component, and a CLI that acts as a client to pre-trained models. The main use-case today is that of inference - exercising the model with input data. A more recent feature means that you can create embeddings, if you pull a model that supports them.</p>\n<p>On Linux, ollama can be installed using a utility script:</p>\n<pre><code class=\"hljs language-bash\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n<p>This provides the <code>ollama</code> CLI command.</p>\n<h2>A quick tour of ollama</h2>\n<p>After the initial installation, you can start a server:</p>\n<pre><code class=\"hljs language-bash\">ollama serve\n</code></pre>\n<p>By default, its REST API will listen on port <code>11434</code> on 127.0.0.1.</p>\n<p>You can find the reference for ollama's REST API here: <a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">API endpoints</a> - which includes things like: creating a chat completion, pulling a model, or generating embeddings.</p>\n<p>You can then browse <a href=\"https://ollama.com/library\">available models on the official website</a>, which resembles the Docker Hub.</p>\n<p>Most models will download with a default parameter size that's small enough to run on most CPUs or GPUs, but if you need to access it, there are larger models for higher accuracy.</p>\n<p>For instance, the <a href=\"https://ollama.com/library/llama2\">llama2</a> model by Meta will default to the 7b model which needs around 8GB of RAM.</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Pull the default model size:</span>\n\nollama pull llama2\n\n<span class=\"hljs-comment\"># Override the parameter size</span>\nollama pull llama2:13b\n</code></pre>\n<p>Once you have a model, you can then either \"run\" it, where you'll be able to ask it questions and interact with it like you would with ChatGPT, or you can send it API requests from your own applications using REST and HTTP.</p>\n<p>For an interactive prompt, give no parameters:</p>\n<pre><code class=\"hljs language-bash\">ollama run llama2\n</code></pre>\n<p>To get an immediate response for use in i.e. scripts:</p>\n<pre><code class=\"hljs language-bash\">ollama run llama2 <span class=\"hljs-string\">\"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"</span>\n</code></pre>\n<p>And you can use the REST API via <code>curl</code>, or your own codebase:</p>\n<pre><code class=\"hljs language-bash\">curl -s http://localhost:11434/api/generate -d <span class=\"hljs-string\">'{\n    \"model\": \"llama2\",\n    \"stream\": false,\n    \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n}'</span> | jq\n</code></pre>\n<h3>Access ollama from Python code</h3>\n<p>Here's how to access the API via Python, the <code>stream</code> parameter will emit JSON progressively when set to True, block until done if set to False. With Node.js, Python, Java, C#, etc the code will be very similar, but using your own preferred HTTP client.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">import</span> json\n\nurl = <span class=\"hljs-string\">\"http://localhost:11434/api/generate\"</span>\npayload = {\n    <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"llama2\"</span>,\n    <span class=\"hljs-string\">\"stream\"</span>: <span class=\"hljs-literal\">False</span>,\n    <span class=\"hljs-string\">\"prompt\"</span>: <span class=\"hljs-string\">\"What are the risks of running privileged Docker containers for CI workloads?\"</span>\n}\nheaders = {\n    <span class=\"hljs-string\">\"Content-Type\"</span>: <span class=\"hljs-string\">\"application/json\"</span>\n}\n\nresponse = requests.post(url, data=json.dumps(payload), headers=headers)\n\n<span class=\"hljs-comment\"># Parse the JSON response</span>\nresponse_json = response.json()\n\n<span class=\"hljs-comment\"># Pretty print the JSON response</span>\n<span class=\"hljs-built_in\">print</span>(json.dumps(response_json, indent=<span class=\"hljs-number\">4</span>))\n</code></pre>\n<h3>Putting it together for a GitHub Action</h3>\n<p>The following GitHub Action will run on for customers who are enrolled for GPU support for actuated.</p>\n<p>The initial set <code>self-actuated/nvidia-run</code> installs either the consumer or datacenter driver for Nvidia, depending on what you have in your system and takes about 30 seconds. This can be cached, if you like.</p>\n<p>Then, we simply run the equivalent bash commands from the previous section to:</p>\n<ul>\n<li>Install ollama</li>\n<li>Start serving the REST API</li>\n<li>Pull the llama2 model from Meta</li>\n<li>Run an inference via CLI</li>\n<li>Run an inference via REST API using curl</li>\n</ul>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">ollama-e2e</span>\n\n<span class=\"hljs-attr\">on:</span>\n    <span class=\"hljs-attr\">workflow_dispatch:</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n    <span class=\"hljs-attr\">ollama-e2e:</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">ollama-e2e</span>\n        <span class=\"hljs-attr\">runs-on:</span> [<span class=\"hljs-string\">actuated-8cpu-16gb</span>, <span class=\"hljs-string\">gpu</span>]\n        <span class=\"hljs-attr\">steps:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v1</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">self-actuated/nvidia-run@master</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">Ollama</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            curl -fsSL https://ollama.com/install.sh | sudo -E sh\n</span>        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Start</span> <span class=\"hljs-string\">serving</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              ollama serve &#x26;\n              sleep 5\n</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Pull</span> <span class=\"hljs-string\">llama2</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              ollama pull llama2\n</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Invoke</span> <span class=\"hljs-string\">via</span> <span class=\"hljs-string\">the</span> <span class=\"hljs-string\">CLI</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              ollama run llama2 \"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"\n</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Invoke</span> <span class=\"hljs-string\">via</span> <span class=\"hljs-string\">API</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            curl -s http://localhost:11434/api/generate -d '{\n              \"model\": \"llama2\",\n              \"stream\": false,\n              \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n            }' | jq\n</span></code></pre>\n<h3>Interactive access with SSH</h3>\n<p>By modifying your CI job, you can drop into a remote SSH session and run interactive commands at any point in the workflow.</p>\n<p>That's how I came up with the commands for the Nvidia driver installation, and for the various ollama commands I shared.</p>\n<p>Find out more about SSH for GitHub Actions <a href=\"https://docs.actuated.dev/tasks/debug-ssh/\">in the actuated docs</a>.</p>\n<h2>Wrapping up</h2>\n<p>Within a very short period of time ollama helped us pull a popular AI model that can be used for chat and completions. We were then able to take what we learned and run it on a GPU at an accelerated speed and accuracy by using actuated's <a href=\"/blog/gpus-for-github-actions\">new GPU support for GitHub Actions and GitLab CI</a>.</p>\n<p>ollama isn't the only way to find, download and run AI models, just like Docker wasn't the only way to download and install Nginx or Postgresql, but it provides a useful and convenient interface for those of us who are still learning about AI, and are not as concerned with the internal workings of the models.</p>\n<p>Over on the OpenFaaS blog, in the tutorial <a href=\"https://www.openfaas.com/blog/openai-streaming-responses/\">Stream OpenAI responses from functions using Server Sent Events</a>, we covered how to stream a response from a model to a function, and then back to a user. There, we used the <a href=\"https://github.com/c0sogi/llama-api\">llama-api</a> open source project, which is a single-purpose HTTP API for simulating llama2.</p>\n<p>One of the benefits of ollama is <a href=\"https://github.com/ollama/ollama/tree/main/examples\">the detailed range of examples</a> in the docs, and the ability to run other models that may include computer vision such as with the <a href=\"https://llava-vl.github.io/\">LLaVA: Large Language and Vision Assistant model</a> or generating code with <a href=\"https://codellama.dev/about\">Code Llama</a>.</p>\n<p>Right now, many of us are running and tuning models in development, some of us are using OpenAI's API or self-hosted models in production, but there's very little talk about doing thorough end to end testing or exercising models in CI. That's where actuated can help.</p>\n<p>Feel free to <a href=\"/pricing/\">reach out for early access, or to see if we can help your team with your CI needs</a>.</p>","title":"Run AI models with ollama in CI with GitHub Actions","description":"With the new GPU support for actuated, we've been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards.","tags":["ai","ollama","ml","localmodels","githubactions","openai","llama","machinelearning"],"author_img":"alex","image":"/images/2024-04-ollama-in-ci/background.png","date":"2024-04-25"}},"__N_SSG":true}