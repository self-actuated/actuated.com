{"pageProps":{"post":{"slug":"right-sizing-vms-github-actions","fileName":"2024-03-01-right-sizing-vms-github-actions.md","contentHtml":"<p>When we <a href=\"/blog/arm-ci-cncf-ampere\">onboarded the etcd project from the CNCF</a>, they'd previously been using a self-hosted runner for their repositories on a bare-metal host. There are several drawbacks to this approach, <a href=\"/blog/is-the-self-hosted-runner-safe-github-actions\">including potential security issues, especially when using Docker</a>.</p>\n<p>actuated VM sizes can be configured by a label, and you can pick any combination of vCPU and RAM, there's no need to pick a pre-defined size.</p>\n<p>At the same time, it can be hard to know what size to pick, and if you make the VM size too large, then you won't be able to run as many jobs at once.</p>\n<p>There's three main things to consider:</p>\n<ul>\n<li>The number of vCPUs - if there are not enough for the job, it'll be slower, and may hit timeouts which cause a failure</li>\n<li>The amount of RAM - if there's not enough, all the RAM could be consumed, and the VM will crash. You may not even get a helpful error message</li>\n<li>The amount of disk space per VM - if you make this too high, you'll be limited on the amount of jobs you can run at once, if you make it too low, jobs can fail, sometimes with non-obvious error messages</li>\n</ul>\n<p>We wrote a tool called vmmeter which takes samples of resource consumption over the duration of a build, and will then report back with the peak and average values.</p>\n<p>vmmeter is written in Go, and is available to use as a pre-built binary. We may consider open-sourcing it in the future. The information you gather still needs to be carefully considered and some experimentation will be required to get the right balance between VM size and performance.</p>\n<p>The tool can be run in an action by adding some YAML, however, it can also be run on any Linux system using bash, or potentially within a different CI/CD system. See the note at the end if you're interested in trying that out.</p>\n<h2 id=\"running-vmmeter-inside-github-actions\">Running vmmeter inside GitHub Actions</h2>\n<p>This action will work with a Linux VM environment, so with a hosted runner or with actuated. It may not work when used within the <code>containers:</code> section of a workflow, or with a Kubernetes-based runner.</p>\n<p>Add to the top of your GitHub action:</p>\n<pre><code class=\"hljs language-yaml\">\n<span class=\"hljs-attr\">steps:</span>\n<span class=\"hljs-comment\"># vmmeter start</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">alexellis/setup-arkade@master</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">self-actuated/vmmeter-action@master</span>\n<span class=\"hljs-comment\"># vmmeter end</span>\n</code></pre>\n<p>The first set installs arkade, which we then use to extract vmmeter from a container image to the host.</p>\n<p>Then <code>self-actuated/vmmeter-action</code> is used to run the tool in the background, and also runs a post-setup setup to stop the measurements, and upload the results to the workflow run.</p>\n<p>To show you how the tool works, I ran a simple <a href=\"https://github.com/actuated-samples/kernel-builder-linux-6.0/blob/master/.github/workflows/microvm-kernel.yml\">build of the Linux Kernel</a> without any additional modules or options added in.</p>\n<p>Here's the summary text that was uploaded to the workflow run:</p>\n<pre><code>Total RAM: 61.2GB\nTotal vCPU: 32\nLoad averages:\nMax 1 min: 5.63 (17.59%)\nMax 5 min: 1.25 (3.91%)\nMax 15 min: 0.41 (1.28%)\n\nRAM usage (10 samples):\nMax RAM usage: 2.528GB\n\nMax 10s avg RAM usage: 1.73GB\nMax 1 min avg RAM usage: 1.208GB\nMax 5 min avg RAM usage: 1.208GB\n\nDisk read: 374.2MB\nDisk write: 458.2MB\nMax disk I/O inflight: 0\nFree: 45.57GB\tUsed: 4.249GB\t(Total: 52.52GB)\n\nEgress adapter RX: 271.4MB\nEgress adapter TX: 1.535MB\n\nEntropy min: 256\nEntropy max: 256\n\nMax open connections: 125\nMax open files: 1696\nProcesses since boot: 18081\n\nRun time: 45s\n</code></pre>\n<p>The above text will be added to each job's summary when using vmmeter, but you can disable the summary by setting <code>createSummary: false</code> in the action's inputs. The output will still be available in the logs of the action under the <em>post</em> step, click to expand it.</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">self-actuated/vmmeter-action@master</span>\n  <span class=\"hljs-attr\">with:</span>\n    <span class=\"hljs-attr\">createSummary:</span> <span class=\"hljs-literal\">false</span>\n</code></pre>\n<p>The main thing to look for is the peak load on the system. This roughly corresponds to the amount of vCPUs used at peak. If the number is close to the amount you allocated, then try allocating more and measuring the effect in build time and peak usage.</p>\n<p>We've found that some jobs are RAM hungry, and others use a lot of CPU. So if you find that the RAM requested is much higher than the peak or average usage, the chances are that you can safely reduce it.</p>\n<p>Disk usage is self-explanatory, if you've allocated around 30GB per VM, and a job is getting close to that limit, it may need increasing to avoid future failures.</p>\n<p>Disk, network read/write and open files are potential indicators of I/O contention. if a job reads or writes a large amount of data over the network interface, then that may become a bottleneck. <a href=\"/blog/local-caching-for-github-actions\">Caching</a> is one of the ways to work around that, whether you set up your workflow to use GitHub's hosted cache, or one running in the same datacenter or region as your CI servers.</p>\n<h2 id=\"wrapping-up\">Wrapping up</h2>\n<p>In one case, a build on the etcd-io project was specified with 16 vCPU and 32GB of RAM, but when running vmmeter, they found that less than 2 vCPU was used at peak and less than 3GB of RAM. That's a significant difference.</p>\n<p>Toolpath is a commercial customer, and we were able to help them reduce their wall time per pull request from 6 hours to 60 minutes. Or from 6x 1 hour jobs to 6x 15-20 minute jobs running in parallel. Jason Gray told me during a product market fit interview that \"the level of expertise and support pays for itself\". We'd noticed that his teams jobs were requesting far too much CPU, but not enough RAM and were able to make recommendations. We then saw that disk space was running dangerously low, and were able to reconfigure their dedicated build servers for them, remotely, without them having to even think about it.</p>\n<p>If you'd like to try out vmmeter, it's free to use on GitHub's hosted runners and on actuated runners. We wouldn't recommend making it a permanent fixture in your workflow, because if it were to fail or exit early for any reason, it may mark the whole build as a failure.</p>\n<p>Instead, we recommend you use it learn and explore, and fine-tune your VM sizes. Getting the numbers closer to a right-size could reduce your costs with hosted runners and your efficiency with actuated runners.</p>\n<p>The source-code for the action is available here: <a href=\"https://github.com/self-actuated/vmmeter-action\">self-actuated/vmmeter-action</a>.</p>\n<ul>\n<li><a href=\"https://github.com/self-actuated/vmmeter-action/blob/master/index.js\">index.js</a> is used to setup the tool and start taking measurements</li>\n<li><a href=\"https://github.com/self-actuated/vmmeter-action/blob/master/post.js\">post.js</a> communicates to vmmeter over a TCP port and tells it to dump its response to <code>/tmp/vmmeter.log</code>, then to exit</li>\n</ul>\n<p><em>What if you're not using GitHub Actions?</em></p>\n<p>You can run vmmeter with bash on your own system, and may also able to use vmmeter in GitLab CI or Jenkins. You can even just start it up right now, do some work and then call the collect endpoint to see what was used over that period of time, a bit like a generic profiler.</p>\n<p>Here are the steps if you want to try out vmmeter on a different CI system like GitLab CI, Jenkins, or just as a standalone tool:</p>\n<h3 id=\"running-vmmeter-outside-of-github-actions\">Running vmmeter outside of GitHub Actions</h3>\n<p>Download arkade, then extract vmmeter from its OCI image:</p>\n<pre><code class=\"hljs language-bash\">curl https://get.arkade.dev | sudo sh\nsudo -E arkade oci install ghcr.io/openfaasltd/vmmeter:latest --path /usr/local/bin/\n</code></pre>\n<p>Start the vmmeter in the background, and check its logs to see that it started up:</p>\n<pre><code class=\"hljs language-bash\">/usr/local/bin/vmmeter &#x26;\n<span class=\"hljs-built_in\">cat</span> /tmp/vmmeter.log\n</code></pre>\n<p>At the end of the measurement period, make a HTTP request via curl to the collect the results, and to shutdown the tool:</p>\n<pre><code class=\"hljs language-bash\">port=$(<span class=\"hljs-built_in\">cat</span> /tmp/vmmeter.port)\ncurl http://127.0.0.1:<span class=\"hljs-variable\">$port</span>/collect\n</code></pre>","title":"Right sizing VMs for GitHub Actions","description":"How do you pick the right VM size for your GitHub Actions runners? We wrote a custom tool to help you find out.","tags":["efficiency","githubactions","metering"],"author_img":"alex","image":"/images/2024-03-right-sizing/background.png","date":"2024-03-01"}},"__N_SSG":true}