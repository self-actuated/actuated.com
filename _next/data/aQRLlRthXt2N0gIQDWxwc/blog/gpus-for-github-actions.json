{"pageProps":{"post":{"slug":"gpus-for-github-actions","fileName":"2024-03-12-gpus-for-github-actions.md","contentHtml":"<p>With the surge of interest in AI and machine learning models, it's not hard to think of reasons why people want GPUs in their workstations and production environments. They make building &#x26; training, fine-tuning and serving (inference) from a machine learning model just that much quicker than running with a CPU alone.</p>\n<p>So if you build and test code for CPUs in CI pipelines like GitHub Actions, why wouldn't you do the same with code built for GPUs? Why exercise only a portion of your codebase?</p>\n<h2>GPUs for GitHub Actions</h2>\n<p>One of our earliest customers moved all their GitHub Actions to actuated for a team of around 30 people, but since Firecracker has no support for GPUs, they had to keep a few self-hosted runners around for testing their models. Their second hand Dell servers were racked in their own datacentre, with 8x 3090 GPUs in each machine.</p>\n<p>Their request for GPU support in actuated predated the hype around OpenAI, and was the catalyst for us doing this work.</p>\n<p>They told us how many issues they had keeping drivers in sync when trying to use self-hosted runners, and the security issues they ran into with mounting Docker sockets or running privileged containers in Kubernetes.</p>\n<p>With a microVM and actuated, you'll be able to test out different versions of drivers as you see fit, and know there will never be side-effects between builds. You can read more in <a href=\"https://docs.actuated.dev/faq/\">our FAQ</a> on how actuated differs from other solutions which rely on the poor isolation afforded by containers. Actuated is the closest you can get to a hosted runner, whilst having full access to your own hardware.</p>\n<p>I'll tell you a bit more about it, how to build your own workstation with commodity hardware, or where to rent a powerful bare-metal host with a capable GPU for less than 200 USD / mo that you can use with actuated.</p>\n<h3>Available in early access</h3>\n<p>So today, we're announcing early access to actuated for GPUs. Whether your machine has one GPU, two, or ten, you can allocate them directly to a microVM for a CI job, giving strong isolation, and the same ephemeral environment that you're used to with GitHub's hosted runners.</p>\n<p><img src=\"/images/2024-03-gpus/3060.jpg\" alt=\"Our test rig with 2x 3060s\"></p>\n<blockquote>\n<p>Our test rig has 2x Nvidia 3060 GPUs and is available for customer demos and early testing.</p>\n</blockquote>\n<p>We've compiled a list of vendors that provide access to fast, bare-metal compute, but at the moment, there are only a few options for bare-metal with GPUs.</p>\n<ul>\n<li>Self-build a workstation, you'll recover the total cost in &#x3C; 1 month vs hosted</li>\n<li>Use a European bare-metal host like Hetzner or OVH Cloud</li>\n</ul>\n<p>We have a full bill of materials available for anyone who wants to build a workstation with 2x Nvidia 3060 graphics cards, giving 24GB of usage RAM at a relatively low maximum power consumption of 170W. It's ideal for CI and end to end testing.</p>\n<p>If you'd like to go even more premium, the Nvidia RTX 4000 card comes with 20GB of RAM, so two of those would give you 40GB of RAM available for Large Language Models (LLMs).</p>\n<p>For Hetzner, you can get started with an i5 bare-metal host with 14 cores, 64GB RAM and a dedicated Nvidia RTX 4000 for around 184 EUR / mo (less than 200 USD / mo). If that sounds like ridiculously good value, it's because it is.</p>\n<h2>What does the build look like?</h2>\n<p>Once you've installed the actuated agent, it's the same process as a regular bare-metal host.</p>\n<p>It'll show up on your actuated dashboard, and you can start sending jobs to it immediately.</p>\n<p><img src=\"/images/2024-03-gpus/runner.png\" alt=\"The server with 2x GPUs showing up in the dashboard\"></p>\n<blockquote>\n<p>The server with 2x GPUs showing up in the dashboard</p>\n</blockquote>\n<p>Here's how we install the Nvidia driver for a consumer-grade card. The process is very similar for the datacenter range of GPUs found in enterprise servers.</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">nvidia-smi</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n    <span class=\"hljs-attr\">nvidia-smi:</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">nvidia-smi</span>\n        <span class=\"hljs-attr\">runs-on:</span> [<span class=\"hljs-string\">actuated-8cpu-16gb</span>, <span class=\"hljs-string\">gpu</span>]\n        <span class=\"hljs-attr\">steps:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v1</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Download</span> <span class=\"hljs-string\">Nvidia</span> <span class=\"hljs-string\">install</span> <span class=\"hljs-string\">package</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n           curl -s -S -L -O https://us.download.nvidia.com/XFree86/Linux-x86_64/525.60.11/NVIDIA-Linux-x86_64-525.60.11.run \\\n              &#x26;&#x26; chmod +x ./NVIDIA-Linux-x86_64-525.60.11.run\n</span>        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">Nvidia</span> <span class=\"hljs-string\">driver</span> <span class=\"hljs-string\">and</span> <span class=\"hljs-string\">Kernel</span> <span class=\"hljs-string\">module</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              sudo ./NVIDIA-Linux-x86_64-525.60.11.run \\\n                  --accept-license \\\n                  --ui=none \\\n                  --no-questions \\\n                  --no-x-check \\\n                  --no-check-for-alternate-installs \\\n                  --no-nouveau-check\n</span>        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">nvidia-smi</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            nvidia-smi\n</span></code></pre>\n<p>This is a very similar approach to installing a driver on your own machine, just without any interactive prompts. It took around 38s which is not very long considering how much time AI and ML operations can run for when doing end to end testing. The process installs some binaries like <code>nvidia-smi</code> and compiles a Kernel module to load the graphics driver, these could easily be cached with GitHub Action's built-in caching mechanism.</p>\n<p>For convenience, we created a composite action that reduces the duplication if you have lots of workflows with the Nvidia driver installed.</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">gpu-job</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n    <span class=\"hljs-attr\">gpu-job:</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">gpu-job</span>\n        <span class=\"hljs-attr\">runs-on:</span> [<span class=\"hljs-string\">actuated-8cpu-16gb</span>, <span class=\"hljs-string\">gpu</span>]\n        <span class=\"hljs-attr\">steps:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v1</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">self-actuated/nvidia-run@master</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">nvidia-smi</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            nvidia-smi\n</span></code></pre>\n<p>Of course, if you have an AMD graphics card, or even an ML accelerator like a PCIe Google Corale, that can also be passed through into a VM in a dedicated way.</p>\n<p>The mechanism being used is called VFIO, and allows a VM to take full, dedicated, isolated control over a PCI device.</p>\n<h2>A quick example to get started</h2>\n<p>To show the difference between using a GPU and CPU, I ran <a href=\"https://github.com/openai/whisper\">OpenAI's Whisper project</a>, which transcribes audio or video to a text file.</p>\n<p>With the <a href=\"https://www.youtube.com/watch?v=l9VuQZ4a5pc\">following demo video of Actuated's SSH gateway</a>, running with the tiny model.</p>\n<ul>\n<li>A AMD Ryzen 9 5950X 16-Core Processor took 39 seconds</li>\n<li>The same host with a 3060 GPU mounted via actuated took 16 seconds</li>\n</ul>\n<p>That's over 2x quicker, for a 5:34 minute video. If you process a lot of clips, or much longer clips then the difference may be even more marked.</p>\n<p>The tiny model is really designed for demos, and in production you'd use the medium or large model which is much more resource intensive.</p>\n<p>Here's a screenshot showing what this looks like with the medium model, which is much larger and more accurate:</p>\n<p><a href=\"/images/2024-03-gpus/gpu-medium.png\">Medium model running on a GPU via actuated</a></p>\n<blockquote>\n<p>Medium model running on a GPU via actuated</p>\n</blockquote>\n<p>With a CPU, even with 16 vCPU, all of them get pinned at 100%, and then it takes a significantly longer time to process.</p>\n<p><a href=\"https://twitter.com/alexellisuk/status/1767252171978871195/\"><img src=\"/images/2024-03-gpus/cpu-medium.png\" alt=\"You can run the medium model on CPU, but would you want to?\"></a></p>\n<blockquote>\n<p>You can run the medium model on CPU, but would you want to?</p>\n</blockquote>\n<p>With the medium model:</p>\n<ul>\n<li>The CPU took 8 minutes 54 seconds, pinning 16 cores at 100%</li>\n<li>The GPU took only 55s with very little CPU consumption</li>\n</ul>\n<p>The GPU increased the speed by 9x, imagine how much quicker it'd be if you used an Nvidia 3090, 4090, or even an RTX 4000.</p>\n<p>If you want to just explore the system, and run commands interactively, you can use <a href=\"https://docs.actuated.dev/tasks/debug-ssh/\">actuated's SSH feature</a> to get a shell. Once you know the commands you want to run, you can copy them into your workflow YAML file for GitHub Actions.</p>\n<p>We took the SSH debug session for a test-drive. We installed <a href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\">the NVIDIA Container Toolkit</a>, then ran the ollama tool to test out some Large Language Models (LLMs).</p>\n<p><a href=\"https://ollama.com/\">Ollama</a> is an open source tool for downloading and testing prepackaged models like Mistral or Llama2.</p>\n<p><a href=\"https://twitter.com/alexellisuk/status/1767899463471796278/\"><img src=\"https://pbs.twimg.com/media/GIjXEXZWkAAiuKc?format=png&#x26;name=medium\" alt=\"Our experiment with ollama within a GitHub Actions runner\"></a></p>\n<blockquote>\n<p>Our experiment with ollama within a GitHub Actions runner</p>\n</blockquote>\n<h2>The technical details</h2>\n<p>Since launch, actuated powered by Firecracker has securely isolated over 220k CI jobs for GitHub Actions users. Whilst it's a complex project to integrate, it has been very reliable in production.</p>\n<p>Now in order to bring GPUs to actuated, we needed to add support for a second Virtual Machine Manager (VMM), and we picked cloud-hypervisor.</p>\n<p>cloud-hypervisor was originally a fork from Firecracker and shares a significant amount of code. One place it diverged was adding support for PCI devices, such as GPUs. Through VFIO, cloud-hypervisor allows for a GPU to be passed through to a VM in a dedicated way, so it can be used in isolation.</p>\n<p>Here's the first demo that I ran when we had everything working, showing the output from <code>nvidia-smi</code>:</p>\n<p><a href=\"https://twitter.com/alexellisuk/status/1765706313068130695/photo/1\"><img src=\"https://pbs.twimg.com/media/GIEMJZUWMAAS0A8?format=jpg&#x26;name=large\" alt=\"The first run of nvidia-smi\"></a></p>\n<blockquote>\n<p>The first run of nvidia-smi</p>\n</blockquote>\n<h2>Reach out for more</h2>\n<p>In a relatively short period of time, we were able to update our codebase to support both Firecracker and cloud-hypervisor, and to enable consumer-grade GPUs to be passed through to VMs in isolation.</p>\n<p>You can rent a really powerful and capable machine from Hetzner for under 200 USD / mo, or build your own workstation with dual graphics cards like our demo rig, for less than 2000 USD and then you own that and can use it as much as you want, plugged in under your desk or left in a cabinet in your office.</p>\n<p><strong>A quick recap on use-cases</strong></p>\n<p>Let's say you want to run end to end tests for an application that uses a GPU? Perhaps it runs on Kubernetes? You can do that.</p>\n<p>Do you want to fine-tune, train, or run a batch of inferences on a model? You can do that. GitHub Actions has a 6 hour timeout, which is plenty for many tasks.</p>\n<p>Would it make sense to run Stable Diffusion in the background, with different versions, different inputs, across a matrix? GitHub Actions makes that easy, and actuated can manage the GPU allocations for you.</p>\n<p>Do you run inference from OpenFaaS functions? We have a tutorial on <a href=\"https://www.openfaas.com/blog/transcribe-audio-with-openai-whisper/\">OpenAI Whisper within a function with GPU acceleration here</a> and a separate one on how to <a href=\"https://www.openfaas.com/blog/openai-streaming-responses/\">serve Server Sent Events (SSE) from OpenAI or self-hosted models</a>, which is popular for chat-style interfaces to AI models.</p>\n<p>If you're interested in GPU support for GitHub Actions, then reach out to talk to us with <a href=\"https://actuated.dev/pricing\">this form</a>.</p>","title":"Accelerate GitHub Actions with dedicated GPUs","description":"You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases.","tags":["ai","ml","githubactions","openai","transcription","machinelearning"],"author_img":"alex","image":"/images/2024-03-gpus/background.png","date":"2024-03-12"}},"__N_SSG":true}