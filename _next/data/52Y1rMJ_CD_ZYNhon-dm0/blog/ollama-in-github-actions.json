{"pageProps":{"post":{"slug":"ollama-in-github-actions","fileName":"2024-03-25-ollama-in-github-actions.md","contentHtml":"<p>That means you can run real end to end tests in CI with the same models you may use in dev and production. And if you use OpenAI or AWS SageMaker extensively, you could perhaps swap out what can be a very expensive API endpoint for your CI or testing environments to save money.</p>\n<p>If you'd like to learn more about how and why you'd want access to GPUs in CI, read my past update: <a href=\"/blog/gpus-for-github-actions\">Accelerate GitHub Actions with dedicated GPUs</a>.</p>\n<p>We'll first cover what ollama is, why it's so popular, how to get it, what kinds of fun things you can do with it, then how to access it from actuated using a real GPU.</p>\n<p><img src=\"/images/2024-04-ollama-in-ci/logos.png\" alt=\"ollama can run in CI with isolated GPU acceleration using actuated\"></p>\n<blockquote>\n<p>ollama can now run in CI with isolated GPU acceleration using actuated</p>\n</blockquote>\n<h2>What's ollama?</h2>\n<p><a href=\"https://ollama.com/\">ollama</a> is an open source project that aims to do for AI models, what Docker did for Linux containers. Whilst Docker created a user experience to share and run containers using container images in the Open Container Initiative (OCI) format, ollama bundles well-known AI models and makes it easy to run them without having to think about Python versions or Nvidia CUDA libraries.</p>\n<p>The project packages and runs various models, but seems to take its name from Meta's popular <a href=\"https://llama.meta.com/\">llama2 model</a>, which whilst <a href=\"https://llama.meta.com/faq\">not released under an open source license</a>, allows for a generous amount of free usage for most types of users.</p>\n<p>The ollama project can be run directly on a Linux, MacOS or Windows host, or within a container. There's a server component, and a CLI that acts as a client to pre-trained models. The main use-case today is that of inference - exercising the model with input data. A more recent feature means that you can create embeddings, if you pull a model that supports them.</p>\n<p>On Linux, ollama can be installed using a utility script:</p>\n<pre><code class=\"hljs language-bash\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n<p>This provides the <code>ollama</code> CLI command.</p>\n<h2>A quick tour of ollama</h2>\n<p>After the initial installation, you can start a server:</p>\n<pre><code class=\"hljs language-bash\">ollama serve\n</code></pre>\n<p>By default, its REST API will listen on port <code>11434</code> on 127.0.0.1.</p>\n<p>You can find the reference for ollama's REST API here: <a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">API endpoints</a> - which includes things like: creating a chat completion, pulling a model, or generating embeddings.</p>\n<p>You can then browse <a href=\"https://ollama.com/library\">available models on the official website</a>, which resembles the Docker Hub. This set currently includes: gemma (built upon Google's DeepMind), mistral (an LLM), codellama (for generating Code), phi (from Microsoft research), vicuna (for chat, based upon llama2), llava (a vision encoder), and many more.</p>\n<p>Most models will download with a default parameter size that's small enough to run on most CPUs or GPUs, but if you need to access it, there are larger models for higher accuracy.</p>\n<p>For instance, the <a href=\"https://ollama.com/library/llama2\">llama2</a> model by Meta will default to the 7b model which needs around 8GB of RAM.</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Pull the default model size:</span>\n\nollama pull llama2\n\n<span class=\"hljs-comment\"># Override the parameter size</span>\n\nollama pull llama2:13b\n</code></pre>\n<p>Once you have a model, you can then either \"run\" it, where you'll be able to ask it questions and interact with it like you would with ChatGPT, or you can send it API requests from your own applications using REST and HTTP.</p>\n<p>For an interactive prompt, give no parameters:</p>\n<pre><code class=\"hljs language-bash\">ollama run llama2\n</code></pre>\n<p>To get an immediate response for use in i.e. scripts:</p>\n<pre><code class=\"hljs language-bash\">ollama run llama2 <span class=\"hljs-string\">\"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"</span>\n</code></pre>\n<p>And you can use the REST API via <code>curl</code>, or your own codebase:</p>\n<pre><code class=\"hljs language-bash\">curl -s http://localhost:11434/api/generate -d <span class=\"hljs-string\">'{\n    \"model\": \"llama2\",\n    \"stream\": false,\n    \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n}'</span> | jq\n</code></pre>\n<p>We are just scratching the surface with what ollama can do, with a focus on testing and pulling pre-built models, but you can also create and share models using a <a href=\"https://github.com/ollama/ollama/blob/main/docs/modelfile.md\">Modelfile</a>, which is another homage to the Docker experience by the ollama developers.</p>\n<h3>Access ollama from Python code</h3>\n<p>Here's how to access the API via Python, the <code>stream</code> parameter will emit JSON progressively when set to True, block until done if set to False. With Node.js, Python, Java, C#, etc the code will be very similar, but using your own preferred HTTP client. For Golang (Go) users, ollama founder <a href=\"https://twitter.com/jmorgan\">Jeffrey Morgan</a> maintains a <a href=\"https://pkg.go.dev/github.com/jmorganca/ollama/api\">higher-level Go SDK</a>.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">import</span> json\n\nurl = <span class=\"hljs-string\">\"http://localhost:11434/api/generate\"</span>\npayload = {\n    <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"llama2\"</span>,\n    <span class=\"hljs-string\">\"stream\"</span>: <span class=\"hljs-literal\">False</span>,\n    <span class=\"hljs-string\">\"prompt\"</span>: <span class=\"hljs-string\">\"What are the risks of running privileged Docker containers for CI workloads?\"</span>\n}\nheaders = {\n    <span class=\"hljs-string\">\"Content-Type\"</span>: <span class=\"hljs-string\">\"application/json\"</span>\n}\n\nresponse = requests.post(url, data=json.dumps(payload), headers=headers)\n\n<span class=\"hljs-comment\"># Parse the JSON response</span>\nresponse_json = response.json()\n\n<span class=\"hljs-comment\"># Pretty print the JSON response</span>\n<span class=\"hljs-built_in\">print</span>(json.dumps(response_json, indent=<span class=\"hljs-number\">4</span>))\n</code></pre>\n<p>When you're constructing a request by API, make sure you include any tags in the <code>model</code> name, if you've used one. I.e. <code>\"model\": \"llama2:13b\"</code>.</p>\n<p>I hear from so many organisations who have gone to lengths to get SOC2 compliance, doing CVE scanning, or who are running Open Policy Agent or Kyverno to enforce strict Pod admission policies in Kubernetes, but then are happy to run their CI in Pods in privileged mode. So I asked the model why that may not be a smart idea. You can run the sample for yourself or <a href=\"https://gist.githubusercontent.com/alexellis/4c85e5927d251153c41379c4cac1a6c8/raw/257a02df0e01dff966c68509baf299bc32d3a11e/security.txt\">see the response here</a>. We also go into detail in the <a href=\"https://docs.actuated.dev/faq/\">actuated FAQ</a>, the security situation around self-hosted runners and containers is the main reason we built the solution.</p>\n<h3>Putting it together for a GitHub Action</h3>\n<p>The following GitHub Action will run on for customers who are enrolled for GPU support for actuated. If you'd like to gain access, contact us via the form on the <a href=\"https://actuated.dev/pricing\">Pricing page</a>.</p>\n<p>The <code>self-actuated/nvidia-run</code> installs either the consumer or datacenter driver for Nvidia, depending on what you have in your system. This only takes about 30 seconds and could be cached if you like. The ollama models could also be <a href=\"https://docs.actuated.dev/tasks/local-github-cache/\">cached using a local S3 bucket</a>.</p>\n<p>Then, we simply run the equivalent bash commands from the previous section to:</p>\n<ul>\n<li>Install ollama</li>\n<li>Start serving the REST API</li>\n<li>Pull the llama2 model from Meta</li>\n<li>Run an inference via CLI</li>\n<li>Run an inference via REST API using curl</li>\n</ul>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">ollama-e2e</span>\n\n<span class=\"hljs-attr\">on:</span>\n    <span class=\"hljs-attr\">workflow_dispatch:</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n    <span class=\"hljs-attr\">ollama-e2e:</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">ollama-e2e</span>\n        <span class=\"hljs-attr\">runs-on:</span> [<span class=\"hljs-string\">actuated-8cpu-16gb</span>, <span class=\"hljs-string\">gpu</span>]\n        <span class=\"hljs-attr\">steps:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v1</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">self-actuated/nvidia-run@master</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">Ollama</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            curl -fsSL https://ollama.com/install.sh | sudo -E sh\n</span>        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Start</span> <span class=\"hljs-string\">serving</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              # Run the background, there is no way to daemonise at the moment\n              ollama serve &#x26;\n</span>\n              <span class=\"hljs-comment\"># A short pause is required before the HTTP port is opened</span>\n              <span class=\"hljs-string\">sleep</span> <span class=\"hljs-number\">5</span>\n\n              <span class=\"hljs-comment\"># This endpoint blocks until ready</span>\n              <span class=\"hljs-string\">time</span> <span class=\"hljs-string\">curl</span> <span class=\"hljs-string\">-i</span> <span class=\"hljs-string\">http://localhost:11434</span>\n\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Pull</span> <span class=\"hljs-string\">llama2</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              ollama pull llama2\n</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Invoke</span> <span class=\"hljs-string\">via</span> <span class=\"hljs-string\">the</span> <span class=\"hljs-string\">CLI</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n              ollama run llama2 \"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?\"\n</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Invoke</span> <span class=\"hljs-string\">via</span> <span class=\"hljs-string\">API</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n            curl -s http://localhost:11434/api/generate -d '{\n              \"model\": \"llama2\",\n              \"stream\": false,\n              \"prompt\":\"What are the risks of running privileged Docker containers for CI workloads?\"\n            }' | jq\n</span></code></pre>\n<p>There is no built-in way to daemonise the ollama server, so for now we run it in the background using bash. The readiness endpoint can then be accessed which blocks until the server has completed its initialisation.</p>\n<h3>Interactive access with SSH</h3>\n<p>By modifying your CI job, you can drop into a remote SSH session and run interactive commands at any point in the workflow.</p>\n<p>That's how I came up with the commands for the Nvidia driver installation, and for the various ollama commands I shared.</p>\n<p>Find out more about SSH for GitHub Actions <a href=\"https://docs.actuated.dev/tasks/debug-ssh/\">in the actuated docs</a>.</p>\n<p><img src=\"/images/2024-04-ollama-in-ci/ssh.png\" alt=\"Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM\"></p>\n<blockquote>\n<p>Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM</p>\n</blockquote>\n<h2>Wrapping up</h2>\n<p>Within a very short period of time ollama helped us pull a popular AI model that can be used for chat and completions. We were then able to take what we learned and run it on a GPU at an accelerated speed and accuracy by using actuated's <a href=\"/blog/gpus-for-github-actions\">new GPU support for GitHub Actions and GitLab CI</a>. Most hosted CI systems provide a relatively small amount of disk space for jobs, with actuated you can customise this and that may be important if you're going to be downloading large AI models. You can also easily customise the amount of RAM and vCPU using the <code>runs-on</code> label to any combination you need.</p>\n<p>ollama isn't the only way to find, download and run AI models, just like Docker wasn't the only way to download and install Nginx or Postgresql, but it provides a useful and convenient interface for those of us who are still learning about AI, and are not as concerned with the internal workings of the models.</p>\n<p>Over on the OpenFaaS blog, in the tutorial <a href=\"https://www.openfaas.com/blog/openai-streaming-responses/\">Stream OpenAI responses from functions using Server Sent Events</a>, we covered how to stream a response from a model to a function, and then back to a user. There, we used the <a href=\"https://github.com/c0sogi/llama-api\">llama-api</a> open source project, which is a single-purpose HTTP API for simulating llama2.</p>\n<p>One of the benefits of ollama is <a href=\"https://github.com/ollama/ollama/tree/main/examples\">the detailed range of examples</a> in the docs, and the ability to run other models that may include computer vision such as with the <a href=\"https://llava-vl.github.io/\">LLaVA: Large Language and Vision Assistant model</a> or generating code with <a href=\"https://codellama.dev/about\">Code Llama</a>.</p>\n<p>Right now, many of us are running and tuning models in development, some of us are using OpenAI's API or self-hosted models in production, but there's very little talk about doing thorough end to end testing or exercising models in CI. That's where actuated can help.</p>\n<p>Feel free to <a href=\"/pricing\">reach out for early access, or to see if we can help your team with your CI needs</a>.</p>","title":"Run AI models with ollama in CI with GitHub Actions","description":"With the new GPU support for actuated, we've been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards.","tags":["ai","ollama","ml","localmodels","githubactions","openai","llama","machinelearning"],"author_img":"alex","image":"/images/2024-04-ollama-in-ci/background.png","date":"2024-04-25"}},"__N_SSG":true}