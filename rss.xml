<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Actuated - blog</title>
        <link>https://actuated.dev</link>
        <description>Keep your team productive &amp; focused with blazing fast CI</description>
        <lastBuildDate>Fri, 16 Jun 2023 10:27:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <image>
            <title>Actuated - blog</title>
            <url>https://actuated.dev/images/actuated.png</url>
            <link>https://actuated.dev</link>
        </image>
        <item>
            <title><![CDATA[Understand your usage of GitHub Actions]]></title>
            <link>https://actuated.dev/blog/github-actions-usage-cli</link>
            <guid>https://actuated.dev/blog/github-actions-usage-cli</guid>
            <pubDate>Fri, 16 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how you or your team is using GitHub Actions across your personal account or organisation.]]></description>
            <content:encoded><![CDATA[<p>Whenever GitHub Actions users get in touch with us to ask about actuated, we ask them a number of questions. What do you build? What pain points have you been running into? What are you currently spending? And then - how many minutes are you using?</p>
<p>That final question is a hard one for many to answer because the GitHub UI and API will only show billable minutes. Why is that a problem? Some teams only use open-source repositories with free runners. Others may have a large free allowance of credit for one reason or another and so also don't really know what they're using. Then you have people who already use some form of self-hosted runners - they are also excluded from what GitHub shows you.</p>
<p>So we built an Open Source CLI tool called <a href="https://github.com/self-actuated/actions-usage">actions-usage</a> to generate a report of your total minutes by querying GitHub's REST API.</p>
<p>And over time, we had requests to break-down per day - so for our customers in the Middle East like <a href="https://kubiya.ai/">Kubiya</a>, it's common to see a very busy day on Sunday, and not a lot of action on Friday. Given that some teams use mono-repos, we also added the ability to break-down per repository - so you can see which ones are the most active. And finally, we added the ability to see hot-spots of usage like the longest running repo or the most active day.</p>
<p>You can run the tool in three ways:</p>
<ul>
<li>Against an organisation</li>
<li>Against a personal user account</li>
<li>Or in a GitHub Action</li>
</ul>
<p>I'll show you each briefly, but the one I like the most is the third option because it's kind of recursive.</p>
<p>Before we get started, download arkade, and use it to install the tool:</p>
<pre><code class="hljs language-bash">curl -sLS https://get.arkade.dev | sudo sh
arkade install actions-usage
</code></pre>
<p>Later on, I'll also show you how to use the <code>alexellis/arkade-get</code> action to install the tool for CI.</p>
<h2>Finding out about your organisation</h2>
<p>If you want to find out about your organisation, you can run the tool like this:</p>
<pre><code class="hljs language-bash">actions-usage \
    -org <span class="hljs-variable">$GITHUB_REPOSITORY_OWNER</span> \
    -days 28 \
    -by-repo \
    -punch-card \
    -token-file ~/PAT.txt
</code></pre>
<p>You'll need a Personal Access Token, there are instructions on how to create this in the <a href="https://github.com/self-actuated/actions-usage">actions-usage README file</a></p>
<p>There are many log lines printed to stderr during the scan of repositories and the workflows. You can omit all of this by adding <code>2> /dev/null </code> to the command.</p>
<p>First off we show the totals:</p>
<pre><code>Fetching last 28 days of data (created>=2023-05-19)

Generated by: https://github.com/self-actuated/actions-usage
Report for actuated-samples - last: 28 days.

Total repos: 24
Total private repos: 0
Total public repos: 24

Total workflow runs: 107
Total workflow jobs: 488

Total users: 1
</code></pre>
<p>Then break down on success/failure and cancelled jobs overall, plus the biggest and average build time:</p>
<pre><code>Success: 369/488
Failure: 45/488
Cancelled: 73/488

Longest build: 29m32s
Average build time: 1m26s
</code></pre>
<p>Next we have the day by day breakdown. You can see that we try to focus on our families on Sunday at OpenFaaS Ltd, instead of working:</p>
<pre><code>Day            Builds
Monday         61
Tuesday        50
Wednesday      103
Thursday       110
Friday         153
Saturday       10
Sunday         0
Total          488
</code></pre>
<p>Our customers in the Middle East work to a different week, and so you'd see Saturday with no builds or nothing, and Sunday like a normal working day.</p>
<p>Then we have the repo-by-repo breakdown with some much more granular data:</p>
<pre><code>Repo                                      Builds         Success        Failure        Cancelled      Skipped        Total          Average        Longest
actuated-samples/k3sup-matrix-test        355            273            20             62             0              2h59m1s        30s            1m29s
actuated-samples/discourse                49             38             7              4              0              6h37m21s       8m7s           20m1s
actuated-samples/specs                    35             31             1              3              0              10m20s         18s            32s
actuated-samples/cypress-test             17             4              13             0              0              6m23s          23s            49s
actuated-samples/cilium-test              9              4              2              3              0              1h10m41s       7m51s          29m32s
actuated-samples/kernel-builder-linux-6.0 9              9              0              0              0              11m28s         1m16s          1m27s
actuated-samples/actions-usage-job        8              4              2              1              0              46s            6s             11s
actuated-samples/faasd-nix                6              6              0              0              0              24m20s         4m3s           10m49s
</code></pre>
<p>Finally, we have the original value that the tool set out to display:</p>
<pre><code>Total usage: 11h40m20s (700 mins)
</code></pre>
<p>We display the value in a Go duration for readability and in minutes because that's the number that GitHub uses to talk about usage.</p>
<p>One customer told us that they were running into rate limits when querying for 28 days of data, so they dropped down to 14 days and then multiplied the result by two to get a rough estimate.</p>
<pre><code>-days 14 
</code></pre>
<p>The team at Todoist got in touch with us to see if actuated could reduce their bill on GitHub Actions. When he tried to run the tool the rate-limit was exhausted even when he changed the flag to <code>-days 1</code>. Why? They were using 550,000 minutes!</p>
<p>So we can see one of the limitations already of this approach. Fortunately, actuated customers have their job stats recorded in a database and can generate reports <a href="https://docs.actuated.dev/dashboard/">from the dashboard very quickly</a>.</p>
<h2>Finding out about your personal account</h2>
<p>Actuated isn't built for personal users, but for teams, so we didn't add this feature initially. Then we saw a few people reach out via Twitter and GitHub and decided to add it for them.</p>
<p>For your personal account, you only have to change one of the input parameters:</p>
<pre><code class="hljs language-bash">actions-usage \
    -user alexellis \
    -days 28 \
    -by-repo \
    -punch-card \
    -token-file ~/ae-pat.txt 2> /dev/null 
</code></pre>
<p>Now I actually have > 250 repositories and most of them don't even have Actions enabled, so this makes the tool less useful for me personally. So it was great when a community member suggested offering a way to filter repos when you have so many that the tool takes a long time to run or can't complete due to rate-limits.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Being that today I used it to get the same insights from a Github Org where I currently work, which contains 1.4k of repositories.<br><br>And this was running for a considerable time. I am only related to only a few repositories within this organization.</p>&mdash; lbrealdeveloper (@lbrealdeveloper) <a href="https://twitter.com/lbrealdeveloper/status/1669401439431544832?ref_src=twsrc%5Etfw">June 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I've already created an issue and have found someone who'd like to contribute the change: <a href="https://github.com/self-actuated/actions-usage/issues/8">Offer a way to filter repos for large organisations / users #8</a></p>
<p>This is the beauty of open source and community. We all get to benefit from each other's ideas and contributions.</p>
<h2>Running actions-usage with a GitHub Action</h2>
<p>Now this is my favourite way to run the tool. I can run it on a schedule and get a report sent to me via email or Slack.</p>
<p><img src="/images/2023-06-actions-usage/" alt="Example output from running the tool as a GitHub Action"></p>
<blockquote>
<p>Example output from running the tool as a GitHub Action</p>
</blockquote>
<p>Create <code>actions-usage.yaml</code> in your <code>.github/workflows</code> folder:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">actions-usage</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">main</span>
  <span class="hljs-attr">workflow_dispatch:</span>

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">actions-usage:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">daily-stats</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-any-1cpu-2gb</span>
    <span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">actions-usage:</span> <span class="hljs-string">latest</span>
        <span class="hljs-attr">print-summary:</span> <span class="hljs-literal">false</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Generate</span> <span class="hljs-string">actions-usage</span> <span class="hljs-string">report</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
       echo "### Actions Usage report by [actuated.dev](https://actuated.dev)" >> SUMMARY
       echo "\`\`\`" >> SUMMARY
       actions-usage \
        -org $GITHUB_REPOSITORY_OWNER \
        -days 1 \
        -by-repo \
        -punch-card \
        -token ${{ secrets.GITHUB_TOKEN }}  2> /dev/null  >> SUMMARY
       echo "\`\`\`" >> SUMMARY
       cat SUMMARY >> $GITHUB_STEP_SUMMARY
</span></code></pre>
<p>Ths is designed to run within an organisation, but you can change the <code>-org</code> flag to <code>-user</code> and then use your own username.</p>
<p>The days are for the past day of activity, but you can change this to any number like 7, 14 or 28 days.</p>
<p>You can learn about the other flags by running <code>actions-usage --help</code> on your own machine.</p>
<h2>Wrapping up</h2>
<p>actions-usage is a practical tool that we use with customers to get an idea of their usage and how we can help with actuated. That said, it's also a completely free and open source tool for which the community is finding their own set of use-cases.</p>
<p>And there are no worries about privacy, we've gone very low tech here. The output is only printed to the console, and we never receive any of your data unless you specifically copy and paste the output into an email.</p>
<p>Feel free to create an issue if you have a feature request or a question.</p>
<p>Check out <a href="https://self-actuated/actions-usage">self-actuated/actions-usage</a> on GitHub</p>
<p>I wrote an eBook writing CLIs like this in Go and keep it up to date on a regular basis - adding new examples and features of Go.</p>
<p>Why not check out what people are saying about it on Gumroad?</p>
<p><a href="https://store.openfaas.com/l/everyday-golang">Everyday Go - The Fast Track</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Secure CI for GitLab with Firecracker microVMs]]></title>
            <link>https://actuated.dev/blog/secure-microvm-ci-gitlab</link>
            <guid>https://actuated.dev/blog/secure-microvm-ci-gitlab</guid>
            <pubDate>Fri, 16 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how actuated for GitLab CI can help you secure your CI/CD pipelines with Firecracker.]]></description>
            <content:encoded><![CDATA[<p>We started building actuated for GitHub Actions because we at OpenFaaS Ltd had a need for: unmetered CI minutes, faster &#x26; more powerful x86 machines, native Arm builds and low maintenance CI builds.</p>
<p>And most importantly, we needed it to be low-maintenance, and securely isolated.</p>
<p>None of the solutions at the time could satisfy all of those requirements, and even today with GitHub adopting the community-based Kubernetes controller to run CI in Pods, there is still a lot lacking.</p>
<p>As we've gained more experience with customers who largely had the same needs as we did for GitHub Actions, we started to hear more and more from GitLab CI users. From large enterprise companies who are concerned about the security risks of running CI with privileged Docker containers, Docker socket binding (from the host!) or the flakey nature and slow speed of VFS with Docker In Docker (DIND).</p>
<blockquote>
<p>The <a href="https://docs.gitlab.com/runner/security/">GitLab docs have a stark warning</a> about using both of these approaches. It was no surprise that when a consultant at Thoughtworks reached out to me, he listed off the pain points and concerns that we'd set out to solve for GitHub Actions.</p>
<p>At KubeCon, I also spoke to several developers who worked at Deutsche Telekom who had numerous frustrations with the user-experience and management overhead of the Kubernetes executor.</p>
</blockquote>
<p>So with growing interest from customers, we built a solution for GitLab CI - just like we did for GitHub Actions. We're excited to share it with you today in tech preview.</p>
<p><img src="/images/2023-06-gitlab-preview/conceptual.png" alt="actuated for GitLab CI"></p>
<p>For every build that requires a runner, we will schedule and boot a complete system with Firecracker using Linux KVM for secure isolation. After the job is completed, the VM will be destroyed and removed from the GitLab instance.</p>
<p>actuated for GitLab is for self-hosted GitLab instances, whether hosted on-premises or on the public cloud.</p>
<p>If you'd like to use it or find out more, please apply here: <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Sign-up for the Actuated pilot</a></p>
<h2>Secure CI with Firecracker microVMs</h2>
<p><a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> is the open-source technology that provides isolation between tenants on certain AWS products like Lambda and Fargate. There's a growing number of cloud native solutions evolving around Firecracker, and we believe that it's the only way to run CI/CD securely.</p>
<p>Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel-based Virtual Machine (KVM) to create and manage microVMs. It's lightweight, fast, and most importantly, provides proper isolation, which anything based upon Docker cannot.</p>
<p>There are no horrible Kernel tricks or workarounds to be able to use user namespaces, no need to change your tooling from what developers love - Docker, to Kaninko or Buildah or similar.</p>
<p>You'll get <code>sudo</code>, plus a fresh Docker engine in every VM, booted up with systemd, so things like Kubernetes work out of the box, if you need them for end to end testing (as so many of us do these days).</p>
<p>You can learn the differences between VMs, containers and microVMs like Firecracker in my video from Cloud Native Rejekts at KubeCon Amsterdam:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pTQ_jVYhAoc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Many people have also told me that they learned how to use Firecracker from my webinar last year with Richard Case: <a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">A cracking time: Exploring Firecracker &#x26; MicroVMs</a>.</p>
<h2>Let's see it then</h2>
<p>Here's a video demo of the tech preview we have available for customers today.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PybSPduDT6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>You'll see that when I create a commit in our self-hosted copy of GitLab Enterprise, within 1 second a microVM is booting up and running the CI job.</p>
<p>Shortly after that the VM is destroyed which means there are absolutely no side-effects or any chance of leakage between jobs.</p>
<p>Here's a later demo of three jobs within a single pipeline, all set to run in parallel.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Here&#39;s 3x <a href="https://twitter.com/gitlab?ref_src=twsrc%5Etfw">@GitLab</a> CI jobs running in parallel within the same Pipeline demoed by <a href="https://twitter.com/alexellisuk?ref_src=twsrc%5Etfw">@alexellisuk</a> <br><br>All in their own ephemeral VM powered by Firecracker ðŸ”¥<a href="https://twitter.com/hashtag/cicd?src=hash&amp;ref_src=twsrc%5Etfw">#cicd</a> <a href="https://twitter.com/hashtag/secure?src=hash&amp;ref_src=twsrc%5Etfw">#secure</a> <a href="https://twitter.com/hashtag/isolation?src=hash&amp;ref_src=twsrc%5Etfw">#isolation</a> <a href="https://twitter.com/hashtag/microvm?src=hash&amp;ref_src=twsrc%5Etfw">#microvm</a> <a href="https://twitter.com/hashtag/baremetal?src=hash&amp;ref_src=twsrc%5Etfw">#baremetal</a> <a href="https://t.co/fe5HaxMsGB">pic.twitter.com/fe5HaxMsGB</a></p>&mdash; actuated (@selfactuated) <a href="https://twitter.com/selfactuated/status/1668575246952136704?ref_src=twsrc%5Etfw">June 13, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Everything's completed before I have a chance to even open the logs in the UI of GitLab.</p>
<h2>Wrapping up</h2>
<p>actuated for GitLab is for self-hosted GitLab instances, whether hosted on-premises or on the public cloud.</p>
<p>Here's what we bring to the table:</p>
<ul>
<li>ðŸš€ Faster x86 builds</li>
<li>ðŸš€ Secure isolation with Firecracker microVMs</li>
<li>ðŸš€ Native Arm builds that can actually finish</li>
<li>ðŸš€ Fixed-costs &#x26; less management</li>
<li>ðŸš€ Insights into CI usage across your organisation</li>
</ul>
<p>Runners are registered and running a job in a dedicated VM within less than one second. Our scheduler can pack in jobs across a fleet of servers, <a href="https://docs.actuated.dev/provision-server/">they just need to have KVM available</a>.</p>
<p>If you think your automation for runners could be improved, or work with customers who need faster builds, better isolation or Arm support, get in touch with us.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Sign-up for the Actuated pilot</a></li>
<li><a href="https://docs.actuated.dev/">Browse the docs and FAQ for actuated for GitHub Actions</a></li>
<li><a href="https://actuated.dev/">Read customer testimonials</a></li>
</ul>
<p>You can follow <a href="https://twitter.com/selfactuated">@selfactuated</a> on Twitter, or find <a href="https://twitter.com/alexellisuk">me there too</a> to keep an eye on what we're building.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Faster Nix builds with GitHub Actions and actuated]]></title>
            <link>https://actuated.dev/blog/faster-nix-builds</link>
            <guid>https://actuated.dev/blog/faster-nix-builds</guid>
            <pubDate>Mon, 12 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Speed up your Nix project builds on GitHub Actions with runners powered by Firecracker.]]></description>
            <content:encoded><![CDATA[<p><a href="https://github.com/openfaas/faasd">faasd</a> is a lightweight and portable version of <a href="https://www.openfaas.com/">OpenFaaS</a> that was created to run on a single host. In my spare time I maintain <a href="https://github.com/welteki/faasd-nix">faasd-nix</a>, a project that packages faasd and exposes a NixOS module so it can be run with NixOS.</p>
<p>The module itself depends on faasd, containerd and the CNI plugins and all of these binaries are built in CI with Nix and then cached using <a href="https://www.cachix.org/">Cachix</a> to save time on subsequent builds.</p>
<p>I often deploy faasd with NixOS on a Raspberry Pi and to the cloud, so I build binaries for both <code>x86_64</code> and <code>aarch64</code>. The build usually runs on the default GitHub hosted action runners. Now because GitHub currently doesn't have Arm support, I use QEMU instead which can emulate them. The drawback of this approach is that builds can sometimes be several times slower.</p>
<blockquote>
<p>For some of our customers, their builds couldn't even complete in 6 hours using QEMU, and only took between 5-20 minutes using native Arm hardware. Alex Ellis, Founder of Actuated.</p>
</blockquote>
<p>While upgrading to the latest nixpkgs release recently I decided to try and build the project on runners managed with Actuated to see the improvements that can be made by switching to both bigger <code>x86_64</code> iron and native Arm hardware.</p>
<h2>Nix and GitHub actions</h2>
<p>One of the features Nix offers are reproducible builds. Once a package is declared it can be built on any system. There is no need to prepare your machine with all the build dependencies. The only requirement is that Nix is installed.</p>
<blockquote>
<p>If you are new to Nix, then I'd recommend you read the <a href="https://zero-to-nix.com/start/install">Zero to Nix</a> guide. It's what got me excited about the project.</p>
</blockquote>
<p>Because Nix is declarative and offers reproducible builds, it is easy to setup a concise build pipeline for GitHub actions. A lot of steps usually required to setup the build environment can be left out. For instance, faasd requires Go, but there's no need to install it onto the build machine, and you'd normally have to install btrfs-progs to build containerd, but that's not something you have to think about, because Nix will take care of it for you.</p>
<p>Another advantage of the reproducible builds is that if it works on your local machine it most likely also works in CI. No need to debug and find any discrepancies between your local and CI environment.</p>
<blockquote>
<p>Of course, if you ever do get frustrated and want to debug a build, you can use the built-in <a href="https://docs.actuated.dev/tasks/debug-ssh/">SSH feature in Actuated</a>. Alex Ellis, Founder of Actuated.</p>
</blockquote>
<p>This is what the workflow looks like for building faasd and its related packages:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v21</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">faasd</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">containerd</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd-containerd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">cni-plugin</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd-cni-plugins
</span></code></pre>
<p>All this pipeline does is install Nix, using the <a href="https://github.com/marketplace/actions/install-nix">cachix/install-nix-action</a> and run the <code>nix build</code> command for the packages that need to be built.</p>
<h2>Notes on the nix build for aarch64</h2>
<p>To build the packages for multiple architectures there are a couple of options:</p>
<ul>
<li>cross-compiling, Nix has great support for cross compilation.</li>
<li>compiling through binfmt QEMU.</li>
<li>compiling natively on an aarch64 machine.</li>
</ul>
<p>The preferred option would be to compile everything natively on an aarch64 machine as that would result in the best performance. However, at the time of writing GitHub does not provide Arm runners. That is why QEMU is used by many people to compile binaries in CI.</p>
<p>Enabling the binfmt wrapper on NixOS can be done easily through the NixOS configuration. On non-NixOS machines, like on the GitHub runner VM, the QEMU static binaries need to be installed and the Nix daemon configuration updated.</p>
<p>Instructions to configure Nix for compilation with QEMU can be found on the <a href="https://nixos.wiki/wiki/NixOS_on_ARM#Compiling_through_binfmt_QEMU">NixOS wiki</a></p>
<p>The workflow for building aarch64 packages with QEMU on GitHub Actions looks like this:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-qemu-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v21</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">extra_nix_config:</span> <span class="hljs-string">|
            extra-platforms = aarch64-linux
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">faasd</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">containerd</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd-containerd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">cni-plugin</span> <span class="hljs-string">ðŸ”§</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd-cni-plugins
</span>
</code></pre>
<p>Install the QEMU static binaries using <a href="https://github.com/docker/setup-qemu-action">docker/setup-qemu-action</a>. Let the nix daemon know that it can build for aarch64 by adding <code>extra-platforms = aarch64-linux</code> via the <code>extra_nix_config</code> input on the install nix action. Update the nix build commands to specify platform e.g. <code>nix build .#packages.aarch64-linux.faasd</code>.</p>
<h2>Speeding up the build with a Raspberry Pi</h2>
<p>Nix has great support for caching and build speeds can be improved greatly by never building things twice. This project normally uses <a href="https://www.cachix.org/">Cachix</a> for caching and charing binaries across systems. For this comparison caching was disabled. All packages and their dependencies are built from scratch again each time.</p>
<p>Building the project takes around 4 minutes and 20 seconds on the standard GitHub hosted runner. After switching to a more powerful Actuated runner with 4CPUs and 8GB of RAM the build time dropped to 2 minutes and 15 seconds.</p>
<p><img src="/images/2023-06-faster-nix-builds/amd64-build-comparison.png" alt="Comparison of more powerful Actuated runner with GitHub hosted runner"></p>
<blockquote>
<p>Comparison of more powerful Actuated runner with GitHub hosted runner.</p>
</blockquote>
<p>While build times are still acceptable for x86_64 this is not the case for the aarch64 build. It takes around 55 minutes to complete the Arm build with QEMU on a GitHub runner.</p>
<p>Running the same build with QEMU on the Actuated runner already brings down the build time to 19 minutes and 40 seconds. Running the build natively on a Raspberry Pi 4 (8GB) completed in 11 minutes and 47 seconds. Building on a more powerful Arm machine would potentially reduce this time to a couple of minutes.</p>
<p><img src="/images/2023-06-faster-nix-builds/arm64-build-comparison.png" alt="Results of the matrix build comparing the GitHub hosted runner and the 2 Actuated runners"></p>
<blockquote>
<p>Results of the matrix build comparing the GitHub hosted runner and the 2 Actuated runners.</p>
</blockquote>
<p>Running the build natively on the Pi did even beat the fast bare-metal machine that is using QEMU.</p>
<p>My colleague Alex ran the same build on his Raspberry Pi using Actuated and an NVMe mounted over USB-C, he got the build time down even further. Why? Because it increased the I/O performance. In fact, if you build this on server-grade Arm like the Ampere Altra, it would be about 4x faster than the Pi 4.</p>
<p>Building for Arm:</p>
<ul>
<li>Alex's Raspberry Pi with NVMe: 10m49s</li>
<li>An Ampere Altra on Equinix Metal: 3m29s</li>
</ul>
<p>Building for x86_64</p>
<ul>
<li>AMD Epyc on Equinix Metal: 1m57s</li>
</ul>
<p><img src="/images/2023-06-faster-nix-builds/alex-results.png" alt="Alex&#x27;s results"></p>
<p>These results show that whatever the Arm hardware you pick, it'll likely be faster than QEMU, even when QEMU is run on the fastest bare-metal available, the slowest Arm hardware will beat it by minutes.</p>
<h2>Wrapping up</h2>
<p>Building your projects with Nix allows your GitHub actions pipelines to be concise and easy to maintain.</p>
<p>Even when you are not using Nix to build your project it can still help you to create concise and easy to maintain GitHub Action workflows. With Nix shell environments you can use Nix to declare which dependencies you want to make available inside an isolated shell environment for your project: <a href="https://determinate.systems/posts/nix-github-actions">Streamline your GitHub Actions dependencies using Nix</a></p>
<p>Building Nix packages or entire NixOS systems on GitHub Actions can be slow especially if you need to build for Arm. Bringing your own metal to GitHub actions can speed up your builds. If you need Arm runners, Actuated is one of the only options for securely isolated CI that is safe for Open Source and public repositories. Alex explains why in: <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></p>
<p>Another powerful feature of the Nix ecosystem is the ability to run integration tests using virtual machines (NixOS test). This feature requires hardware acceleration to be available in the CI runner. Actuated makes it possible to run these tests in GitHub Actions CI pipelines: <a href="https://actuated.dev/blog/kvm-in-github-actions">how to run KVM guests in your GitHub Actions</a>.</p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions">Bring Your Own Metal - Case Study with GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fixing the cache latency for self-hosted GitHub Actions]]></title>
            <link>https://actuated.dev/blog/faster-self-hosted-cache</link>
            <guid>https://actuated.dev/blog/faster-self-hosted-cache</guid>
            <pubDate>Wed, 24 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[The cache for GitHub Actions can speed up CI/CD pipelines. But what about when it slows you down?]]></description>
            <content:encoded><![CDATA[<p>In some of our builds for actuated we cache things like the Linux Kernel, so we don't needlessly rebuild it when we update packages in our base images. It can shave minutes off every build meaning our servers can be used more efficiently. Most customers we've seen so far only make light to modest use of GitHub's hosted cache, so haven't noticed much of a latency problem.</p>
<p>But you don't have to spend too long on the <a href="https://github.com/actions/cache/issues?q=is%3Aissue+cache+slow+">issuer tracker for GitHub Actions</a> to find people complaining about the cache being slow or locking up completely for self-hosted runners.</p>
<p>Go, Rust, Python and other languages don't tend to make heavy use of caches, and Docker has some of its own mechanisms like building cached steps into published images aka <em><a href="https://docs.docker.com/build/cache/backends/inline/">inline caching</a></em>. But for the Node.js ecosystem, the <code>node_modules</code> folder and yarn cache can become huge and take a long time to download. That's one place where you may start to see tension between the speed of self-hosted runners and the latency of the cache. If your repository is a monorepo or has lots of large artifacts, you may get a speed boost by caching that too.</p>
<p>So why is GitHub's cache so fast for hosted runners, and (sometimes) so slow self-hosted runners?</p>
<p>Simply put - GitHub runs VMs and the accompanying cache on the same network, so they can talk over a high speed backbone connection. But when you run a self-hosted runner, then any download or upload operations are taking place over the public Internet.</p>
<p>Something else that can slow builds down is having to download large base images from the Docker Hub. We've already <a href="https://docs.actuated.dev/tasks/registry-mirror/">covered how to solve that for actuated in the docs</a>.</p>
<h2>Speeding up in the real world</h2>
<p>We recently worked with Roderik, the CTO of <a href="https://settlemint.com">SettleMint</a> to migrate their CI from a self-hosted Kubernetes solution Actions Runtime Controller (ARC) to actuated. He told me that they originally moved from GitHub's hosted runners to ARC to save money, increase speed and to lower the latency of their builds. Unfortunately, running container builds within Kubernetes provided very poor isolation, and side effects were being left over between builds, even with a pool of ephemeral containers. They also wanted to reduce the amount of effort required to maintain a Kubernetes cluster and control-plane for CI.</p>
<p>Roderik explained that he'd been able to get times down by using <a href="https://pnpm.io">pnpm</a> instead of yarn, and said every Node project should try it out to see the speed increases. He believes the main improvement is due to efficient downloading and caching. pnpm is a drop-in replacement for npm and yarn, and is compatible with both.</p>
<blockquote>
<p>In some cases, we found that downloading dependencies from the Internet was faster than using GitHub's remote cache. The speed for a hosted runner was often over 100MBs/sec, but for a self-hosted runner it was closer to 20MBs/sec.</p>
</blockquote>
<p>That's when we started to look into how we could run a cache directly on the same network as our self-hosted runners, or even on the machine that was scheduling the Firecracker VMs.</p>
<blockquote>
<p>"With the local cache that Alex helped us set up, the cache is almost instantaneous. It doesn't even have time to show a progress bar."</p>
</blockquote>
<p>Long story short, SettleMint have successfully migrated their CI for x86 and Arm to actuated for the whole developer team:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Super happy with my new self hosted GHA runners powered by <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a>, native speeds on both AMD and ARM bare metal monster machines. Our CI now goes brrrrâ€¦ <a href="https://t.co/quZ4qfcLmu">pic.twitter.com/quZ4qfcLmu</a></p>&mdash; roderik.eth (@r0derik) <a href="https://twitter.com/r0derik/status/1661109934346346510?ref_src=twsrc%5Etfw">May 23, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This post is about speed improvements for caching, but if you're finding that QEMU is too slow to build your Arm containers on hosted runners, you may benefit from switching to actuated with bare-metal Arm servers.</p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/how-to-run-multi-arch-builds-natively">How to split up multi-arch Docker builds to run natively</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>
<h2>Set up a self-hosted cache for GitHub Actions</h2>
<p>In order to set up a self-hosted cache for GitHub Actions, we switched out the official <a href="https://github.com/actions/cache">actions/cache@v3</a> action for <a href="https://github.com/tespkg/actions-cache">tespkg/actions-cache@v1</a> created by Target Energy Solutions, a UK-based company, which can target S3 instead of the proprietary GitHub cache.</p>
<p>We then had to chose between Seaweedfs and Minio for the self-hosted S3 server. Of course, there's also nothing stopping you from actually using AWS S3, or Google Cloud Storage, or another hosted service.</p>
<p>Then, the question was - should we run the S3 service directly on the server that was running Firecracker VMs, for ultimate near-loopback speed, or on a machine provisioned in the same region, just like GitHub does with Azure?</p>
<p>Either would be a fine option. If you decide to host a public S3 cache, make sure that authentication and TLS are both enabled. You may also want to set up an IP whitelist just to deter any bots that may scan for public endpoints.</p>
<h3>Set up Seaweedfs</h3>
<p>The <a href="https://github.com/seaweedfs/seaweedfs">Seaweedfs</a> README describes the project as:</p>
<blockquote>
<p>"a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, cross-DC active-active replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding."</p>
</blockquote>
<p>We liked it so much that we'd already added it to the arkade marketplace, arkade is a faster, developer-focused alternative to brew.</p>
<pre><code class="hljs language-bash">arkade get seaweedfs
sudo <span class="hljs-built_in">mv</span> ~/.arkade/bin/seaweedfs /usr/local/bin
</code></pre>
<p>Define a secret key and access key to be used from the CI jobs in the <code>/etc/seaweedfs/s3.conf</code> file:</p>
<pre><code>{
  "identities": [
    {
      "name": "actuated",
      "credentials": [
        {
          "accessKey": "s3cr3t",
          "secretKey": "s3cr3t"
        }
      ],
      "actions": [
        "Admin",
        "Read",
        "List",
        "Tagging",
        "Write"
      ]
    }
  ]
}
</code></pre>
<p>Create <code>seaweedfs.service</code>:</p>
<pre><code>[Unit]
Description=SeaweedFS
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/seaweedfs server -ip=192.168.128.1 -volume.max=0 -volume.fileSizeLimitMB=2048 -dir=/home/runner-cache -s3 -s3.config=/etc/seaweedfs/s3.conf
Restart=on-failure

[Install]
WantedBy=multi-user.target
</code></pre>
<p>We have set <code>-volume.max=0 -volume.fileSizeLimitMB=2048</code> to minimize the amount of space used and to allow large zip files of up to 2GB, but you can change this to suit your needs. See <code>seaweedfs server --help</code> for more options.</p>
<p>Install it and check that it started:</p>
<pre><code>sudo cp ./seaweedfs.service /etc/systemd/system/seaweedfs.service
sudo systemctl enable seaweedfs

sudo journalctl -u seaweedfs -f
</code></pre>
<h2>Try it out</h2>
<p>You'll need to decide what you want to cache and whether you want to use a hosted, or self-hosted S3 service - either directly on the actuated server or on a separate machine in the same region.</p>
<p>Roderik explained that the pnpm cache was important for node_modules, but that actually caching the git checkout saved a lot of time too. So he added both into his builds.</p>
<p>Here's an example:</p>
<pre><code class="hljs language-yaml">    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"Set current date as env variable"</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        echo "CHECKOUT_DATE=$(date +'%V-%Y')" >> $GITHUB_ENV
</span>      <span class="hljs-attr">id:</span> <span class="hljs-string">date</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">tespkg/actions-cache@v1</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">endpoint:</span> <span class="hljs-string">"192.168.128.1"</span>
        <span class="hljs-attr">port:</span> <span class="hljs-number">8333</span>
        <span class="hljs-attr">insecure:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">accessKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">secretKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">bucket:</span> <span class="hljs-string">actuated-runners</span>
        <span class="hljs-attr">region:</span> <span class="hljs-string">local</span>
        <span class="hljs-attr">use-fallback:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">path:</span> <span class="hljs-string">./.git</span>
        <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-checkout-${{</span> <span class="hljs-string">env.CHECKOUT_DATE</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
          ${{ runner.os }}-checkout-
</span></code></pre>
<ul>
<li><code>use-fallback</code> - option means that if seaweedfs is not installed on the host, or is inaccessible, the action will fall back to using the GitHub cache.</li>
<li><code>key</code> - as per GitHub's action - created when saving a cache and the key used to search for a cache</li>
<li><code>restore-keys</code> - as per GitHub's action - if no cache hit occurs for key, these restore keys are used sequentially in the order provided to find and restore a cache.</li>
<li><code>bucket</code> - the name of the bucket to use in seaweedfs</li>
<li><code>accessKey</code> and <code>secretKey</code> - the credentials to use to access the bucket - we'd recommend using an organisation-level secret for this</li>
<li><code>endpoint</code> - the IP address <code>192.168.128.1</code> refers to the host machine where the Firecracker VM is running</li>
</ul>
<p>See also: <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows">Official GitHub Actions Cache action</a></p>
<p>You may also want to create a self-signed certificate for the S3 service and then set <code>insecure: false</code> to ensure that the connection is encrypted. If you're running these builds within private repositories, tampering is unlikely.</p>
<p>Roderik explained that the cache key uses a week-year format, rather than a SHA. Why? Because a SHA would change on every build, meaning that a save and load would be performed on every build, using up more space and slowing things down. In this example, There's only ever 52 cache entries per year.</p>
<blockquote>
<p>You define a key which is unique if the cache needs to be updated. Then you define a restore key that matches part or all of the key.
Part means it takes the last one that matches, then updates at the end of the run, in the post part, it then uses the key to upload the zip file if the key is different from the one stored.</p>
</blockquote>
<p>In one instance, a cached checkout went from 2m40s to 11s. That kind of time saving adds up quickly if you have a lot of builds.</p>
<p>Roderik's pipeline has multiple steps, and may need to run multiple times, so we're looking at 55s instead of 13 minutes for 5 jobs or runs.</p>
<p><img src="/images/2023-05-faster-cache/SettleMint.png" alt="Example pipeline"></p>
<blockquote>
<p>One of the team's pipelines</p>
</blockquote>
<p>Here's how to enable a cache for <code>pnpm</code>:</p>
<pre><code class="hljs language-yaml">    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">PNPM</span>
      <span class="hljs-attr">uses:</span> <span class="hljs-string">pnpm/action-setup@v2</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">run_install:</span> <span class="hljs-string">|
          - args: [--global, node-gyp]
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">pnpm</span> <span class="hljs-string">store</span> <span class="hljs-string">directory</span>
      <span class="hljs-attr">id:</span> <span class="hljs-string">pnpm-cache</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">tespkg/actions-cache@v1</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">endpoint:</span> <span class="hljs-string">"192.168.128.1"</span>
        <span class="hljs-attr">port:</span> <span class="hljs-number">8333</span>
        <span class="hljs-attr">insecure:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">accessKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">secretKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">bucket:</span> <span class="hljs-string">actuated-runners</span>
        <span class="hljs-attr">region:</span> <span class="hljs-string">local</span>
        <span class="hljs-attr">use-fallback:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">path:</span>
          <span class="hljs-string">${{</span> <span class="hljs-string">steps.pnpm-cache.outputs.STORE_PATH</span> <span class="hljs-string">}}</span>
          <span class="hljs-string">~/.cache</span>
          <span class="hljs-string">.cache</span>
        <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-pnpm-store-${{</span> <span class="hljs-string">hashFiles('**/pnpm-lock.yaml')</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
          ${{ runner.os }}-pnpm-store-
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">dependencies</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        pnpm install --frozen-lockfile --prefer-offline
</span>      <span class="hljs-attr">env:</span>
        <span class="hljs-attr">HUSKY:</span> <span class="hljs-string">'0'</span>
        <span class="hljs-attr">NODE_ENV:</span> <span class="hljs-string">development</span>
</code></pre>
<p>Picking a good key and restore key can help optimize when the cache is read from and written to:</p>
<blockquote>
<p>"You need to determine a good key and restore key. For pnpm, we use the hash of the lock file in the key, but leave it out of the restore key. So if I update the lock file, it starts from the last cache, updates it, and stores the new cache with the new hash"</p>
</blockquote>
<p>If you'd like a good starting-point for GitHub Actions Caching, Han Verstraete from our team wrote up a good primer for the actuated docs:</p>
<p><a href="https://docs.actuated.dev/examples/github-actions-cache/">Example: GitHub Actions cache</a></p>
<h2>Conclusion</h2>
<p>We were able to dramatically speed up caching for GitHub Actions by using a self-hosted S3 service. We used Seaweedfs directly on the server running Firecracker with a fallback to GitHub's cache if the S3 service was unavailable.</p>
<p><a href="https://twitter.com/alexellisuk/status/1661282581617229827/"><img src="https://pbs.twimg.com/media/Fw4PQEfWwAIl-6u?format=jpg&#x26;name=medium" alt="Brr"></a></p>
<blockquote>
<p>An <a href="https://amperecomputing.com/en/">Ampere</a> Altra Arm server running parallel VMs using Firecracker. The CPU is going brr. <a href="https://docs.actuated.dev/provision-server/">Find a server with our guide</a></p>
</blockquote>
<p>We also tend to recommend that all customers enable a mirror of the Docker Hub to counter restrictive rate-limits. The other reason is to avoid any penalties that you'd see from downloading large base images - or from downloading small to medium sized images when running in high concurrency.</p>
<p>You can find out how to configure a container mirror for the Docker Hub using actuated here: <a href="https://docs.actuated.dev/tasks/registry-mirror/">Set up a registry mirror</a>. When testing builds for the <a href="https://github.com/discourse/discourse">Discourse</a> team, there was a 2.5GB container image used for UI testing with various browsers preinstalled within it. We found that we could shave off a few minutes off the build time by using the local mirror. Imagine 10x of those builds running at once, needlessly downloading 250GB of data.</p>
<p>What if you're not an actuated customer? Can you still benefit from a faster cache? You could try out a hosted service like AWS S3 or Google Cloud Storage, provisioned in a region closer to your runners. The speed probably won't quite be as good, but it should still be a lot faster than reaching over the Internet to GitHub's cache.</p>
<p>If you'd like to try out actuated for your team, <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">reach out to us to find out more</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Book 20 mins with me if you think your team could benefit from the below for GitHub Actions:<br><br>ðŸš€ Insights into CI usage across your organisation<br>ðŸš€ Faster x86 builds<br>ðŸš€ Native Arm builds that can actually finish<br>ðŸš€ Fixed-costs &amp; less management<a href="https://t.co/iTiZsH9pgv">https://t.co/iTiZsH9pgv</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1656300308325179393?ref_src=twsrc%5Etfw">May 10, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Keyless deployment to OpenFaaS with OIDC and GitHub Actions]]></title>
            <link>https://actuated.dev/blog/oidc-proxy-for-openfaas</link>
            <guid>https://actuated.dev/blog/oidc-proxy-for-openfaas</guid>
            <pubDate>Fri, 05 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[We're announcing a new OIDC proxy for OpenFaaS for keyless deployments from GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>In 2021, GitHub released <a href="https://openid.net/connect/">OpenID Connect (OIDC)</a> support for CI jobs running under GitHub Actions. This was a huge step forward for security meaning that any GitHub Action could mint an OIDC token and use it to securely federate into another system without having to store long-lived credentials in the repository.</p>
<p>I wrote a prototype for OpenFaaS shortly after the announcement and a deep dive explaining how it works. I used <a href="https://inlets.dev/blog/2022/11/16/automate-a-self-hosted-https-tunnel.html">inlets</a> to set up a HTTPS tunnel, and send the token to my machine for inspection. Various individuals and technical teams have used my content as <a href="https://twitter.com/mlbiam/status/1653391969110900741?s=20">a reference guide</a> when working with GitHub Actions and OIDC.</p>
<p>See the article: <a href="https://blog.alexellis.io/deploy-without-credentials-using-oidc-and-github-actions/">Deploy without credentials with GitHub Actions and OIDC</a></p>
<p>Since then, custom actions for GCP, AWS and Azure have been created which allow an OIDC token from a GitHub Action to be exchanged for a short-lived access token for their API - meaning you can manage cloud resources securely. For example, see: <a href="https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services">Configuring OpenID Connect in Amazon Web Services</a> - we have actuated customers who use this approach to deploy to ECR from their self-hosted runners without having to store long-lived credentials in their repositories.</p>
<h2>Why OIDC is important for OpenFaaS customers</h2>
<p>Before we talk about the new OIDC proxy for OpenFaaS, I should say that <a href="https://openfaas.com/pricing">OpenFaaS Enterprise</a> <em>also has</em> an IAM feature which includes OIDC support for the CLI, dashboard and API. It supports any trusted OIDC provider, not just GitHub Actions. Rather than acting as a proxy, it actually implements a full fine-grained authorization and permissions policy language that resembles the one you'll be used to from AWS.</p>
<p>However, not everyone needs this level of granularity.</p>
<p><a href="https://il.linkedin.com/in/shaked-askayo-18403714a">Shaked, the CTO of Kubiya.ai</a> is an <a href="https://openfaas.com/">OpenFaaS</a> &#x26; <a href="https://inlets.dev">inlets</a> customer. His team at <a href="https://kubiya.ai">Kubiya</a> is building a conversational AI for DevOps - if you're ever tried ChatGPT, imagine that it was hooked up to your infrastructure and had superpowers. On a recent call, he told me that their team now has 30 different repositories which deploy OpenFaaS functions to their various AWS EKS clusters. That means that a secret has to be maintained at the organisation level and then consumed via <code>faas-cli login</code> in each job.</p>
<p>It gets a little worse for them - because different branches deploy to different OpenFaaS gateways and to different EKS clusters.</p>
<p>In addition to managing various credentials for each cluster they add - they were uncomfortable with exposing all of their functions on the Internet.</p>
<p>So today the team working on actuated is releasing a new OIDC proxy which can be deployed to any OpenFaaS cluster to avoid the need to manage and share long-lived credentials with GitHub.</p>
<p><img src="/images/2023-05-openfaas-oidc-proxy/conceptual.png" alt="Conceptual design of the OIDC proxy for OpenFaaS"></p>
<blockquote>
<p>Conceptual design of the OIDC proxy for OpenFaaS</p>
</blockquote>
<p><strong>About the OIDC proxy for OpenFaaS</strong></p>
<ul>
<li>It can be deployed via Helm</li>
<li>Only allows access to a given set of GitHub organisations</li>
<li>Uses a standard Ingress record</li>
<li>It only accepts OIDC tokens signed from GitHub Actions</li>
<li>It only allows requests to the <code>/system</code> endpoints of the OpenFaaS REST API - keeping your functions safe</li>
</ul>
<p>Best of all, unlike OpenFaaS Enterprise, it's free for all actuated customers - whether they're using OpenFaaS CE, Standard or Enterprise.</p>
<p>Here's what Shaked had to say about the new proxy:</p>
<blockquote>
<p>That's great - thank you! Looking forward to it as it will simplify our usage of the openfaas templates and will speed up our development process
Shaked, CTO, Kubiya.ai</p>
</blockquote>
<h2>How to deploy the proxy for OpenFaaS</h2>
<p>Here's what you need to do:</p>
<ul>
<li>Install OpenFaaS on a Kubernetes cluster</li>
<li>Create a new DNS A or CNAME record for the Ingress to the proxy i.e. <code>oidc-proxy.example.com</code></li>
<li>Install the OIDC proxy using Helm</li>
<li>Update your GitHub Actions workflow to use the OIDC token</li>
</ul>
<blockquote>
<p>My cluster is not publicly exposed on the Internet, so I'm using an <a href="https://inlets.dev/blog/2022/11/16/automate-a-self-hosted-https-tunnel.html">inlets tunnel</a> to expose the OIDC Proxy from my local KinD cluster. I'll be using the domain <code>minty.exit.o6s.io</code> but you'd create something more like <code>oidc-proxy.example.com</code> for your own cluster.</p>
</blockquote>
<p>First Set up your values.yaml for Helm:</p>
<pre><code class="hljs language-yaml"><span class="hljs-comment"># The public URL to access the proxy</span>
<span class="hljs-attr">publicURL:</span> <span class="hljs-string">https://oidc-proxy.example.com</span>

<span class="hljs-comment"># Comma separated list of repository owners for which short-lived OIDC tokens are authorized.</span>
<span class="hljs-comment"># For example: alexellis,self-actuated</span>
<span class="hljs-attr">repositoryOwners:</span> <span class="hljs-string">'alexellis,self-actuated'</span>
<span class="hljs-attr">ingress:</span>
    <span class="hljs-attr">host:</span> <span class="hljs-string">oidc-proxy.example.com</span>
    <span class="hljs-attr">issuer:</span> <span class="hljs-string">letsencrypt-prod</span>
</code></pre>
<p>The chart will create an Ingress record for you using an existing issuer. If you want to use something else like Inlets or Istio to expose the OIDC proxy, then simply set <code>enabled: false</code> under the <code>ingress:</code> section.</p>
<p>Create a secret for the actuated subscription key:</p>
<pre><code class="hljs language-bash">kubectl create secret generic actuated-license \
  -n openfaas \
  --from-file=actuated-license=<span class="hljs-variable">$HOME</span>/.actuated/LICENSE
</code></pre>
<p>Then run:</p>
<pre><code class="hljs language-bash">helm repo add actuated https://self-actuated.github.io/charts/
helm repo update

helm upgrade --install actuated/openfaas-oidc-proxy \
    -f ./values.yaml
</code></pre>
<p>For the full setup - <a href="https://github.com/self-actuated/charts/tree/master/chart/openfaas-oidc-proxy">see the README for the Helm chart</a></p>
<p>You can now go to one of your repositories and update the workflow to authenticate to the REST API via an OIDC token.</p>
<p>In order to get an OIDC token within a build, add the <code>id_token: write</code> permission to the permissions list.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">keyless_deploy</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">workflow_dispatch:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">'*'</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">keyless_deploy:</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">contents:</span> <span class="hljs-string">'read'</span>
      <span class="hljs-attr">id-token:</span> <span class="hljs-string">'write'</span>
</code></pre>
<p>Then set <code>runs-on</code> to <code>actuated</code> to use your faster actuated servers:</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">-   runs-on: ubuntu-latest</span>
<span class="hljs-addition">+   runs-on: actuated</span>
</code></pre>
<p>Then in the workflow, install the OpenFaaS CLI:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
    <span class="hljs-attr">with:</span>
        <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">faas-cli</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">curl</span> <span class="hljs-string">-sLS</span> <span class="hljs-string">https://cli.openfaas.com</span> <span class="hljs-string">|</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sh</span>
</code></pre>
<p>Then get a token:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">|
        OPENFAAS_URL=https://minty.exit.o6s.io
        OIDC_TOKEN=$(curl -sLS "${ACTIONS_ID_TOKEN_REQUEST_URL}&#x26;audience=$OPENFAAS_URL" -H "User-Agent: actions/oidc-client" -H "Authorization: Bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN")
        JWT=$(echo $OIDC_TOKEN | jq -j '.value')
</span></code></pre>
<p>Finally, use the token whenever you need it by passing in the <code>--token</code> flag to any of the <code>faas-cli</code> commands:</p>
<pre><code class="hljs language-yaml"><span class="hljs-string">faas-cli</span> <span class="hljs-string">list</span> <span class="hljs-string">-n</span> <span class="hljs-string">openfaas-fn</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
<span class="hljs-string">faas-cli</span> <span class="hljs-string">ns</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
<span class="hljs-string">faas-cli</span> <span class="hljs-string">store</span> <span class="hljs-string">deploy</span> <span class="hljs-string">printer</span> <span class="hljs-string">--name</span> <span class="hljs-string">p1</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>

<span class="hljs-string">faas-cli</span> <span class="hljs-string">describe</span> <span class="hljs-string">p1</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
</code></pre>
<p>Since we have a lot of experience with GitHub Actions, we decided to make the above simpler by creating a custom <a href="https://docs.github.com/en/actions/creating-actions/creating-a-composite-action">Composite Action</a>. If you check out the code for <a href="https://github.com/self-actuated/openfaas-oidc">self-actuated/openfaas-oidc</a> you'll see that it obtains a token, then writes it into an openfaaas config file, so that the <code>--token</code> flag isn't required.</p>
<p>Here's how it changes:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/openfaas-oidc@v1</span>
  <span class="hljs-attr">with:</span> 
    <span class="hljs-attr">gateway:</span> <span class="hljs-string">https://minty.exit.o6s.io</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Check</span> <span class="hljs-string">OpenFaaS</span> <span class="hljs-string">version</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">|
    OPENFAAS_CONFIG=$HOME/.openfaas/
    faas-cli version
</span></code></pre>
<p>Here's the complete example:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">federate</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">workflow_dispatch:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">'*'</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">auth:</span>
    <span class="hljs-comment"># Add "id-token" with the intended permissions.</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">contents:</span> <span class="hljs-string">'read'</span>
      <span class="hljs-attr">id-token:</span> <span class="hljs-string">'write'</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">faas-cli</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">curl</span> <span class="hljs-string">-sLS</span> <span class="hljs-string">https://cli.openfaas.com</span> <span class="hljs-string">|</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sh</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/openfaas-oidc@v1</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">gateway:</span> <span class="hljs-string">https://minty.exit.o6s.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          export OPENFAAS_URL=https://minty.exit.o6s.io
          faas-cli store deploy env --name http-header-printer
          faas-cli list
</span></code></pre>
<p>How can we be sure that our functions cannot be invoked over the proxy?</p>
<p>Just add an extra line to test it out:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          export OPENFAAS_URL=https://minty.exit.o6s.io
          faas-cli store deploy env --name http-header-printer
          sleep 5
</span>
          <span class="hljs-string">echo</span> <span class="hljs-string">|</span> <span class="hljs-string">faas-cli</span> <span class="hljs-string">invoke</span> <span class="hljs-string">http-header-printer</span>
</code></pre>
<p><img src="/images/2023-05-openfaas-oidc-proxy/failed-invoke.png" alt="A failed invocation over the proxy"></p>
<blockquote>
<p>A failed invocation over the proxy</p>
</blockquote>
<p>Best of all, now that you're using OIDC, you can now go and delete any of those long lived basic auth credentials from your secrets!</p>
<h2>Wrapping up</h2>
<p>The new OIDC proxy for OpenFaaS is available for all actuated customers and works with OpenFaaS CE, Standard and Enterprise. You can use it on as many clusters as you like, whilst you have an active subscription for actuated at no extra cost.</p>
<p>In a short period of time, you can set up the Helm chart for the OIDC proxy and no longer have to worry about storing various secrets in GitHub Actions for all your clusters, simply obtain a token and use it to deploy to any cluster - securely. There's no risk that your functions will be exposed on the Internet, because the OIDC proxy only works for the <code>/system</code> endpoints of the OpenFaaS REST API.</p>
<p><strong>An alternative for those who need it</strong></p>
<p><a href="https://openfaas.com/pricing">OpenFaaS Enterprise</a> has its own OIDC integration with much more fine-grained permissions implemented. It means that team members using the CLI, Dashboard or API do not need to memorise or share basic authentication credentials with each other, or worry about getting the right password for the right cluster.</p>
<p>An OpenFaaS Enterprise policy can restrict all the way down to read/write permissions on a number of namespaces, and also integrates with OIDC.</p>
<p>See an example:</p>
<ul>
<li><a href="https://docs.openfaas.com/openfaas-pro/iam/github-actions-federation/">OpenFaaS Enterprise - IAM with GitHub Actions</a></li>
<li><a href="https://docs.openfaas.com/openfaas-pro/iam/gitlab-federation/">OpenFaaS Enterprise - IAM with GitLab</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lessons learned managing GitHub Actions and Firecracker]]></title>
            <link>https://actuated.dev/blog/managing-github-actions</link>
            <guid>https://actuated.dev/blog/managing-github-actions</guid>
            <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Alex shares lessons from building a managed service for GitHub Actions with Firecracker.]]></description>
            <content:encoded><![CDATA[<p>Over the past few months, we've launched over 20,000 VMs for customers and have handled over 60,000 webhook messages from the GitHub API. We've learned a lot from every customer PoC and from our own usage in OpenFaaS.</p>
<p>First of all - what is it we're offering? And how is it different to managed runners and the self-hosted runner that GitHub offers?</p>
<p>Actuated replicates the hosted experience you get from paying for hosted runners, and brings it to hardware under your own control. That could be a bare-metal Arm server, or a regular Intel/AMD cloud VM that has nested virtualisation enabled.</p>
<p>Just like managed runners - every time actuated starts up a runner, it's within a single-tenant virtual machine (VM), with an immutable filesystem.</p>
<p><a href="https://twitter.com/alexellisuk/status/1641471236738875399/photo/1"><img src="https://pbs.twimg.com/media/FsesfBIWAAEYjGY?format=jpg&#x26;name=large" alt="The Asahi Linux lab"></a></p>
<blockquote>
<p>Asahi Linux running on my lab of two M1 Mac Minis - used for building the Arm64 base images and Kernels.</p>
</blockquote>
<p>Can't you just use a self-hosted runner on a VM? Yes, of course you can. But it's actually more nuanced than that. The self-hosted runner <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">isn't safe</a> for OSS or public repos. And whether you run it directly on the host, or in Kubernetes - it's subject to side-effects, poor isolation, malware and in some cases uses very high privileges that could result in taking over a host completely.</p>
<p>You can learn more in the <a href="https://actuated.dev/blog/blazing-fast-ci-with-microvms">actuated announcement</a> and <a href="https://docs.actuated.dev/faq">FAQ</a>.</p>
<h2>how does it work?</h2>
<p>We run a SaaS - a managed control-plane which is installed onto your organisation as a GitHub App. At that point, we'll receive webhooks about jobs in a queued state.</p>
<p><img src="/images/2023-03-lessons-learned/conceptual.jpg" alt="Conceptual architecture"></p>
<p>As you can see in the diagram above, when a webhook is received, and we determine it's for your organisation, we'll schedule a <a href="https://firecracker-microvm.github.io/">Firecracker</a> MicroVM on one of your servers.</p>
<p>We have no access to your code or build secrets. We just obtain a registration token and send the runner a bit of metadata. Then we get out the way and let the self-hosted runner do its thing - in an isolated Kernel, with an immutable filesystem and its own Docker daemon.</p>
<p>Onboarding doesn't take very long - you can use your own servers or get them from a cloud provider. We've got a <a href="https://docs.actuated.dev/provision-server/">detailed guide</a>, but can also recommend an option on a discovery call.</p>
<p>Want to learn more about how Firecracker compares to VMs and containers? <a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch my webinar on YouTube</a></p>
<h2>Lesson 1 - GitHub's images are big and beautiful</h2>
<p>The first thing we noticed when building our actuated VM images was that the GitHub ones are huge.</p>
<p>And if you've ever tried to find out how they're built, or hoped to find a nice little Dockerfile, you can may be disappointed. The images for Linux, Windows and MacOS are built <a href="https://github.com/actions/runner-images">through a set of bespoke scripts</a>, and are hard to adapt for your own use.</p>
<p>Don't get me wrong. The scripts are very clever and they work well. GitHub have been tuning these runner images for years, and they cover a variety of different use-cases.</p>
<p>The first challenge for actuated before launching a pilot was getting enough of the most common packages installed through a Dockerfile. Most of our own internal software is built with Docker, so we can get by with quite a spartan environment.</p>
<p>We also had to adapt the sample Kernel configuration provided by the Firecracker team so that it could launch Docker and so it had everything it required to launch <a href="https://twitter.com/alexellisuk/status/1641476545695952905?s=20">Kubernetes</a>.</p>
<p><a href="https://twitter.com/alexellisuk/status/1641476545695952905/photo/1"><img src="https://pbs.twimg.com/media/FseyAyWXsAQJBCe?format=png&#x26;name=large" alt="M1s running Firecracker"></a></p>
<blockquote>
<p>Two M1 Mac Minis running Asahi Linux and four separate versions of K3s</p>
</blockquote>
<p>So by following the 80/20 principle, and focusing on the most common use-cases, we were able to launch quite quickly and cover 80% of the use-cases.</p>
<p>I don't know if you realised, things like Node.js are pre-installed in the environment, but many Node developers also add the "setup-node" action which guess what? Downloads and installs Node.js again. The same is true for many other languages and tools. We do ship Node.js and Python in the image, but the chances are that we could probably remove them at some point.</p>
<p>With one of our earliest pilots, a customer wanted to use a terraform action. It failed and I felt a bit embarrassed by the reason. We were missing unzip in our images.</p>
<p>The cure? Go and add unzip to the Dockerfile, and hit publish on our builder repository. In 3 minutes the problem was solved.</p>
<p>But GitHub Actions is also incredibly versatile and it means even if something is missing, we don't necessary have to publish a new image for you to continue your work. Just add a step to your workflow to install the missing package.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Add</span> <span class="hljs-string">unzip</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">install</span> <span class="hljs-string">-qy</span> <span class="hljs-string">unzip</span>
</code></pre>
<p>With every customer pilot we've done, there's tended to be one or two packages like this that they expected to see. For another customer it was "libpq". As a rule, if something is available in the hosted runner, we'll strongly consider adding it to ours.</p>
<h2>Lesson 2 - Itâ€™s not GitHub, it canâ€™t be GitHub. It was GitHub</h2>
<p>Since actuated is a control-plane, a SaaS, a full-service - supported product, we are always asking first - is it us? Is it our code? Is it our infrastructure? Is it our network? Is it our hardware?</p>
<p>If you open up the <a href="https://githubstatus.com">GitHub status page</a>, you'll notice an outage almost every week - at times on consecutive days, or every few days on GitHub Actions or a service that affects them indirectly - like the package registry, Pages or Pull Requests.</p>
<p><img src="/images/2023-03-lessons-learned/outage.png" alt="Outage this week"></p>
<blockquote>
<p>The second outage this week that unfortunately affected actuated customers.</p>
</blockquote>
<p>I'm not bashing on GitHub here, we're paying a high R&#x26;D cost to build on their platform. We want them to do well.</p>
<p>But this is getting embarrassing. On a recent call, a customer told us: "it's not your solution, it looks great for us, it's the reliability of GitHub, we're worried about adopting it"</p>
<p>What can you say to that? I can't tell them that their concerns are misplaced, because they're not.</p>
<p>I reached out to Martin Woodward - Director of DevRel at GitHub. He told me that "leadership are taking this very seriously. We're doing better than we were 12 months ago."</p>
<p>GitHub is too big to fail. Let's hope they smooth out these bumps.</p>
<h2>Lesson 3 - Webhooks can be unreliable</h2>
<p>There's no good API to collect this historical data at the moment but we do have an open-source tool (<a href="https://github.com/self-actuated/actions-usage">self-actuated/actions-usage</a>) we give to customers to get a summary of their builds before they start out with us.</p>
<p>So we mirror a summary of job events from GitHub into our database, so that we can show customers trends in behaviour, and identify hot-spots on specific repos - long build times, or spikes in failure rates.</p>
<p><img src="https://pbs.twimg.com/media/FqnJ8rLXgAEnJDZ?format=png&#x26;name=large" alt="Insights chart"></p>
<blockquote>
<p>Insights chart from the actuated dashboard</p>
</blockquote>
<p>We noticed that from time to time, jobs would show in our database as "queued" or "in_progress" and we couldn't work out why. A VM had been scheduled, the job had run, and completed.</p>
<p>In some circumstances, GitHub forgot to send us an In Progress event, or they never sent us a queued event.</p>
<p>Or they sent us queued, in progress, then completed, but in the reverse order.</p>
<p>It took us longer than I'm comfortable with to track down this issue, but we've now adapted our API to handle these edge-cases.</p>
<p>Some deeper digging showed that people have also had issues with Stripe webhooks coming out of order. We saw this issue only very recently, after handling 60k webhooks - so perhaps it was a change in the system being used at GitHub?</p>
<h2>Lesson 4 - Autoscaling is hard and the API is sometimes wrong</h2>
<p>We launch a VM on your servers for every time we receive a queued event. But we have no good way of saying that a particular VM can only run for a certain job.</p>
<p>If there were five jobs queued up, then GitHub would send us five queued events, and we'd launch five VMs. But if the first job was cancelled, we'd still have all of those VMs running.</p>
<p>Why? Why can't we delete the 5th?</p>
<p>Because there is no determinism. It'd be a great improvement for user experience if we could tell GitHub's API - "great, we see you queued build X, it must run on a runner with label Y". But we can't do that today.</p>
<p>So we developed a "reaper" - a background task that tracks launched VMs and can delete them after a period of inactivity. We did have an initial issue where GitHub was taking over a minute to send a job to a ready runner, which we fixed by increasing the idle timeout value. Right now it's working really well.</p>
<p>There is still one remaining quirk where GitHub's API reports that an active runner where a job is running as idle. This happens surprisingly often - but it's not a big deal, the VM deletion call gets rejected by the GitHub API.</p>
<h2>Lesson 5 - We can launch in under one second, but what about GitHub?</h2>
<p>The way we have things tuned today, the delay from you hitting commit in GitHub, to the job executing is similar to that of hosted runners. But sometimes, GitHub lags a little - especially during an outage or when they're under heavy load.</p>
<p><img src="/images/2023-03-lessons-learned/vms_running.png" alt="VM launches"></p>
<blockquote>
<p>Grafana Cloud showing a gauge of microVMs per managed host</p>
</blockquote>
<p>There could be a delay between when you commit, and when GitHub delivers the "queued" webhook.</p>
<p>Scoring and placing a VM on your servers is very quick, then the boot time of the microVM is generally less than 1 second including starting up a dedicated Docker daemon inside the VM.</p>
<p>Then the runner has to run a configuration script to register itself on the API</p>
<p>Finally, the runner connects to a queue, and GitHub has to send it a payload to start the job.</p>
<p>On those last two steps - we see a high success rate, but occasionally, GitHub's API will fail on either of those two operations. We receive an alert via Grafana Cloud and Discord - then investigate. In the worst case, we re-queue via our API the job and the new VM will pick up the pending job.</p>
<p>Want to <a href="https://www.youtube.com/watch?v=2o28iUC-J1w">watch a demo</a>?</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2>Lesson 6 - Sometimes we need to debug a runner</h2>
<p>When I announced actuated, I heard a lot of people asking for CircleCI's debug experience, <a href="https://docs.actuated.dev/tasks/debug-ssh/">so I built something similar</a> and it's proved to be really useful for us in building actuated.</p>
<p>Only yesterday, Ivan Subotic from Dasch Swiss messaged me and said:</p>
<blockquote>
<p>"How cool!!! you donâ€™t know how many hours I have lost on GitHub Actions without this."</p>
</blockquote>
<p>Recently there were two cases where we needed to debug a runner with an SSH shell.</p>
<p>The first was for a machine on Hetzner, where the Docker Daemon was unable to pull images due to a DNS failure. I added steps to print out <code>/etc/resolv.conf</code> and that would be my first port of call. Debugging is great, but it's slow, if an extra step in the workflow can help us diagnose the problem, it's worth it.</p>
<p>In the end, it took me about a day and a half to work out that Hetzner was blocking outgoing traffic on port 53 to Google and Cloudflare. What was worse - was that it was an intermittent problem.</p>
<p>When we did other customer PoCs on Hetzner, we did not run into this issue. I even launched a "cloud" VM in the same region and performed a couple of <code>nslookup</code>s - they worked as expected for me.</p>
<p>So I developed a <a href="https://github.com/self-actuated/hetzner-dns-action">custom GitHub Action</a> to unblock the customer:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hetzner-dns-action@v1</span>
</code></pre>
<p>Was this environmental issue with Hetzner our responsibility? Arguably not, but our customers pay us to provide a "like managed" solution, and we are currently able to help them be successful.</p>
<p>In the second case, Ivan needed to launch headless Chrome, and was using one of the many <code>setup-X</code> actions from the marketplace.</p>
<p>I opened a debug session on one of our own runners, then worked backwards:</p>
<pre><code class="hljs language-bash">curl -sLS -O https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb

dpkg -i ./google-chrome-stable_current_amd64.deb
</code></pre>
<p>This reported that some packages were missing, I got which packages by running <code>apt-get --fix-missing --no-recommends</code> and provided an example of how to add them.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">chrome:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">chrome</span>
        <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Add</span> <span class="hljs-string">extra</span> <span class="hljs-string">packages</span> <span class="hljs-string">for</span> <span class="hljs-string">Chrome</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            sudo apt install -qyyy --no-install-recommends adwaita-icon-theme fontconfig fontconfig-config fonts-liberation gtk-update-icon-cache hicolor-icon-theme humanity-icon-theme libatk-bridge2.0-0 libatk1.0-0 libatk1.0-data libatspi2.0-0 ...
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">browser-actions/setup-chrome@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">run:</span> <span class="hljs-string">chrome</span> <span class="hljs-string">--version</span>
</code></pre>
<p>We could also add these to the base image by editing the Dockerfile that we maintain.</p>
<h2>Lesson 7 - Docker Hub Rate Limits are a pain</h2>
<p>Docker Hub rate limits are more of a pain on self-hosted runners than they are on GitHub's own runners.</p>
<p>I ran into this problem whilst trying to rebuild around 20 OpenFaaS Pro repositories to upgrade a base image. So after a very short period of time, all code ground to a halt and every build failed.</p>
<p>GitHub has a deal to pay Docker Inc so that you don't run into rate limits. At time of writing, you'll find a valid Docker Hub credential in the <code>$HOME/.docker/config.json</code> file on any hosted runner.</p>
<p>Actuated customers would need to login at the top of every one of their builds that used Docker, and create an organisation-level secret with a pull token from the Docker Hub.</p>
<p>We found a way to automate this, and speed up subsequent jobs by <a href="https://docs.actuated.dev/tasks/registry-mirror/">caching images directly on the customer's server</a>.</p>
<p>All they need to add to their builds is:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>
</code></pre>
<h2>Wrapping up</h2>
<p>I hope that you've enjoyed hearing a bit about our journey so far. With every new pilot customer we learn something new, and improve the offering.</p>
<p>Whilst there was a significant amount of very technical work at the beginning of actuated, most of our time now is spent on customer support, education, and improving the onboarding experience.</p>
<p>If you'd like to know how actuated compares to hosted runners or managing the self-hosted runner on your own, we'd encourage <a href="https://actuated.dev/blog">checking out the blog</a> and <a href="https://actuated.dev/faq">FAQ</a>.</p>
<p>Are your builds slowing the team down? Do you need better organisation-level insights and reporting? Or do you need Arm support? Are you frustrated with managing self-hosted runners?</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Reach out to us take part in the pilot</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to split up multi-arch Docker builds to run natively]]></title>
            <link>https://actuated.dev/blog/how-to-run-multi-arch-builds-natively</link>
            <guid>https://actuated.dev/blog/how-to-run-multi-arch-builds-natively</guid>
            <pubDate>Fri, 24 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[QEMU is a convenient way to publish containers for multiple architectures, but it can be incredibly slow. Native is much faster.]]></description>
            <content:encoded><![CDATA[<p>In two previous articles, we covered huge improvements in performance for the Parca project and VPP (Network Service Mesh) simply by switching to actuated with Arm64 runners instead of using QEMU and hosted runners.</p>
<p>In the <a href="http://actuated.dev/blog/native-arm64-for-github-actions">first case</a>, using QEMU took over 33 minutes, and bare-metal Arm showed a 22x improvement at only 1 minute 26 seconds. For Network Service Mesh, <a href="http://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions">VPP</a> couldn't even complete a build in 6 hours using QEMU - and I got it down to 9 minutes flat using a bare-metal <a href="https://amperecomputing.com/processors/ampere-altra">Ampere Altra server</a>.</p>
<h2>What are we going to see and why is it better?</h2>
<p>In this article, I'll show you how to run multi-arch builds natively on bare-metal hardware using GitHub Actions and actuated.</p>
<p><a href="https://actuated.dev/">Actuated</a> is a SaaS service that we built so that you can Bring Your Own compute to GitHub Actions, and have every build run in an immutable, single-use VM.</p>
<p><a href="https://twitter.com/alexellisuk/status/1639232258887372801?s=20"><img src="https://pbs.twimg.com/media/Fr_CcVJWIAEY1gL?format=png&#x26;name=medium" alt="Comparison of the two builds"></a></p>
<blockquote>
<p>Comparison of splitting out to run in parallel on native hardware and QEMU.</p>
</blockquote>
<p>Not every build will see such a dramatic increase as the ones I mentioned in the introduction. Here, with the inlets-operator, we gained 4 minutes on each commit. But I often speak to users who are running past 30 minutes to over an hour because of QEMU.</p>
<p>Three things got us a speed bump here:</p>
<ul>
<li>We ran on bare-metal, native hardware - not the standard hosted runner from GitHub</li>
<li>We split up the work and ran in parallel - rather than waiting for two builds to run in serial</li>
<li>Finally, we did away with QEMU and ran the Arm build directly on an Arm server</li>
</ul>
<p>Only last week an engineer at Calyptia (the team behind <a href="https://fluentbit.io/">fluent-bit</a>) reached out for help after telling me they had to disable and stop publishing open source images for Arm, it was simply timing out at the 6 hour mark.</p>
<p>So how does this thing work, and is QEMU actually "OK"?</p>
<h2>QEMU can be slow, but it's actually "OK"</h2>
<p>So if the timings are so bad, why does anyone use QEMU?</p>
<p>Well it's free - as in beer, there's no cost at all to use it. And many builds can complete in a reasonable amount of time using QEMU, even if it's not as fast as native.</p>
<p>That's why we wrote up how we build 80+ multi-arch images for various products like OpenFaaS and Inlets:</p>
<p><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></p>
<p>Here's what the build looks like with QEMU:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">split-operator</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">qemu</span> ]

<span class="hljs-attr">jobs:</span>

  <span class="hljs-attr">publish_qemu:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-qemu</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">QEMU</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-qemu-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/amd64,linux/arm64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-qemu
</span></code></pre>
<p>This is the kind of build that is failing or causing serious delays for projects like Parca, VPP and Fluent Bit.</p>
<p>Let's look at the alternative.</p>
<h2>Building on native hardware</h2>
<p>Whilst QEMU emulates the architecture you need within a build, it's not the same as running on the real hardware. This is why we see such a big difference in performance.</p>
<p>The downside is that we have to write a bit more CI configuration and run two builds instead of one, but there is some good news - we can now run them in parallel.</p>
<p>In parallel we:</p>
<ol>
<li>We publish the x86_64 image - <code>ghcr.io/owner/repo:sha-amd64</code></li>
<li>We publish the ARM image - <code>ghcr.io/owner/repo:sha-arm64</code></li>
</ol>
<p>Then:</p>
<ol start="3">
<li>We create a manifest with its own name - <code>ghcr.io/owner/repo:sha</code></li>
<li>We annotate the manifest with the images we built earlier</li>
<li>We push the manifest</li>
</ol>
<p>In this way, anyone can pull the image with the name <code>ghcr.io/owner/repo:sha</code> and it will map to either of the two images for Arm64 or Amd64.</p>
<p><img src="/images/2023-split-native/parallel.jpg" alt="Parallel execution"></p>
<blockquote>
<p>The two builds on the left ran on two separate bare-metal hosts, and the manifest was published using one of GitHub's hosted runners.</p>
</blockquote>
<p>Here's a sample for the inlets-operator, a Go binary which connects to the Kubernetes API.</p>
<p>First up, we have the x86 build:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">split-operator</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span> ]

<span class="hljs-attr">jobs:</span>

  <span class="hljs-attr">publish_x86:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-x86</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">mirror</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/amd64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64
</span></code></pre>
<p>Then we have the arm64 build which is almost identical, but we specify a different value for <code>platforms</code> and the <code>runs-on</code> field.</p>
<pre><code class="hljs language-yaml">
  <span class="hljs-attr">publish_aarch64:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-aarch64</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-aarch64</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">mirror</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/arm64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
</span></code></pre>
<p>Finally, we need to create the manifest. GitHub Actions has a <code>needs</code> variable that we can set to control the execution order:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-attr">publish_manifest:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">needs:</span> [<span class="hljs-string">publish_x86</span>, <span class="hljs-string">publish_aarch64</span>]
    <span class="hljs-attr">steps:</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
      <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
      <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">manifest</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        docker manifest create ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} \
          --amend ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64 \
          --amend ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
        docker manifest annotate --arch amd64 --os linux ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64
        docker manifest annotate --arch arm64 --os linux ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
        docker manifest inspect ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}
</span>
        <span class="hljs-string">docker</span> <span class="hljs-string">manifest</span> <span class="hljs-string">push</span> <span class="hljs-string">ghcr.io/${{</span> <span class="hljs-string">env.REPO_OWNER</span> <span class="hljs-string">}}/inlets-operator:${{</span> <span class="hljs-string">github.sha</span> <span class="hljs-string">}}</span>
</code></pre>
<p>One thing I really dislike about this final stage is how much repetition we get. Fortunately, it's relatively simple to hide this complexity behind a custom GitHub Action.</p>
<p>Note that this is just an example at the moment, but I could make a custom composite action in Bash in about 30 minutes, including testing. So it's not a lot of work and it would make our whole workflow a lot less repetitive.</p>
<pre><code class="hljs language-yaml">   <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/compile-manifest@master</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">ghcr.io/${{</span> <span class="hljs-string">env.REPO_OWNER</span> <span class="hljs-string">}}/inlets-operator</span>
      <span class="hljs-attr">sha:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.sha</span> <span class="hljs-string">}}</span>
      <span class="hljs-attr">platforms:</span> <span class="hljs-string">amd64,arm64</span>
</code></pre>
<h2>Wrapping up</h2>
<p>Yesterday we took a new customer on for actuated who wanted to improve the speed of Arm builds, but on the call we both knew they would need to leave QEMU behind. I put this write-up together to show what would be involved, and I hope it's useful to you.</p>
<p>Where can you run these builds?</p>
<p>Couldn't you just add a low-cost Arm VM from AWS, Oracle Cloud, Azure or Google Cloud?</p>
<p>The answer unfortunately is no.</p>
<p>The self-hosted runner is not suitable for open source / public repositories, the GitHub documentation has a stark warning about this.</p>
<p>The Kubernetes controller that's available has the same issues, because it re-uses the Pods by default, and runs in a dangerous Docker In Docker Mode as a privileged container or by mounting the Docker Socket. I'm not sure which is worse, but both mean that code in CI can take over the host, potentially even the whole cluster.</p>
<p>Hosted runners solve this by creating a fresh VM per job, and destroying it immediately. That's the same approach that we took with actuated, but you get to bring your own metal along, so that you keep costs from growing out of control. Actuated also supports Arm, out of the box.</p>
<p>Want to know more about the security of self-hosted runners? <a href="https://docs.actuated.dev/faq/">Read more in our FAQ</a>.</p>
<p>Want to talk to us about your CI/CD needs? We're happy to help.</p>
<ul>
<li><a href="https://docs.actuated.dev/register/#sign-up-for-the-pilot">Contact us about a PoC</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bring Your Own Metal Case Study with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions</link>
            <guid>https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions</guid>
            <pubDate>Fri, 10 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[See how BYO bare-metal made a 6 hour GitHub Actions build complete 25x faster.]]></description>
            <content:encoded><![CDATA[<p>I'm going to show you how both a regular x86_64 build and an Arm build were made dramatically faster by using Bring Your Own (BYO) bare-metal servers.</p>
<p>At the early stage of a project, GitHub's standard runners with 2x cores, 8GB RAM, and a little free disk space are perfect because they're free for public repos. For private repos they come in at a modest cost, if you keep your usage low.</p>
<p>What's not to love?</p>
<p>Well, <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke</a>, Distinguished Engineer at Cisco contacted me a few weeks ago and told me about the VPP project, and some of the problems he was running into trying to build it with hosted runners.</p>
<blockquote>
<p>The Fast Data Project (FD.io) is an open-source project aimed at providing the world's fastest and most secure networking data plane through <a href="https://fd.io/">Vector Packet Processing (VPP)</a>.</p>
</blockquote>
<p>Whilst VPP can be used as a stand-alone project, it is also a key component in the <a href="https://www.cncf.io/">Cloud Computing Foundation's (CNCF's)</a> Open Source <a href="https://networkservicemesh.io/">Network Service Mesh</a> project.</p>
<p>There were two issues:</p>
<ol>
<li>
<p>The x86_64 build was taking 1 hour 25 minutes on a standard runner.</p>
<p>Why is that a problem? CI is meant to both validate against regression, but to build binaries for releases. If that process can take 50 minutes before failing, it's incredibly frustrating. For an open source project, it's actively hostile to contributors.</p>
</li>
<li>
<p>The Arm build was hitting the 6 hour limit for GitHub Actions then failing</p>
<p>Why? Well it was using <a href="https://www.qemu.org/">QEMU</a>, and I've spoken about this in the past - QEMU is a brilliant, zero cost way to build Arm binaries on a regular machine, but it's slow. And you'll see just how slow in the examples below, including where my Raspberry Pi beat a GitHub runner.</p>
</li>
</ol>
<p>We explain how to use QEMU in Docker Actions in the following blog post:</p>
<p><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></p>
<h2>Rubbing some bare-metal on it</h2>
<p>So GitHub does actually have a beta going for "larger runners", and if Ed wanted to try that out, he'd have to apply to a beta waitlist, upgrade to a Team or Enterprise Plan, and then pick a new runner size.</p>
<p>But that wouldn't have covered him for the Arm build, GitHub don't have any support there right now. I'm sure it will come one, day, but here we are unable to release binaries for our Arm users.</p>
<p>With actuated, we have no interest in competing with GitHub's business model of selling compute on demand. We want to do something more unique than that - we want to enable you to bring your own (BYO) devices and then use them as runners, with VM-level isolation and one-shot runners.</p>
<blockquote>
<p>What does Bring Your Own (BYO) mean?</p>
<p>"Your Own" does not have to mean physical ownership. You do not need to own a datacenter, or to send off a dozen Mac Minis to a Colo.
You can provision bare-metal servers on AWS or with Equinix Metal as quickly as you can get an EC2 instance.
Actually, bare-metal isn't strictly needed at all, and even DigitalOcean's and Azure's VMs will work with actuated because they support KVM, which we use to launch Firecracker.</p>
</blockquote>
<p>And who is behind actuated? We are a nimble team, but have a pedigree with Cloud Native and self-hosted software going back 6-7 years from <a href="https://openfaas.com/">OpenFaaS</a>. OpenFaaS is a well known serverless platform which is used widely in production by commercial companies including Fortune 500s.</p>
<p>Actuated uses a Bring Your Own (BYO) server model, but there's very little for you to do once you've installed the actuated agent.</p>
<p>Here's how to set up the agent software: <a href="https://docs.actuated.dev/install-agent/">Actuated Docs: Install the Agent</a>.</p>
<p>You then get detailed stats about each runner, the build queue and insights across your whole GitHub organisation, in one place:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Actuated now aggregates usage data at the organisation level, so you can get insights and spot changes in behaviour.<br><br>This peak of 57 jobs was when I was quashing CVEs for <a href="https://twitter.com/openfaas?ref_src=twsrc%5Etfw">@openfaas</a> Pro customers in Alpine Linux and a bunch of Go <a href="https://t.co/a84wLNYYjo">https://t.co/a84wLNYYjo</a>â€¦ <a href="https://t.co/URaxgMoQGW">https://t.co/URaxgMoQGW</a> <a href="https://t.co/IuPQUjyiAY">pic.twitter.com/IuPQUjyiAY</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1633059062639108096?ref_src=twsrc%5Etfw">March 7, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2>First up - x86_64</h2>
<p>I forked Ed's repo into the "actuated-samples" repo, and edited the "runs-on:" field from "ubuntu-latest" to "actuated".</p>
<p>The build which previously took 1 hour 25 minutes now took 18 minutes 58 seconds. That's a 4.4x improvement.</p>
<p><img src="/images/2023-03-vpp/x86.png" alt="Improvements over the standard runner"></p>
<p>4.4x doesn't sound like a big number, but look at the actual number.</p>
<p>It used to take well over an hour to get feedback, now you get it in less than 20 minutes.</p>
<p>And for context, this x86_64 build took 17 minutes to build on Ed's laptop, with some existing caches in place.</p>
<p>I used an <a href="https://deploy.equinix.com/product/servers/m3-small/">Equinix Metal m3.small.x86 server</a>, which has 8x Intel Xeon E-2378G cores @ 2.8 GHz. It also comes with a local SSD, local NVMe would have been faster here.</p>
<p>The Firecracker VM that was launched had 12GB of RAM and 8x vCPUs allocated.</p>
<h2>Next up - Arm</h2>
<p>For the Arm build I created a new branch and had to change a few hard-coded references from "_amd64.deb" to "_arm64.deb" and then I was able to run the build. This is common enablement work. I've been doing Arm enablement for Cloud Native and OSS since 2015, so I'm very used to spotting this kind of thing.</p>
<p>So the build took 6 hours, and didn't even complete when running with QEMU.</p>
<p>How long did it take on bare-metal? 14 minutes 28 seconds.</p>
<p><img src="/images/2023-03-vpp/arm64.png" alt="Improvements over QEMU"></p>
<p>That's a 25x improvement.</p>
<p>The Firecracker VM that we launched had 16GB of RAM and 8x vCPUs allocated.</p>
<p>It was running on a Mac Mini M1 configured with 16GB RAM, running with Asahi Linux. I bought it for development and testing, as a one-off cost, and it's a very fast machine.</p>
<p>But, this case-study is <em>not</em> specifically about using consumer hardware, or hardware plugged in under your desk.</p>
<p>Equinix Metal and Hetzner both have the Ampere Altra bare-metal server available on either an hourly or monthly basis, and AWS customers can get access to the a1.metal instance on an hourly basis too.</p>
<blockquote>
<p>To prove the point, that BYO means cloud servers, just as much as physically owned machines, <a href="https://github.com/actuated-samples/govpp/actions/runs/4391240138">I also ran the same build on an Ampere Altra from Equinix Metal</a> with 20 GB of RAM, and 32 vCPUs, it completed in 9 minutes 39 seconds.</p>
</blockquote>
<p>See our hosting recommendations: <a href="https://docs.actuated.dev/provision-server/">Actuated Docs: Provision a Server</a></p>
<p>In October last year, I benchmarked a Raspberry Pi 4 as an actuated server and pitted it directly against QEMU and GitHub's Hosted runners.</p>
<p>It was 24 minutes faster. That's how bad using QEMU can be instead of using bare-metal Arm.</p>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">Then, just for run I scheduled the MicroVM on my <a href="https://twitter.com/Raspberry_Pi?ref_src=twsrc%5Etfw">@Raspberry_Pi</a> instead of an <a href="https://twitter.com/equinixmetal?ref_src=twsrc%5Etfw">@equinixmetal</a> machine.<br><br>Poor little thing has 8GB RAM and 4 Cores with an SSD connected over USB-C.<br><br>Anyway, it still beat QEMU by 24 minutes! <a href="https://t.co/ITyRpbnwEE">pic.twitter.com/ITyRpbnwEE</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583092051398524928?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2>Wrapping up</h2>
<p>So, wrapping up - if you only build x86_64, and have very few build minutes, and are willing to upgrade to a Team or Enterprise Plan on GitHub, "faster runners" may be an option you want to consider.</p>
<p>If you don't want to worry about how many minutes you're going to use, or surprise bills because your team got more productive, or grew in size, or is finally running those 2 hour E2E tests every night, then actuated may be faster and better value overall for you.</p>
<p>But if you need Arm runners, and <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">want to use them with public repos</a>, then there are not many options for you which are going to be secure and easy to manage.</p>
<h3>A recap on the results</h3>
<p><a href="https://twitter.com/alexellisuk/status/1634128821124313091?s=20"><img src="/images/2023-03-vpp/background.jpg" alt="The improvement on the x86 build"></a></p>
<blockquote>
<p><a href="https://twitter.com/alexellisuk/status/1634128821124313091?s=20">The improvement on the x86 build</a></p>
</blockquote>
<p>You can see the builds here:</p>
<p>x86_64 - 4.4x improvement</p>
<ul>
<li>Before: 1 hour 25 minutes - <a href="https://github.com/edwarnicke/govpp/actions/runs/3622982661">x86_64 build on a hosted runner</a></li>
<li>After: 18 minutes - 58 seconds <a href="https://github.com/actuated-samples/govpp/actions/runs/4383082399">x86_64 build on Equinix Metal</a></li>
</ul>
<p>Arm - 25x improvement</p>
<ul>
<li>Before: 6 hours (and failing) - <a href="https://github.com/edwarnicke/govpp/actions/runs/3643464160">Arm/QEMU build on a hosted runner</a></li>
<li>After: 14 minutes 28 seconds - <a href="https://github.com/actuated-samples/govpp/actions/runs/4383475307">Arm build on Mac Mini M1</a></li>
</ul>
<h3>Want to work with us?</h3>
<p>Want to get in touch with us and try out actuated for your team?</p>
<p>We're looking for pilot customers who want to speed up their builds, or make self-hosted runners simpler to manager, and ultimately, about as secure as they're going to get with MicroVM isolation.</p>
<p>Set up a 30 min call with me to ask any questions you may have and find out next steps.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the actuated pilot</a></li>
</ul>
<p>Learn more about how it compares to other solutions in the FAQ: <a href="https://docs.actuated.dev/faq">Actuated FAQ</a></p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to run KVM guests in your GitHub Actions]]></title>
            <link>https://actuated.dev/blog/kvm-in-github-actions</link>
            <guid>https://actuated.dev/blog/kvm-in-github-actions</guid>
            <pubDate>Fri, 17 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[From building cloud images, to running NixOS tests and the android emulator, we look at how and why you'd want to run a VM in GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>GitHub's hosted runners do not support nested virtualization. This means some frequently used tools that require KVM like packer, the Android emulator, etc can not be used in GitHub Actions CI pipelines.</p>
<p>We noticed there are quite a few issues for people requesting KVM support for GitHub Actions:</p>
<ul>
<li><a href="https://github.com/actions/runner-images/issues/183">Enable nested virtualization Â· Issue #183 Â· actions/runner-images</a></li>
<li><a href="https://github.com/community/community/discussions/8305">Revisiting KVM support for Hosted GitHub Actions Â· Discussion #8305 Â· community/community</a></li>
<li><a href="https://github.com/WikiWatershed/model-my-watershed/pull/3586">[Experimental] Add CI GitHub Actions workflow by rajadain Â· Pull Request #3586 Â· WikiWatershed/model-my-watershed</a></li>
</ul>
<p>As mentioned in some of these issues, an alternative would be to run your own self-hosted runner on a bare metal host. This comes with the downside that builds can conflict and cause side effects to system-level packages. On top if this self-hosted runners are considered <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">insecure for public repositories</a>.</p>
<p>Solutions like the "actions-runtime-controller" or ARC that use Kubernetes to orchestrate and run self-hosted runners in Pods are also out of scope if you need to run VMs.</p>
<p>With Actuated we make it possible to launch a Virtual Machine (VM) within a GitHub Action. Jobs are launched in isolated VMs just like GitHub hosted runners but with support for nested virtualization.</p>
<h2>Case Study: Githedgehog</h2>
<p>One of our customers Sergei Lukianov, founding engineer at <a href="https://githedgehog.com">Githedgehog</a> told us he needed somewhere to build Docker images and to test them with Kubernetes, he uses KinD for that.</p>
<p>Prior to adopting Actuated, his team used hosted runners which are considerably slower, and paid on a per minute basis. Actuated made his builds both faster, and more secure than using any of the alternatives for self-hosted runners.</p>
<p>It turned out that he also needed to launch VMs in those jobs, and that's something else that hosted runners cannot cater for right now. Actuatedâ€™s KVM guest support means he can run all of his workloads on fast hardware.</p>
<p>Some other common use cases that require KVM support on the CI runner:</p>
<ul>
<li>Running <a href="https://www.packer.io/">Packer</a> for creating Amazon Machine Images (AMI) or VM images for other cloud platforms.</li>
<li>Accelerating the <a href="https://developer.android.com/studio/run/emulator-commandline">Android Emulator</a> via KVM.</li>
<li>Running <a href="https://nixos.org/">NixOS</a> tests or builds that depend on VMs.</li>
<li>Testing software that can only be done with <a href="https://www.linux-kvm.org/page/Main_Page">KVM</a> or in a VM.</li>
</ul>
<h2>Running VMs in GitHub Actions</h2>
<p>In this section we will walk you through a couple of hands-on examples.</p>
<h3>Firecracker microVM</h3>
<p>In this example we are going to follow the Firecracker quickstart guide to boot up a Firecracker VM but instead of running it on our local machine we will run it from within a GitHub Actions workflow.</p>
<p>The workflow instals Firecracker, configures and boots a guest VM and then waits 20 seconds before shutting down the VM and exiting the workflow. The image below shows the run logs of the workflow. We see the login prompt of the running microVM.</p>
<p><img src="/images/2023-02-17-kvm-in-github-actions/nested-firecracker.png" alt="Running a firecracker microVM in a GitHub Actions job"></p>
<blockquote>
<p>Running a firecracker microVM in a GitHub Actions job</p>
</blockquote>
<p>Here is the workflow file used by this job:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">vm-run</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">vm-run:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">firecracker</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">arkade</span> <span class="hljs-string">system</span> <span class="hljs-string">install</span> <span class="hljs-string">firecracker</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">microVM</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">./run-vm.sh</span>
</code></pre>
<p>The <a href="https://github.com/alexellis/setup-arkade">setup-arkade</a> is to install arkade on the runner. Next firecracker is installed from the arkade system apps.</p>
<p>As a last step we run a firecracker microVM. The <code>run-vm.sh</code> script is based on the <a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/getting-started.md">firecracker quickstart</a> and collects all the steps into a single script that can be run in the CI pipeline.</p>
<p>It script will:</p>
<ul>
<li>Get the kernel and rootfs for the microVM</li>
<li>Start fireckracker and configure the guest kernel and rootfs</li>
<li>Start the guest machine</li>
<li>Wait for 20 seconds and kill the firecracker process so workflow finishes.</li>
</ul>
<p>The <code>run-vm.sh</code> script:</p>
<pre><code class="hljs language-sh"><span class="hljs-meta">#!/bin/bash</span>

<span class="hljs-comment"># Get a kernel and rootfs</span>
<span class="hljs-built_in">arch</span>=`<span class="hljs-built_in">uname</span> -m`
dest_kernel=<span class="hljs-string">"hello-vmlinux.bin"</span>
dest_rootfs=<span class="hljs-string">"hello-rootfs.ext4"</span>
image_bucket_url=<span class="hljs-string">"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/<span class="hljs-variable">$arch</span>"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$kernel</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_kernel</span> <span class="hljs-variable">$kernel</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$rootfs</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_rootfs</span> <span class="hljs-variable">$rootfs</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Saved kernel file to <span class="hljs-variable">$dest_kernel</span> and root block device to <span class="hljs-variable">$dest_rootfs</span>."</span>

<span class="hljs-comment"># Start firecracker</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Starting firecracker"</span>
firecracker --api-sock /tmp/firecracker.socket &#x26;
firecracker_pid=$!

<span class="hljs-comment"># Set the guest kernel and rootfs</span>
rch=`<span class="hljs-built_in">uname</span> -m`
kernel_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-vmlinux.bin"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"keep_bootcon console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

rootfs_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-rootfs.ext4"</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/drives/rootfs'</span> \
  -H <span class="hljs-string">'Accept: application/json'</span>           \
  -H <span class="hljs-string">'Content-Type: application/json'</span>     \
  -d <span class="hljs-string">"{
        \"drive_id\": \"rootfs\",
        \"path_on_host\": \"<span class="hljs-variable">${rootfs_path}</span>\",
        \"is_root_device\": true,
        \"is_read_only\": false
   }"</span>

<span class="hljs-comment"># Start the guest machine</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/actions'</span>       \
  -H  <span class="hljs-string">'Accept: application/json'</span>          \
  -H  <span class="hljs-string">'Content-Type: application/json'</span>    \
  -d <span class="hljs-string">'{
      "action_type": "InstanceStart"
   }'</span>

<span class="hljs-comment"># Kill the firecracker process to exit the workflow</span>
<span class="hljs-built_in">sleep</span> 20
<span class="hljs-built_in">kill</span> -9 <span class="hljs-variable">$firecracker_pid</span>

</code></pre>
<p>The full example can be found on <a href="https://github.com/skatolo/nested-firecracker">GitHub</a></p>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker you can watch Alex's webinar on the topic.</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h3>NixOS integration tests</h3>
<p>With nix there is the ability to provide a set of declarative configuration to define integration tests that spin up virtual machines using <a href="https://www.qemu.org/">QEMU</a> as the backend. While running these tests in CI without hardware acceleration is supported this is considerably slower.</p>
<p>For a more detailed overview of the test setup and configuration see the original tutorial on nix.dev:</p>
<ul>
<li><a href="https://nix.dev/tutorials/nixos/build-and-deploy/integration-testing-using-virtual-machines">Integration testing using virtual machines (VMs)</a></li>
</ul>
<p>The workflow file for running NixOS tests on  GitHub Actions:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">nixos-tests</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">nixos-test:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-python@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">python-version:</span> <span class="hljs-string">'3.x'</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v16</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">extra_nix_config:</span> <span class="hljs-string">"system-features = nixos-test benchmark big-parallel kvm"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NixOS</span> <span class="hljs-string">test</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">nix</span> <span class="hljs-string">build</span> <span class="hljs-string">-L</span> <span class="hljs-string">.#checks.x86_64-linux.postgres</span>
</code></pre>
<p>We just install Nix using the <a href="https://github.com/cachix/install-nix-action">install-nix-action</a> and run the tests in the next step.</p>
<p>The full example is available on <a href="https://github.com/skatolo/gh-actions-nixos-tests">GitHub</a></p>
<h3>Other examples of using a VM</h3>
<p>In the previous section we showed you some brief examples for the kind of workflows you can run. Here are some other resources and tutorials that should be easy to adapt and run in CI.</p>
<ul>
<li>Create KVM virtual machine images with the <a href="https://developer.hashicorp.com/packer/plugins/builders/qemu">Packer QEMU Builder</a></li>
<li>Launching an Ubuntu cloud image with cloud-init: <a href="https://fabianlee.org/2020/02/23/kvm-testing-cloud-init-locally-using-kvm-for-an-ubuntu-cloud-image/">KVM: Testing cloud-init locally using KVM for an Ubuntu cloud image</a></li>
</ul>
<h2>Conclusion</h2>
<p>Hosted runners do not support nested virtualization. That makes them unsuitable for running CI jobs that require KVM support.</p>
<p>For Actuated runners we provide a custom Kernel that enables KVM support. This will allow you to run Virtual Machines within your CI jobs.</p>
<p>At time of writing there is no support for aarch64 runners. Only Intel and AMD CPUs support nested virtualisation.</p>
<p>While it is possible to deploy your own self-hosted runners to run jobs that need KVM support, this is not recommended:</p>
<ul>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></li>
</ul>
<p>Want to see a demo or talk to our team? <a href="https://forms.gle/8XmpTTWXbZwWkfqT6">Contact us here</a></p>
<p>Just want to try it out instead? <a href="https://docs.actuated.dev/register/">Register your GitHub Organisation and set-up a subscription</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Make your builds run faster with Caching for GitHub Actions]]></title>
            <link>https://actuated.dev/blog/caching-in-github-actions</link>
            <guid>https://actuated.dev/blog/caching-in-github-actions</guid>
            <pubDate>Fri, 10 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how we made a Golang project build 4x faster using GitHub's built-in caching mechanism.]]></description>
            <content:encoded><![CDATA[<p>GitHub provides a <a href="https://github.com/actions/cache">cache action</a> that allows caching dependencies and build outputs to improve workflow execution time.</p>
<p>A common use case would be to cache packages and dependencies from tools such as npm, pip, Gradle, ... . If you are using Go, caching go modules and the build cache can save you a significant amount of build time as we will see in the next section.</p>
<p>Caching can be configured manually, but a lot of setup actions already use the <a href="https://github.com/actions/cache">actions/cache</a> under the hood and provide a configuration option to enable caching.</p>
<p>We use the actions cache to speed up workflows for building the Actuated base images. As part of those workflows we build a kernel and then a rootfs. Since the kernelâ€™s configuration is changed infrequently it makes sense to cache that output.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/build-time-comparison.png" alt="Build time comparison"></p>
<blockquote>
<p>Comparing workflow execution times with and without caching.</p>
</blockquote>
<p>Building the kernel takes around <code>1m20s</code> on our aarch-64 Actuated runner and <code>4m10s</code> for the x86-64 build so we get some significant time improvements by caching the kernel.</p>
<p>The output of the cache action can also be used to do something based on whether there was a cache hit or miss. We use this to skip the kernel publishing step when there was a cache hit.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">if:</span> <span class="hljs-string">${{</span> <span class="hljs-string">steps.cache-kernel.outputs.cache-hit</span> <span class="hljs-type">!=</span> <span class="hljs-string">'true'</span> <span class="hljs-string">}}</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">Kernel</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">make</span> <span class="hljs-string">publish-kernel-x86-64</span>
</code></pre>
<h2>Caching Go dependency files and build outputs</h2>
<p>In this minimal example we are going to setup caching for Go dependency files and build outputs. As an example we will be building <a href="https://github.com/alexellis/registry-creds">alexellis/registry-creds</a>. This is a Kubernetes operator that can be used to replicate Kubernetes ImagePullSecrets to all namespaces.</p>
<p>It has the K8s API as a dependency which is quite large so we expect to save some time by cashing the Go mod download. By also caching the Go build cache it should be possible to speed up the workflow even more.</p>
<h3>Configure caching manually</h3>
<p>We will first create the workflow and run it without any caching.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">ci</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">"alexellis/registry-creds"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</span></code></pre>
<p>The <a href="https://github.com/actions/checkout">checkout action</a> is used to check out the registry-creds repo so the workflow can access it. The next step sets up Go using the <a href="https://github.com/actions/setup-go">setup-go action</a> and as a last step we run <code>go build</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/no-cache-workflow.png" alt="No cache workflow run"></p>
<p>When triggering this workflow we see that each run takes around <code>1m20s</code>.</p>
<p>Modify the workflow and add an additional step to configure the caches using the <a href="https://github.com/actions/cache">cache action</a>:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span> <span class="hljs-string">caches</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/cache@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">path:</span> <span class="hljs-string">|
        ~/.cache/go-build
        ~/go/pkg/mod
</span>      <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-golang-${{</span> <span class="hljs-string">hashFiles('**/go.sum')</span> <span class="hljs-string">}}</span>
      <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
        ${{ runner.os }}-golang-
</span></code></pre>
<p>The <code>path</code> parameter is used to set the paths on the runner to cache or restore. The <code>key</code> parameter sets the key used when saving the cache. A hash of the go.sum file is used as part of the cache key.</p>
<p>Optionally the restore-keys are used to find and restore a cache if there was no hit for the key. In this case we always restore the cache even if there was no specific hit for the go.sum file.</p>
<p>The first time this workflow is run the cache is not populated so we see a similar execution time as without any cache of around <code>1m20s</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/workflow-cache-comparison.png" alt="Comparing workflow runs"></p>
<p>Running the workflow again we can see that it now completes in just <code>18s</code>.</p>
<h3>Use setup-go built-in caching</h3>
<p>The V3 edition of the <a href="https://github.com/actions/setup-go">setup-go</a> action has support for caching built-in. Under the hood it also uses the <a href="https://github.com/actions/cache">actions/cache</a> with a similar configuration as in the example above.</p>
<p>The advantage of using the built-in functionality is that it requires less configuration settings. Caching can be enabled by adding a single line to the workflow configuration:</p>
<pre><code class="hljs language-diff">name: ci

on: push

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          repository: "alexellis/registry-creds"
      - name: Setup Golang
        uses: actions/setup-go@v3
        with:
          go-version: ~1.19
<span class="hljs-addition">+         cache: true</span>
      - name: Build
        run: |
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</code></pre>
<p>Triggering the workflow with the build-in cache yields similar time gains as with the manual cache configuration.</p>
<h2>Conclusion</h2>
<p>We walked you through a short example to show you how to set up caching for a Go project and managed to build the project 4x faster.</p>
<p>If you are building with Docker you can use <a href="https://docs.docker.com/build/ci/github-actions/examples/#cache">Docker layer caching</a> to make your builds faster. Buildkit automatically caches the build results and allows exporting the cache to an external location. It has support for <a href="https://docs.docker.com/build/cache/backends/">uploading the build cache to GitHub Actions cache</a></p>
<p>See also: <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows">GitHub: Caching dependencies in Workflows</a></p>
<p>Keep in mind that there are some limitations to the GitHub Actions cache. Cache entries that have not been accessed in over 7 days will be removed. There is also a limit on the total cache size of 10 GB per repository.</p>
<p>Some points to take away:</p>
<ul>
<li>Using the actions cache is not limited to GitHub hosted runners but can be used with self-hosted runners as well.</li>
<li>Workflows using the cache action can be converted to run on Actuated runners without any modifications.</li>
<li>Jobs on Actuated runners start in a clean VM each time. This means dependencies need to be downloaded and build artifacts or caches rebuilt each time. Caching these files in the actions cache can improve workflow execution time.</li>
</ul>
<blockquote>
<p>Want to learn more about Go and GitHub Actions?</p>
<p>Alex's eBook <a href="https://openfaas.gumroad.com/l/everyday-golang">Everyday Golang</a> has a chapter dedicated to building Go programs with Docker and GitHub Actions.</p>
</blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The efficient way to publish multi-arch containers from GitHub Actions]]></title>
            <link>https://actuated.dev/blog/multi-arch-docker-github-actions</link>
            <guid>https://actuated.dev/blog/multi-arch-docker-github-actions</guid>
            <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to publish container images for both Arm and Intel machines from GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>In 2017, I wrote an article on <a href="https://docs.docker.com/build/building/multi-stage/">multi-stage builds with Docker</a>, and it's now part of the Docker Documentation. In my opinion, multi-arch builds were the proceeding step in the evolution of container images.</p>
<h2>What's multi-arch and why should you care?</h2>
<p>If you want users to be able to use your containers on different types of computer, then you'll often need to build different versions of your binaries and containers.</p>
<p>The <a href="https://github.com/openfaas/faas-cli">faas-cli</a> tool is how users interact with <a href="https://github.com/openfaas/faas">OpenFaaS</a>.</p>
<p>It's distributed in binary format for users, with builds for Windows, MacOS and Linux.</p>
<ul>
<li><code>linux/amd64</code>, <code>linux/arm64</code>, <code>linux/arm/v7</code></li>
<li><code>darwin/amd64</code>, <code>darwin/arm64</code></li>
<li><code>windows/amd64</code></li>
</ul>
<p>But why are there six different binaries for three Operating Systems? With the advent of Raspberry Pi, M1 Macs (Apple Silicon) and AWS Graviton servers, we have had to start building binaries for more than just Intel systems.</p>
<p>If you're curious how to build multi-arch binaries with Go, you can check out the release process for the open source arkade tool here, which is a simpler example than faas-cli: <a href="https://github.com/alexellis/arkade/blob/master/Makefile">arkade Makefile</a> and <a href="https://github.com/alexellis/arkade/blob/master/.github/workflows/publish.yml">GitHub Actions publish job</a></p>
<p>So if we have to support at least six different binaries for Open Source CLIs, what about container images?</p>
<h2>Do we need multi-arch containers too?</h2>
<p>Until recently, it was common to hear people say: "I can't find any containers that work for Arm". This was because the majority of container images were built only for Intel. Docker Inc has done a sterling job of making their "official" images work on different platforms, that's why you can now run <code>docker run -t -i ubuntu /bin/bash</code> on a Raspberry Pi, M1 Mac and your regular PC.</p>
<p>Many open source projects have also caught on to the need for multi-arch images, but there are still a few like Bitnami, haven't yet seen value. I think that is OK, this kind work does take time and effort. Ultimately, it's up to the project maintainers to listen to their users and decide if they have enough interest to add support for Arm.</p>
<p>A multi-arch image is a container that will work on two or more different combinations of operating system and CPU architecture.</p>
<p>Typically, this would be:</p>
<ul>
<li><code>linux/amd64</code> - "normal" computers made by Intel or AMD</li>
<li><code>linux/arm64</code> - 64-bit Arm servers like <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-graviton-performance-testing/what-is-aws-graviton.html">AWS Graviton</a> or <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a></li>
<li><code>linux/arm/v7</code> - the 32-bit Raspberry Pi Operating System</li>
</ul>
<p>So multi-arch is really about catering for the needs of Arm users. Arm hardware platforms like the Ampere Altra come with 80 efficient CPU cores, have a very low TDP compared to traditional Intel hardware, and are available from various cloud providers.</p>
<h2>How do we build multi-arch containers work?</h2>
<p>There are a few tools and tricks that we can combine together to take a single Dockerfile and output an image that anyone can pull, which will be right for their machine.</p>
<p>Let's take the: <code>ghcr.io/inlets-operator:latest</code> image from <a href="https://inlets.dev/">inlets</a>.</p>
<p>When a user types in <code>docker pull</code>, or deploys a Pod to Kubernetes, their local containerd daemon will fetch the manifest file and inspect it to see what SHA reference to use for to download the required layers for the image.</p>
<p><img src="/images/2023-02-multi-arch/multi-arch.png" alt="How manifests work"></p>
<blockquote>
<p>How manifests work</p>
</blockquote>
<p>Let's look at a manifest file with the crane tool. I'm going to use <a href="https://arkade.dev">arkade</a> to install crane:</p>
<pre><code class="hljs language-bash">arkade get crane

crane manifest ghcr.io/inlets/inlets-operator:latest
</code></pre>
<p>You'll see a manifests array, with a platform section for each image:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.list.v2+json"</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">"manifests"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:bae8025e080d05f1db0e337daae54016ada179152e44613bf3f8c4243ad939df"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"amd64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:3ddc045e2655f06653fc36ac88d1d85e0f077c111a3d1abf01d05e6bbc79c89f"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"arm64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<h2>How do we convert a Dockerfile to multi-arch?</h2>
<p>Instead of using the classic version of Docker, we can enable the buildx and Buildkit plugins which provide a way to build multi-arch images.</p>
<p>We'll continue with the Dockerfile from the open source inlets-operator project.</p>
<p>Within <a href="https://github.com/inlets/inlets-operator/blob/master/Dockerfile">the Dockerfile</a>, we need to make a couple of changes.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- FROM golang:1.18 as builder</span>
<span class="hljs-addition">+ FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder</span>

<span class="hljs-addition">+ ARG TARGETPLATFORM</span>
<span class="hljs-addition">+ ARG BUILDPLATFORM</span>
<span class="hljs-addition">+ ARG TARGETOS</span>
<span class="hljs-addition">+ ARG TARGETARCH</span>
</code></pre>
<p>The BUILDPLATFORM variable is the native architecture and platform of the machine performing the build, this is usually amd64.</p>
<p>The TARGETPLATFORM is important for the final step of the build, and will normally be injected based upon one each of the platforms you have specified for the build command.</p>
<p>For Go specifically, we also updated the <code>go build</code> command to tell Go to use cross-compilation based upon the TARGETOS and TARGETARCH environment variables, which are populated by Docker.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- go build -o inlets-operator</span>
<span class="hljs-addition">+ GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o inlets-operator</span>
</code></pre>
<p>Here's the full example:</p>
<pre><code>FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder

ARG TARGETPLATFORM
ARG BUILDPLATFORM
ARG TARGETOS
ARG TARGETARCH

ARG Version
ARG GitCommit

ENV CGO_ENABLED=0
ENV GO111MODULE=on

WORKDIR /go/src/github.com/inlets/inlets-operator

# Cache the download before continuing
COPY go.mod go.mod
COPY go.sum go.sum
RUN go mod download

COPY .  .

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go test -v ./...

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go build -ldflags "-s -w -X github.com/inlets/inlets-operator/pkg/version.Release=${Version} -X github.com/inlets/inlets-operator/pkg/version.SHA=${GitCommit}" \
  -a -installsuffix cgo -o /usr/bin/inlets-operator .

FROM --platform=${BUILDPLATFORM:-linux/amd64} gcr.io/distroless/static:nonroot

LABEL org.opencontainers.image.source=https://github.com/inlets/inlets-operator

WORKDIR /
COPY --from=builder /usr/bin/inlets-operator /
USER nonroot:nonroot

CMD ["/inlets-operator"]
</code></pre>
<h2>How to do you configure GitHub Actions to publish multi-arch images?</h2>
<p>Now that the Dockerfile has been configured, it's time to start working on the GitHub Action.</p>
<p>This example is taken from the Open Source <a href="https://github.com/inlets/inlets-operator">inlets-operator</a>. It builds a container image containing a Go binary and uses a Dockerfile in the root of the repository.</p>
<p>View <a href="https://github.com/inlets/inlets-operator/blob/master/.github/workflows/publish.yaml">publish.yaml</a>, adapted for actuated:</p>
<pre><code class="hljs language-diff">name: publish

on:
  push:
    tags:
      - '*'

jobs:
  publish:
<span class="hljs-addition">+    permissions:</span>
<span class="hljs-addition">+      packages: write</span>

<span class="hljs-deletion">-   runs-on: ubuntu-latest</span>
<span class="hljs-addition">+   runs-on: actuated</span>
    steps:
      - uses: actions/checkout@master
        with:
          fetch-depth: 1

<span class="hljs-addition">+     - name: Setup mirror</span>
<span class="hljs-addition">+       uses: self-actuated/hub-mirror@master</span>
      - name: Get TAG
        id: get_tag
        run: echo TAG=${GITHUB_REF#refs/tags/} >> $GITHUB_ENV
      - name: Get Repo Owner
        id: get_repo_owner
        run: echo "REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')" > $GITHUB_ENV

<span class="hljs-addition">+     - name: Set up QEMU</span>
<span class="hljs-addition">+       uses: docker/setup-qemu-action@v2</span>
<span class="hljs-addition">+     - name: Set up Docker Buildx</span>
<span class="hljs-addition">+       uses: docker/setup-buildx-action@v2</span>
      - name: Login to container Registry
        uses: docker/login-action@v2
        with:
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
          registry: ghcr.io

      - name: Release build
        id: release_build
        uses: docker/build-push-action@v3
        with:
          outputs: "type=registry,push=true"
<span class="hljs-addition">+         platforms: linux/amd64,linux/arm/v6,linux/arm64</span>
          build-args: |
            Version=${{  env.TAG }}
            GitCommit=${{ github.sha }}
          tags: |
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ env.TAG }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:latest
</code></pre>
<p>All of the images and corresponding manifest are published to GitHub's Container Registry (GHCR). The action itself is able to authenticate to GHCR using a built-in, short-lived token. This is dependent on the "permissions" section and "packages: write" being set.</p>
<p>You'll see that we added a <code>Setup mirror</code> step, this explained in the <a href="/examples/registry-mirror">Registry Mirror example</a> and is not required for Hosted Runners.</p>
<p>The <code>docker/setup-qemu-action@v2</code> step is responsible for setting up QEMU, which is used to emulate the different CPU architectures.</p>
<p>The <code>docker/build-push-action@v3</code> step is responsible for passing in a number of platform combinations such as: <code>linux/amd64</code> for cloud, <code>linux/arm64</code> for Arm servers and <code>linux/arm/v6</code> for Raspberry Pi.</p>
<h2>What if you're not using GitHub Actions?</h2>
<p>The various GitHub Actions published by the Docker team are a great way to get started, but if you look under the hood, they're just syntactic sugar for the Docker CLI.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> DOCKER_CLI_EXPERIMENTAL=enabled

<span class="hljs-comment"># Have Docker download the latest buildx plugin</span>
docker buildx install

<span class="hljs-comment"># Create a buildkit daemon with the name "multiarch"</span>
docker buildx create \
    --use \
    --name=multiarch \
    --node=multiarch

<span class="hljs-comment"># Install QEMU</span>
docker run --<span class="hljs-built_in">rm</span> --privileged \
    multiarch/qemu-user-static --reset -p <span class="hljs-built_in">yes</span>

<span class="hljs-comment"># Run a build for the different platforms</span>
docker buildx build \
    --platform=linux/arm64,linux/amd64 \
    --output=<span class="hljs-built_in">type</span>=registry,push=<span class="hljs-literal">true</span> --tag image:tag .
</code></pre>
<p>For OpenFaaS users, we do all of the above any time you type in <code>faas-cli publish</code> and the <code>faas-cli build</code> command just runs a regular Docker build, without any of the multi-arch steps.</p>
<p>If you're interested, you can checkout the code here: <a href="https://github.com/openfaas/faas-cli/blob/master/commands/publish.go">publish.go</a>.</p>
<h2>Putting it all together</h2>
<ul>
<li>CLIs are published for many different combinations of OS and CPU, but containers are usually only required for Linux with an amd64 or Arm CPU.</li>
<li>Multi-arch images work through a manifest, which then tells containerd which image is needs for the platform it is running on.</li>
<li>QEMU is a tool for emulating different CPU architectures, and is used to build the images for the different platforms.</li>
</ul>
<p>In our experience with OpenFaaS, inlets and actuated, once you have converted one or two projects to build multi-arch images, it becomes a lot easier to do it again, and make all software available for Arm servers.</p>
<p>You can learn more about <a href="https://docs.docker.com/build/building/multi-platform/">Multi-platform images</a> in the Docker Documentation.</p>
<p><em>Want more multi-arch examples?</em></p>
<p>OpenFaaS uses multi-arch Dockerfiles for all of its templates, and the examples are freely available on GitHub including Python, Node, Java and Go.</p>
<p>See also: <a href="https://github.com/openfaas/templates">OpenFaaS templates</a></p>
<p><em>A word of caution</em></p>
<p>QEMU can be incredibly slow at times when using a hosted runner, where a build takes takes 1-2 minutes can extend to over half an hour. If you do run into that, one option is to check out actuated or another solution, which can build directly on an Arm server with a securely isolated Virtual Machine.</p>
<p>In <a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a>, we showed how we decreased the build time of an open-source Go project from 30.5 mins to 1.5 mins. If this is the direction you go in, you can use a <a href="https://docs.actuated.dev/examples/matrix/">matrix-build</a> instead of a QEMU-based multi-arch build.</p>
<p>See also: <a href="https://docs.actuated.dev/provision-server/#arm64-aka-aarch64">Recommended bare-metal Arm servers</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/sbom-in-github-actions</link>
            <guid>https://actuated.dev/blog/sbom-in-github-actions</guid>
            <pubDate>Wed, 25 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions in a few easy steps.]]></description>
            <content:encoded><![CDATA[<h2>What is a Software Bill of Materials (SBOM)?</h2>
<p>In <a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">April 2022 Justin Cormack, CTO of Docker announced</a> that Docker was adding support to generate a Software Bill of Materials (SBOM) for container images.</p>
<p>An SBOM is an inventory of the components that make up a software application. It is a list of the components that make up a software application including the version of each component. The version is important because it can be cross-reference with a vulnerability database to determine if the component has any known vulnerabilities.</p>
<p>Many organisations are also required to company with certain Open Source Software (OSS) licenses. So if SBOMs are included in the software they purchase or consume from vendors, then it can be used to determine if the software is compliant with their specific license requirements, lowering legal and compliance risk.</p>
<p>Docker's enhancements to Docker Desktop and their open source Buildkit tool were the result of a collaboration with Anchore, a company that provides a commercial SBOM solution.</p>
<h2>Check out an SBOM for yourself</h2>
<p>Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free.</p>
<ul>
<li><a href="https://github.com/anchore/syft">syft</a> - a command line tool that can be used to generate an SBOM for a container image.</li>
<li><a href="https://github.com/anchore/grype">grype</a> - a command line tool that can be used to scan an SBOM for vulnerabilities.</li>
</ul>
<p>OpenFaaS Community Edition (CE) is a popular open source serverless platform for Kubernetes. It's maintained by open source developers, and is free to use.</p>
<p>Let's pick a container image from the Community Edition of <a href="https://github.com/orgs/openfaasltd/packages">OpenFaaS</a> like the container image for the OpenFaaS gateway.</p>
<p>We can browse the GitHub UI to find the latest revision, or we can use Google's crane tool:</p>
<pre><code class="hljs language-bash">crane <span class="hljs-built_in">ls</span> ghcr.io/openfaas/gateway | <span class="hljs-built_in">tail</span> -n 5
0.26.0
8e1c34e222d6c194302c649270737c516fe33edf
0.26.1
c26ec5221e453071216f5e15c3409168446fd563
0.26.2
</code></pre>
<p>Now we can introduce one of those tags to syft:</p>
<pre><code class="hljs language-bash">syft ghcr.io/openfaas/gateway:0.26.2
 âœ” Pulled image            
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [39 packages]
NAME                                              VERSION                               TYPE      
alpine-baselayout                                 3.4.0-r0                              apk        
alpine-baselayout-data                            3.4.0-r0                              apk        
alpine-keys                                       2.4-r1                                apk        
apk-tools                                         2.12.10-r1                            apk        
busybox                                           1.35.0                                binary     
busybox                                           1.35.0-r29                            apk        
busybox-binsh                                     1.35.0-r29                            apk        
ca-certificates                                   20220614-r4                           apk        
ca-certificates-bundle                            20220614-r4                           apk        
github.com/beorn7/perks                           v1.0.1                                go-module  
github.com/cespare/xxhash/v2                      v2.1.2                                go-module  
github.com/docker/distribution                    v2.8.1+incompatible                   go-module  
github.com/gogo/protobuf                          v1.3.2                                go-module  
github.com/golang/protobuf                        v1.5.2                                go-module  
github.com/gorilla/mux                            v1.8.0                                go-module  
github.com/matttproud/golang_protobuf_extensions  v1.0.1                                go-module  
github.com/nats-io/nats.go                        v1.22.1                               go-module  
github.com/nats-io/nkeys                          v0.3.0                                go-module  
github.com/nats-io/nuid                           v1.0.1                                go-module  
github.com/nats-io/stan.go                        v0.10.4                               go-module  
github.com/openfaas/faas-provider                 v0.19.1                               go-module  
github.com/openfaas/faas/gateway                  (devel)                               go-module  
github.com/openfaas/nats-queue-worker             v0.0.0-20230117214128-3615ccb286cc    go-module  
github.com/prometheus/client_golang               v1.13.0                               go-module  
github.com/prometheus/client_model                v0.2.0                                go-module  
github.com/prometheus/common                      v0.37.0                               go-module  
github.com/prometheus/procfs                      v0.8.0                                go-module  
golang.org/x/crypto                               v0.5.0                                go-module  
golang.org/x/sync                                 v0.1.0                                go-module  
golang.org/x/sys                                  v0.4.1-0.20230105183443-b8be2fde2a9e  go-module  
google.golang.org/protobuf                        v1.28.1                               go-module  
libc-utils                                        0.7.2-r3                              apk        
libcrypto3                                        3.0.7-r2                              apk        
libssl3                                           3.0.7-r2                              apk        
musl                                              1.2.3-r4                              apk        
musl-utils                                        1.2.3-r4                              apk        
scanelf                                           1.3.5-r1                              apk        
ssl_client                                        1.35.0-r29                            apk        
zlib                                              1.2.13-r0                             apk  
</code></pre>
<p>These are all the components that syft found in the container image. We can see that it found 39 packages, including the OpenFaaS gateway itself.</p>
<p>Some of the packages are Go modules, others are packages that have been installed with <code>apk</code> (Alpine Linux's package manager).</p>
<h2>Checking for vulnerabilities</h2>
<p>Now that we have an SBOM, we can use grype to check for vulnerabilities.</p>
<pre><code class="hljs language-bash">grype ghcr.io/openfaas/gateway:0.26.2
 âœ” Vulnerability DB        [no update available]
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [39 packages]
 âœ” Scanned image           [2 vulnerabilities]
NAME                        INSTALLED  FIXED-IN  TYPE       VULNERABILITY   SEVERITY 
google.golang.org/protobuf  v1.28.1              go-module  CVE-2015-5237   High      
google.golang.org/protobuf  v1.28.1              go-module  CVE-2021-22570  Medium  
</code></pre>
<p>In this instance, we can see there are only two vulnerabilities, both of which are in the <code>google.golang.org/protobuf</code> Go module, and neither of them have been fixed yet.</p>
<ul>
<li>As the maintainer of OpenFaaS CE, I could try to eliminate the dependency from the original codebase, or wait for a workaround to be published by its vendor.</li>
<li>As a consumer of OpenFaaS CE my choices are similar, and it may be worth trying to look into the problem myself to see if the vulnerability is relevant to my use case.</li>
<li>Now, for OpenFaaS Pro, a commercial distribution of OpenFaaS, where source is not available, I'd need to contact the vendor OpenFaaS Ltd and see if they could help, or if they could provide a workaround. Perhaps there would even be a paid support relationship and SLA relating to fixing vulnerabilities of this kind?</li>
</ul>
<p>With this scenario, I wanted to show that different people care about the supply chain, and have different responsibilities for it.</p>
<h2>Generate an SBOM from within GitHub Actions</h2>
<p>The examples above were all run locally, but we can also generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.</p>
<p>Imagine you have the following Dockerfile:</p>
<pre><code>FROM alpine:3.17.0

RUN apk add --no-cache curl ca-certificates

CMD ["curl", "https://www.google.com"]
</code></pre>
<p>I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use <code>3.17.1</code> to fix a specific vulnerability.</p>
<p>Now a typical workflow for this Dockerfile would look like the below:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">build</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]
  <span class="hljs-attr">pull_request:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">checks:</span> <span class="hljs-string">write</span>
  <span class="hljs-attr">contents:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">publish:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">image</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/alexellis/gha-sbom:${{ github.sha }}
</span></code></pre>
<p>Upon each commit, an image is published to GitHub's Container Registry with the image name of: <code>ghcr.io/alexellis/gha-sbom:SHA</code>.</p>
<p>To generate an SBOM, we just need to update the <code>docker/build-push-action</code> to use the <code>sbom</code> flag:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Local</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">local_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">sbom:</span> <span class="hljs-literal">true</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
</code></pre>
<p>By checking the logs from the action, we can see that the image has been published with an SBOM:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment">#16 [linux/amd64] generating sbom using docker.io/docker/buildkit-syft-scanner:stable-1</span>
<span class="hljs-comment">#0 0.120 time="2023-01-25T15:35:19Z" level=info msg="starting syft scanner for buildkit v1.0.0"</span>
<span class="hljs-comment">#16 DONE 1.0s</span>
</code></pre>
<p>The SBOM can be viewed as before:</p>
<pre><code class="hljs language-bash">syft ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a

grype ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a
 âœ” Vulnerability DB        [updated]
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [21 packages]
 âœ” Scanned image           [2 vulnerabilities]
NAME        INSTALLED  FIXED-IN  TYPE  VULNERABILITY  SEVERITY 
libcrypto3  3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High      
libssl3     3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High  
</code></pre>
<p>The image: <code>alpine:3.17.0</code> contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed.</p>
<p>We can resolve the issue by changing the Dockerfile to use <code>alpine:3.17.1</code> instead, and re-running the build.</p>
<pre><code class="hljs language-bash">grype ghcr.io/alexellis/gha-sbom:63c6952d1ded1f53b1afa3f8addbba9efa37b52b
 âœ” Vulnerability DB        [no update available]
 âœ” Pulled image            
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [21 packages]
 âœ” Scanned image           [0 vulnerabilities]
No vulnerabilities found
</code></pre>
<h2>Wrapping up</h2>
<p>There is a lot written on the topic of supply chain security, so I wanted to give you a quick overview, and how to get started wth it.</p>
<p>We looked at Anchore's two open source tools: Syft and Grype, and how they can be used to generate an SBOM and scan for vulnerabilities.</p>
<p>We then produced an SBOM for a pre-existing Dockerfile and GitHub Action, introducing a vulnerability by using an older base image, and then fixing it by upgrading it. We did this by adding additional flags to the <code>docker/build-push-action</code>. We added the sbom flag, and set the provenance flag to false. Provenance is a separate but related topic, which is explained well in an article by Justin Chadwell of Docker (linked below).</p>
<p>I maintain an Open Source alternative to brew for developer-focused CLIs called <a href="https://arkade.dev/">arkade</a>. This already includes Google's crane project, and there's a <a href="https://github.com/alexellis/arkade/issues/839">Pull Request coming shortly to add Syft and Grype to the project</a>.</p>
<p>It can be a convenient way to install these tools on MacOS, Windows or Linux:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Available now</span>
arkade get crane syft

<span class="hljs-comment"># Coming shortly</span>
arkade get grype
</code></pre>
<p>In the beginning of the article we mentioned license compliance. SBOMs generated by syft do not seem to include license information, but in my experience, corporations which take this risk seriously tend to run their own scanning infrastructure <a href="https://www.synopsys.com/software-integrity/security-testing/software-composition-analysis.html">with commercial tools like Blackduck</a> or <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/21-08/prisma-cloud-compute-edition-admin/compliance/manage_compliance">Twistlock</a>.</p>
<p>Tools like Twistlock, and certain registries like <a href="https://jfrog.com/artifactory/">JFrog Artifactory</a> and the <a href="https://goharbor.io/">CNCF's Harbor</a>, can be configured to scan images. GitHub has a free, built-in service called Dependabot that won't just scan, but will also send Pull Requests to fix issues.</p>
<p>But with the SBOM approach, the responsibility is rebalanced, with the supplier taking on an active role in security. The consumer can then use the supplier's SBOMs, or run their own scanning infrastructure - or perhaps both.</p>
<p>See also:</p>
<ul>
<li><a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">Announcing Docker SBOM: A step towards more visibility into Docker images- Justin Cormack</a></li>
<li><a href="https://www.docker.com/blog/generate-sboms-with-buildkit/">Generating SBOMs for Your Image with BuildKit - Justin Chadwell</a></li>
<li><a href="https://github.com/anchore">Anchore on GitHub</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Is the GitHub Actions self-hosted runner safe for Open Source?]]></title>
            <link>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</link>
            <guid>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</guid>
            <pubDate>Fri, 20 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub warns against using self-hosted Actions runners for public repositories - but why? And are there alternatives?]]></description>
            <content:encoded><![CDATA[<p>First of all, why would someone working on an open source project need a self-hosted runner?</p>
<p>Having contributed to dozens of open source projects, and gotten to know many different maintainers, the primary reason tends to be out of necessity. They face an 18 minute build time upon every commit or Pull Request revision, and want to make the best of what little time they can give over to Open Source.</p>
<p>Having faster builds also lowers friction for contributors, and since many contributors are unpaid and rely on their own internal drive and enthusiasm, a fast build time can be the difference between them fixing a broken test or waiting another few days.</p>
<p>To sum up, there are probably just a few main reasons:</p>
<ol>
<li>Faster builds, higher concurrency, more disk space</li>
<li>Needing to build and test Arm binaries or containers on real hardware</li>
<li>Access to services on private networks</li>
</ol>
<p>The first point is probably one most people can relate to. Simply by provisioning an AMD bare-metal host, or a high spec VM with NVMe, you can probably shave minutes off a build.</p>
<p>For the second case, some projects like <a href="https://github.com/fluxcd/flagger">Flagger</a> from the CNCF felt their only option to support users deploying to AWS Graviton, was to seek sponsorship for a large Arm server and to install a self-hosted runner on it.</p>
<p>The third option is more nuanced, and specialist. This may or may not be something you can relate to, but it's worth mentioning. VPNs have very limited speed and there may be significant bandwidth costs to transfer data out of a region into GitHub's hosted runner environment. Self-hosted runners eliminate the cost and give full local link bandwidth, even as high as 10GbE. You just won't get anywhere near that with IPSec or Wireguard over the public Internet.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<h2>But are self-hosted runners safe?</h2>
<p>The GitHub team has a stark warning for those of us who are tempted to deploy a self-hosted runner and to connect it to a public repository.</p>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs. Some of the risks include:</p>
<ul>
<li>Malicious programs running on the machine.</li>
<li>Escaping the machine's runner sandbox.</li>
<li>Exposing access to the machine's network environment.</li>
<li>Persisting unwanted or dangerous data on the machine.</li>
</ul>
</blockquote>
<p>See also: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">Self-hosted runner security</a></p>
<p>Now you may be thinking "I won't approve pull requests from bad actors", but quite often the workflow goes this way: the contributor gets approval, then you don't need to approve subsequent pull requests after that.</p>
<p>An additional risk is if that user's account is compromised, then the attacker can submit a pull request with malicious code or malware. There is no way in GitHub to enforce Multi-Factor Authentication (MFA) for pull requests, even if you have it enabled on your Open Source Organisation.</p>
<p>Here are a few points to consider:</p>
<ul>
<li>Side effects build up with every build, making it less stable over time</li>
<li>You can't enforce MFA for pull requests - so malware can be installed directly on the host - whether intentionally or not</li>
<li>The GitHub docs warn that users can take over your account</li>
<li>Each runner requires maintenance and updates of the OS and all the required packages</li>
</ul>
<p>The chances are that if you're running the Flagger or Network Service Mesh project, you are shipping code that enterprise companies will deploy in production with sensitive customer data.</p>
<p>If you are not worried, try explaining the above to them, to see how they may see the risk differently.</p>
<h2>Doesn't Kubernetes fix all of this?</h2>
<p><a href="https://kubernetes.io/">Kubernetes</a> is a well known platform built for orchestrating containers. It's especially suited to running microservices, webpages and APIs, but has support for batch-style workloads like CI runners too.</p>
<p>You could make a container image and install the self-hosted runner binary within in, then deploy that as a Pod to a cluster. You could even scale it up with a few replicas.</p>
<p>If you are only building Java code, Python or Node.js, you may find this resolves many of the issues that we covered above, but it's hard to scale, and you still get side-effects as the environment is not immutable.</p>
<p>That's where the community project "actions-runtime-controller" or ARC comes in. It's a controller that launches a pool of Pods with the self-hosted runner.</p>
<blockquote>
<p>How much work does ARC need?</p>
<p>Some of the teams I have interviewed over the past 3 months told me that ARC took them a lot of time to set up and maintain, whilst others have told us it was a lot easier for them. It may depend on your use-case, and whether you're more of a personal user, or part of a team with 10-30 people committing code several times per day.
The first customer for actuated, which I'll mention later in the article was a team of ~ 20 people who were using ARC and had grew tired of the maintenance overhead and certain reliability issues.</p>
</blockquote>
<p>Unfortunately, by default ARC uses the same Pod many times as a persistent runner, so side effects still build up, malware can still be introduced and you have to maintain a Docker image with all the software needed for your builds.</p>
<p>You may be happy with those trade-offs, especially if you're only building private repositories.</p>
<p>But those trade-offs gets a lot worse if you use Docker or Kubernetes.</p>
<p>Out of the box, you simply cannot start a Docker container, build a container image or start a Kubernetes cluster.</p>
<p>And to do so, you'll need to resort to what can only be described as dangerous hacks:</p>
<ol>
<li>You expose the Docker socket from the host, and mount it into each Pod - any CI job can take over the host, game over.</li>
<li>You run in <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker in Docker (DIND)</a> mode. DIND requires a privileged Pod, which means that any CI job can take over the host, game over.</li>
</ol>
<p>There is some early work on running Docker In Docker in user-space mode, but this is slow, tricky to set up and complicated. By default, user-space mode uses a non-root account. So you can't install software packages or run commands like apt-get.</p>
<p>See also: <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Using Docker-in-Docker for your CI or testing environment? Think twice.</a></p>
<p>Have you heard of Kaniko?</p>
<p><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> is a tool for building container images from a Dockerfile, without the need for a Docker daemon. It's a great option, but it's not a replacement for running containers, it can only build them.</p>
<p>And when it builds them, in nearly every situation it will need root access in order to mount each layer to build up the image.</p>
<p>See also: <a href="https://suraj.io/post/root-in-container-root-on-host/">The easiest way to prove that root inside the container is also root on the host</a></p>
<p>And what about Kubernetes?</p>
<p>To run a KinD, Minikube or K3s cluster within your CI job, you're going to have to sort to one of the dangerous hacks we mentioned earlier which mean a bad actor could potentially take over the host.</p>
<p>Some of you may be running these Kubernetes Pods in your production cluster, whilst others have taken some due diligence and deployed a separate cluster just for these CI workloads. I think that's a slightly better option, but it's still not ideal and requires even more access control and maintenance.</p>
<p>Ultimately, there is a fine line between overconfidence and negligence. When building code on a public repository, we have to assume that the worst case scenario will happen one day. When using DinD or privileged containers, we're simply making that day come sooner.</p>
<p>Containers are great for running internal microservices and Kubernetes excels here, but there is a reason that AWS insists on hard multi-tenancy with Virtual Machines for their customers.</p>
<blockquote>
<p>See also: <a href="https://www.amazon.science/publications/firecracker-lightweight-virtualization-for-serverless-applications">Firecracker whitepaper</a></p>
</blockquote>
<h2>What's the alternative?</h2>
<p>When GitHub cautioned us against using self-hosted runners, on public repos, they also said:</p>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<p>So using GitHub's hosted runners are probably the most secure option for Open Source projects and for public repositories - if you are happy with the build speed, and don't need Arm runners.</p>
<p>But that's why I'm writing this post, sometimes we need faster builds, or access to specialist hardware like Arm servers.</p>
<p>The Kubernetes solution is fast, but it uses a Pod which runs many jobs, and in order to make it useful enough to run <code>docker run</code>, <code>docker build</code> or to start a Kubernetes cluster, we have to make our machines vulnerable.</p>
<p>With actuated, we set out to re-build the same user experience as GitHub's hosted runners, but without the downsides of self-hosted runners or using Kubernetes Pods for runners.</p>
<p>Actuated runs each build in a microVM on servers that you alone provision and control.</p>
<p>Its centralised control-plane schedules microVMs to each server using an immutable Operating System that is re-built with automation and kept up to date with the latest security patches.</p>
<p>Once the microVM has launched, it connects to GitHub, receives a job, runs to completion and is completely erased thereafter.</p>
<p>You get all of the upsides of self-hosted runners, with a user experience that is as close to GitHub's hosted runners as possible.</p>
<p>Pictured - an Arm Server with 270 GB of RAM and 80 cores - that's a lot of builds.</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62k4gXkAMHX-B?format=jpg&#x26;name=large" alt=""></a></p>
<p>You get to run the following, without worrying about security or side-effects:</p>
<ul>
<li>Docker (<code>docker run</code>) and <code>docker build</code></li>
<li>Kubernetes - Minikube, K3s, KinD</li>
<li><code>sudo</code> / root commands</li>
</ul>
<p>Need to test against a dozen different Kubernetes versions?</p>
<p>Not a problem:</p>
<p><img src="https://actuated.dev/images/k3sup.png" alt="Testing multiple Kubernetes versions"></p>
<p>What about running the same on Arm servers?</p>
<p>Just change <code>runs-on: actuated</code> to <code>runs-on: actuated-aarch64</code> and you're good to go. We test and maintain support for Docker and Kubernetes for both Intel and Arm CPU architectures.</p>
<p>Do you need insights for your Open Source Program Office (OSPO) or for the Technical Steering Committee (TSC)?</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62kSTXgAQLzUb?format=jpg&#x26;name=medium" alt=""></a></p>
<p>We know that no open source project has a single repository that represents all of its activity. Actuated provides insights across an organisation, including total build time and the time queued - which is a reflection of whether you could do with more or fewer build machines.</p>
<p>And we are only just getting started with compiling insights, there's a lot more to come.</p>
<h2>Get involved today</h2>
<p>We've already launched 10,000 VMs for customers jobs, and are now ready to open up the platform to the wider community. So if you'd like to try out what we're offering, we'd love to hear from you. As you offer feedback, you'll get hands on support from our engineering team and get to shape the product through collaboration.</p>
<p>So what does it cost? There is a subscription fee which includes - the control plane for your organisation, the agent software, maintenance of the OS images and our support via Slack. But all the plans are flat-rate, so it may even work out cheaper than paying GitHub for the bigger instances that they offer.</p>
<p>Professional Open Source developers like the ones you see at Red Hat, VMware, Google and IBM, that know how to work in community and understand cloud native are highly sought after and paid exceptionally well. So the open source project you work on has professional full-time engineers allocated to it by one or more companies, as is often the case, then using actuated could pay for itself in a short period of time.</p>
<p>If you represent an open source project that has no funding and is purely maintained by volunteers, what we have to offer may not be suited to your current position. And in that case, we'd recommend you stick with the slower GitHub Runners. Who knows? Perhaps one day GitHub may offer sponsored faster runners at no cost for certain projects?</p>
<p>And finally, what if your repositories are private? Well, we've made you aware of the trade-offs with a static self-hosted runner, or running builds within Kubernetes. It's up to you to decide what's best for your team, and your customers. Actuated works just as well with private repositories as it does with public ones.</p>
<p>See microVMs launching in ~ 1s during a matrix build for testing a Custom Resource Definition (CRD) on different Kubernetes versions:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want to know how actuated works? <a href="https://docs.actuated.dev/faq/">Read the FAQ for more technical details</a>.</p>
<ul>
<li><a href="https://docs.actuated.dev/register/">Find a server for your builds</a></li>
<li><a href="https://docs.actuated.dev/provision-server/">Register for actuated</a></li>
</ul>
<p>Follow us on Twitter - <a href="https://twitter.com/selfactuated">selfactuated</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to make GitHub Actions 22x faster with bare-metal Arm]]></title>
            <link>https://actuated.dev/blog/native-arm64-for-github-actions</link>
            <guid>https://actuated.dev/blog/native-arm64-for-github-actions</guid>
            <pubDate>Tue, 17 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub doesn't provide hosted Arm runners, so how can you use native Arm runners safely & securely?]]></description>
            <content:encoded><![CDATA[<p>GitHub Actions is a modern, fast and efficient way to build and test software, with free runners available. We use the free runners for various open source projects and are generally very pleased with them, after all, who can argue with good enough and free? But one of the main caveats is that GitHub's hosted runners don't yet support the Arm architecture.</p>
<p>So many people turn to software-based emulation using <a href="https://www.qemu.org/">QEMU</a>. QEMU is tricky to set up, and requires specific code and tricks if you want to use software in a standard way, without modifying it. But QEMU is great when it runs with hardware acceleration. Unfortunately, the hosted runners on GitHub do not have KVM available, so builds tend to be incredibly slow, and I mean so slow that it's going to distract you and your team from your work.</p>
<p>This was even more evident when <a href="https://twitter.com/fredbrancz">Frederic Branczyk</a> tweeted about his experience with QEMU on <a href="https://github.com/features/actions">GitHub Actions</a> for his open source observability project named <a href="https://github.com/parca-dev/parca">Parca</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does anyone have a <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> actions self-hosted runner manifest for me to throw at a <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> cluster? I&#39;m tired of waiting for emulated arm64 CI runs taking ages.</p>&mdash; Frederic ðŸ§Š Branczyk @brancz@hachyderm.io (@fredbrancz) <a href="https://twitter.com/fredbrancz/status/1582779459379204096?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I checked out his build and expected "ages" to mean 3 minutes, in fact, it meant 33.5 minutes. I know because I forked his project and ran a test build.</p>
<p>After migrating it to actuated and one of our build agents, the time dropped to 1 minute and 26 seconds, a 22x improvement for zero effort.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>You can see the results here:</p>
<p><a href="https://twitter.com/alexellisuk/status/1583089248084729856/photo/1"><img src="https://pbs.twimg.com/media/FfhC5z1XkAAoYjn?format=jpg&#x26;name=large" alt="Results from the test, side-by-side"></a></p>
<p>As a general rule, the download speed is going to be roughly the same with a hosted runner, it may even be slightly faster due to the connection speed of Azure's network.</p>
<p>But the compilation times speak for themselves - in the Parca build, <code>go test</code> was being run with QEMU. Moving it to run on the ARM64 host directly, resulted in the marked increase in speed. In fact, the team had introduced lots of complicated code to try and set up a Docker container to use QEMU, all that could be stripped out, replacing it with a very standard looking test step:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">the</span> <span class="hljs-string">go</span> <span class="hljs-string">tests</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">go</span> <span class="hljs-string">test</span> <span class="hljs-string">./...</span>
</code></pre>
<h2>Can't I just install the self-hosted runner on an Arm VM?</h2>
<p>There are relatively cheap Arm VMs available from Oracle OCI, Google and Azure based upon the Ampere Altra CPU. AWS have their own Arm VMs available in the Graviton line.</p>
<p>So why shouldn't you just go ahead and install the runner and add them to your repos?</p>
<p>The moment you do that you run into three issues:</p>
<ul>
<li>You now have to maintain the software packages installed on that machine</li>
<li>If you use KinD or Docker, you're going to run into conflicts between builds</li>
<li>Out of the box scheduling is poor - by default it only runs one build at a time there</li>
</ul>
<p>Chasing your tail with package updates, faulty builds due to caching and conflicts is not fun, you may feel like you're saving money, but you are paying with your time and if you have a team, you're paying with their time too.</p>
<p>Most importantly, GitHub say that it cannot be used safely with a public repository. There's no security isolation, and state can be left over from one build to the next, including harmful code left intentionally by bad actors, or accidentally from malware.</p>
<h2>So how do we get to a safer, more efficient Arm runner?</h2>
<p>The answer is to get us as close as possible to a hosted runner, but with the benefits of a self-hosted runner.</p>
<p>That's where actuated comes in.</p>
<p>We run a SaaS that manages bare-metal for you, and talks to GitHub upon your behalf to schedule jobs efficiently.</p>
<ul>
<li>No need to maintain software, we do that for you with an automated OS image</li>
<li>We use microVMs to isolate builds from each other</li>
<li>Every build is immutable and uses a clean environment</li>
<li>We can schedule multiple builds at once without side-effects</li>
</ul>
<p>microVMs on Arm require a bare-metal server, and we have tested all the options available to us. Note that the Arm VMs discussed above do not currently support KVM or nested virtualisation.</p>
<ul>
<li>a1.metal on AWS - 16 cores / 32GB RAM - 300 USD / mo</li>
<li>c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM - 2.5 USD / hr</li>
<li><a href="https://www.hetzner.com/dedicated-rootserver/matrix-rx">RX-Line</a> from <a href="https://hetzner.com">Hetzner</a> with 128GB / 256GB RAM, NVMe &#x26; 80 cores for approx 200-250 EUR / mo.</li>
<li><a href="https://amzn.to/3WiSDE7">Mac Mini M1</a> - 8 cores / 16GB RAM - tested with Asahi Linux - one-time payment of ~ 1500 USD</li>
</ul>
<p>If you're already an AWS customer, the a1.metal is a good place to start. If you need expert support, networking and a high speed uplink, you can't beat Equinix Metal (we have access to hardware there and can help you get started) - you can even pay per minute and provision machines via API. The Mac Mini &#x3C;1 has a really fast NVMe and we're running one of these with Asahi Linux for our own Kernel builds for actuated. The RX Line from Hetzner has serious power and is really quite affordable, but just be aware that you're limited to a 1Gbps connection, a setup fee and monthly commitment, unless you pay significantly more.</p>
<p>I even tried Frederic's Parca job <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">on my 8GB Raspberry Pi with a USB NVMe</a>. Why even bother, do I hear you say? Well for a one-time payment of 80 USD, it was 26m30s quicker than a hosted runner with QEMU!</p>
<p><a href="https://alexellisuk.medium.com/upgrade-your-raspberry-pi-4-with-a-nvme-boot-drive-d9ab4e8aa3c2">Learn how to connect an NVMe over USB-C to your Raspberry Pi 4</a></p>
<h2>What does an Arm job look like?</h2>
<p>Since I first started trying to build code for Arm in 2015, I noticed a group of people who had a passion for this efficient CPU and platform. They would show up on GitHub issue trackers, ready to send patches, get access to hardware and test out new features on Arm chips. It was a tough time, and we should all be grateful for their efforts which go largely unrecognised.</p>
<blockquote>
<p>If you're looking to make your <a href="https://twitter.com/alexellisuk">software compatible with Arm</a>, feel free to reach out to me via Twitter.</p>
</blockquote>
<p>In 2020 when Apple released their M1 chip, Arm went mainstream, and projects that had been putting off Arm support like KinD and Minikube, finally had that extra push to get it done.</p>
<p>I've had several calls with teams who use Docker on their M1/M2 Macs exclusively, meaning they build only Arm binaries and use only Arm images from the Docker Hub. Some of them even ship to project using Arm images, but I think we're still a little behind the curve there.</p>
<p>That means Kubernetes - KinD/Minikube/K3s and Docker - including Buildkit, compose etc, all work out of the box.</p>
<p>I'm going to use the arkade CLI to download KinD and kubectl, however you can absolutely find the download links and do all this manually. I don't recommend it!</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">e2e-kind-test</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">start-kind:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">and</span> <span class="hljs-string">kubectl</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">kubectl:</span> <span class="hljs-string">latest</span>
          <span class="hljs-attr">kind:</span> <span class="hljs-string">latest</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">a</span> <span class="hljs-string">KinD</span> <span class="hljs-string">cluster</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          mkdir -p $HOME/.kube/
          kind create cluster --wait 300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Wait</span> <span class="hljs-string">until</span> <span class="hljs-string">CoreDNS</span> <span class="hljs-string">is</span> <span class="hljs-string">ready</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          kubectl rollout status deploy/coredns -n kube-system --timeout=300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">nodes</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">nodes</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">pods</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">pod</span> <span class="hljs-string">-A</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
</code></pre>
<p>That's our <code>x86_64</code> build, or Intel/AMD build that will run on a hosted runner, but will be kind of slow.</p>
<p>Let's convert it to run on an actuated ARM64 runner:</p>
<pre><code class="hljs language-diff">jobs:
  start-kind:
<span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: actuated-aarch64</span>
</code></pre>
<p>That's it, we've changed the runner type and we're ready to go.</p>
<p><img src="/images/2023-native-arm64-for-oss/in-progress-dashboard.png" alt="In progress build on the dashboard"></p>
<blockquote>
<p>An in progress build on the dashboard</p>
</blockquote>
<p>Behind the scenes, actuated, the SaaS schedules the build on a bare-metal ARM64 server, the boot up takes less than 1 second, and then the standard GitHub Actions Runner talks securely to GitHub to run the build. The build is isolated from other builds, and the runner is destroyed after the build is complete.</p>
<p><img src="/images/2023-native-arm64-for-oss/arm-kind.png" alt="Setting up an Arm KinD cluster took about 49s"></p>
<blockquote>
<p>Setting up an Arm KinD cluster took about 49s</p>
</blockquote>
<p>Setting up an Arm KinD cluster took about 49s, then it's over to you to test your Arm images, or binaries.</p>
<p>If I were setting up CI and needed to test software on both Arm and x86_64, then I'd probably create two separate builds, one for each architecture, with a <code>runs-on</code> label of <code>actuated</code> and <code>actuated-aarch64</code> respectively.</p>
<p>Do you need to test multiple versions of Kubernetes? Let's face it, it changes so often, that who doesn't need to do that. You can use the <code>matrix</code> feature to test multiple versions of Kubernetes on Arm and x86_64.</p>
<p>I show 5x clusters being launched in parallel in the video below:</p>
<p><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Demo - Actuated - secure, isolated CI for containers and Kubernetes</a></p>
<p>What about Docker?</p>
<p>Docker comes pre-installed in the actuated OS images, so you can simply use <code>docker build</code>, without any need to install extra tools like Buildx, or to have to worry about multi-arch Dockerfiles. Although these are always good to have, and are <a href="https://github.com/openfaas/golang-http-template/blob/master/template/golang-middleware/Dockerfile">available out of the box in OpenFaaS</a>, if you're curious what a multi-arch Dockerfile looks like.</p>
<h2>Wrapping up</h2>
<p>Building on bare-metal Arm hosts is more secure because side effects cannot be left over between builds, even if malware is installed by a bad actor. It's more efficient because you can run multiple builds at once, and you can use the latest software with our automated Operating System image. Enabling actuated on a build is as simple as changing the runner type.</p>
<p>And as you've seen from the example with the OSS Parca project, moving to a native Arm server can improve speed by 22x, shaving off a massive 34 minutes per build.</p>
<p>Who wouldn't want that?</p>
<p>Parca isn't a one-off, I was also told by <a href="https://twitter.com/cohix">Connor Hicks from Suborbital</a> that they have an Arm build that takes a good 45 minutes due to using QEMU.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<p>So if that sounds like where you are, reach out to us and we'll get you set up.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register to pilot actuated with us</a></li>
</ul>
<p>Additional links:</p>
<ul>
<li><a href="https://docs.actuated.dev/">Actuated docs</a></li>
<li><a href="https://docs.actuated.dev/faq">FAQ &#x26; comparison to other solutions</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blazing fast CI with MicroVMs]]></title>
            <link>https://actuated.dev/blog/blazing-fast-ci-with-microvms</link>
            <guid>https://actuated.dev/blog/blazing-fast-ci-with-microvms</guid>
            <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[I saw an opportunity to fix self-hosted runners for GitHub Actions. Actuated is now in pilot and aims to solve most if not all of the friction.]]></description>
            <content:encoded><![CDATA[<p>Around 6-8 months ago I started exploring MicroVMs out of curiosity. Around the same time, I saw an opportunity to <strong>fix</strong> self-hosted runners for GitHub Actions. <a href="https://docs.actuated.dev/">Actuated</a> is now in pilot and aims to solve <a href="https://twitter.com/alexellisuk/status/1573599285362532353?s=20&#x26;t=dFcd54c4KIynk6vIGTb7QA">most if not all of the friction</a>.</p>
<p>There's three parts to this post:</p>
<ol>
<li>A quick debrief on Firecracker and MicroVMs vs legacy solutions</li>
<li>Exploring friction with GitHub Actions from a hosted and self-hosted perspective</li>
<li>Blazing fast CI with Actuated, and additional materials for learning more about Firecracker</li>
</ol>
<blockquote>
<p>We're looking for customers who want to solve the problems explored in this post.
<a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot</a></p>
</blockquote>
<h2>1) A quick debrief on Firecracker ðŸ”¥</h2>
<blockquote>
<p>Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services.</p>
</blockquote>
<p>I learned about <a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> mostly by experimentation, building bigger and more useful prototypes. This helped me see what the experience was going to be like for users and the engineers working on a solution. I met others in the community and shared notes with them. Several people asked "Are microVMs the next thing that will replace containers?" I don't think they are, but they are an important tool where hard isolation is necessary.</p>
<p>Over time, one thing became obvious:</p>
<blockquote>
<p>MicroVMs fill a need that legacy VMs and containers can't.</p>
</blockquote>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker, you can replay my deep dive session with Richard Case, Principal Engineer (previously Weaveworks, now at SUSE).</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h2>2) So what's wrong with GitHub Actions?</h2>
<p>First let me say that I think GitHub Actions is a far better experience than Travis ever was, and we have moved all our CI for OpenFaaS, inlets and actuated to Actions for public and private repos. We've built up a good working knowledge in the community and the company.</p>
<p>I'll split this part into two halves.</p>
<h3>What's wrong with hosted runners?</h3>
<p><strong>Hosted runners are constrained</strong></p>
<p>Hosted runners are incredibly convenient, and for most of us, that's all we'll ever need, especially for public repositories with fast CI builds.</p>
<p>Friction starts when the 7GB of RAM and 2 cores allocated causes issues for us - like when we're launching a KinD cluster, or trying to run E2E tests and need more power. Running out of disk space is also a common problem when using Docker images.</p>
<p>GitHub recently launched new paid plans to get faster runners, however the costs add up, the more you use them.</p>
<p>What if you could pay a flat fee, or bring your own hardware?</p>
<p><strong>They cannot be used with public repos</strong></p>
<p>From GitHub.com:</p>
<blockquote>
<p>We recommend that you only use self-hosted runners with private repositories. This is because forks of your public repository can potentially run dangerous code on your self-hosted runner machine by creating a pull request that executes the code in a workflow.</p>
</blockquote>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs.</p>
</blockquote>
<p>Read more about the risks: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners">Self-hosted runner security</a></p>
<p>Despite a stern warning from GitHub, at least one notable CNCF project runs self-hosted ARM64 runners on public repositories.</p>
<p>On one hand, I don't blame that team, they have no other option if they want to do open source, it means a public repo, which means risking everything knowingly.</p>
<p>Is there another way we can help them?</p>
<p>I spoke to the GitHub Actions engineering team, who told me that using an ephemeral VM and an immutable OS image would solve the concerns.</p>
<p><strong>There's no access to ARM runners</strong></p>
<p>Building with QEMU is incredibly slow as Frederic Branczyk, Co-founder, Polar Signals found out when his Parca project was taking 33m5s to build.</p>
<p>I forked it and changed a line: <code>runs-on: actuated-aarch64</code> and reduced the total build time to 1m26s.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><strong>They limit maximum concurrency</strong></p>
<p>On the free plan, you can only launch 20 hosted runners at once, this increases as you pay GitHub more money.</p>
<p><strong>Builds on private repos are billed per minute</strong></p>
<p>I think this is a fair arrangement. GitHub donates Azure VMs to open source users or any public repo for that matter, and if you want to build closed-source software, you can do so by renting VMs per hour.</p>
<p>There's a free allowance for free users, then Pro users like myself get a few more build minutes included. However, These are on the standard, 2 Core 7GB RAM machines.</p>
<p>What if you didn't have to pay per minute of build time?</p>
<h3>What's wrong with self-hosted runners?</h3>
<p><strong>It's challenging to get all the packages right as per a hosted runner</strong></p>
<p>I spent several days running and re-running builds to get all the software required on a self-hosted runner for the private repos for <a href="https://www.openfaas.com/pricing/">OpenFaaS Pro</a>. Guess what?</p>
<p>I didn't want to touch that machine again afterwards, and even if I built up a list of apt packages, it'd be wrong in a few weeks. I then had a long period of tweaking the odd missing package and generating random container image names to prevent Docker and KinD from conflicting and causing side-effects.</p>
<p>What if we could get an image that had everything we needed and was always up to date, and we didn't have to maintain that?</p>
<p><strong>Self-hosted runners cause weird bugs due to caching</strong></p>
<p>If your job installs software like apt packages, the first run will be different from the second. The system is mutable, rather than immutable and the first problem I faced was things clashing like container names or KinD cluster names.</p>
<p><strong>You get limited to one job per machine at a time</strong></p>
<p>The default setup is for a self-hosted Actions Runner to only run one job at a time to avoid the issues I mentioned above.</p>
<p>What if you could schedule as many builds as made sense for the amount of RAM and core the host has?</p>
<p><strong>Docker isn't isolated at all</strong></p>
<p>If you install Docker, then the runner can take over that machine since Docker runs at root on the host. If you try user-namespaces, many things break in weird and frustrating aways like Kubernetes.</p>
<p>Container images and caches can cause conflicts between builds.</p>
<p><strong>Kubernetes isn't a safe alternative</strong></p>
<p>Adding a single large machine isn't a good option because of the dirty cache, weird stateful errors you can run into, and side-effects left over on the host.</p>
<p>So what do teams do?</p>
<p>They turn to a controller called <a href="https://github.com/actions/actions-runner-controller">Actions Runtime Controller (ARC)</a>.</p>
<p>ARC is non trivial to set up and requires you to create a GitHub App or PAT (please don't do that), then to provision, monitor, maintain and upgrade a bunch of infrastructure.</p>
<p>This controller starts a number of re-usable (not one-shot) Pods and has them register as a runner for your jobs. Unfortunately, they still need to use Docker or need to run Kubernetes which leads us to two awful options:</p>
<ol>
<li>Sharing a Docker Socket (easy to become root on the host)</li>
<li>Running Docker In Docker (requires a privileged container, root on the host)</li>
</ol>
<p>There is a third option which is to use a non-root container, but that means you can't use <code>sudo</code> in your builds. You've now crippled your CI.</p>
<p>What if you don't need to use Docker build/run, Kaniko or Kubernetes in CI at all? Well ARC may be a good solution for you, until the day you do need to ship a container image.</p>
<h2>3) Can we fix it? Yes we can.</h2>
<p><a href="https://docs.actuated.dev/">Actuated</a> ("cause (a machine or device) to operate.") is a semi-managed solution that we're building at OpenFaaS Ltd.</p>
<p><img src="https://docs.actuated.dev/images/conceptual-high-level.png" alt="A semi-managed solution, where you provide hosts and we do the rest."></p>
<blockquote>
<p>A semi-managed solution, where you provide hosts and we do the rest.</p>
</blockquote>
<p>You provide your own hosts to run jobs, we schedule to them and maintain a VM image with everything you need.</p>
<p>You install our GitHub App, then change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code> or <code>runs-on: actuated-aarch64</code> for ARM.</p>
<p>Then, provision one or more VMs with nested virtualisation enabled on GCP, DigitalOcean or Azure, or a bare-metal host, and <a href="https://docs.actuated.dev/add-agent/">install our agent</a>. That's it.</p>
<p>If you need ARM support for your project, the <a href="https://aws.amazon.com/ec2/instance-types/a1/">a1.metal from AWS</a> is ideal with 16 cores and 32GB RAM, or an <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a> machine like the c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM if you really need to push things. The 2020 M1 Mac Mini also works well with <a href="https://asahilinux.org/">Asahi Linux</a>, and can be maxed out at 16GB RAM / 8 Cores. <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">I even tried Frederic's Parca job on my Raspberry Pi</a> and it was 26m30s quicker than a hosted runner!</p>
<p>Whenever a build is triggered by a repo in your organisation, the control plane will schedule a microVM on one of your own servers, then GitHub takes over from there. When the GitHub runner exits, we forcibly delete the VM.</p>
<p>You get:</p>
<ul>
<li>A fresh, isolated VM for every build, no re-use at all</li>
<li>A fast boot time of ~ &#x3C;1-2s</li>
<li>An immutable image, which is updated regularly and built with automation</li>
<li>Docker preinstalled and running at boot-up</li>
<li>Efficient scheduling and packing of builds to your fleet of hosts</li>
</ul>
<p>It's capable of running Docker and Kubernetes (KinD, kubeadm, K3s) with full isolation. You'll find some <a href="https://docs.actuated.dev/">examples in the docs</a>, but anything that works on a hosted runner we expect to work with actuated also.</p>
<p>Here's what it looks like:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want the deeply technical information and comparisons? <a href="https://docs.actuated.dev/faq/">Check out the FAQ</a></p>
<p>You may also be interested in a debug experience that we're building for GitHub Actions. It can be used to launch a shell session over SSH with hosted and self-hosted runners: <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">Debug GitHub Actions with SSH and launch a cloud shell</a></p>
<h2>Wrapping up</h2>
<p>We're piloting actuated with customers today. If you're interested in faster, more isolated CI without compromising on security, we would like to hear from you.</p>
<p><strong>Register for the pilot</strong></p>
<p>We're looking for customers to participate in our pilot.</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot ðŸ“</a></p>
<p>Actuated is live in pilot and we've already run thousands of VMs for our customers, but we're only just getting started here.</p>
<p><img src="https://blog.alexellis.io/content/images/2022/11/vm-launch.png" alt="VM launch events over the past several days"></p>
<blockquote>
<p>Pictured: VM launch events over the past several days</p>
</blockquote>
<p>Other links:</p>
<ul>
<li><a href="https://docs.actuated.dev/faq/">Read the FAQ</a></li>
<li><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Watch a short video demo</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>
<p><strong>What about GitLab?</strong></p>
<p>We're focusing on GitHub Actions users for the pilot, but have a prototype for GitLab. If you'd like to know more, reach out using the <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Apply for the pilot form</a>.</p>
<p><strong>Just want to play with Firecracker or learn more about microVMs vs legacy VMs and containers?</strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch A cracking time: Exploring Firecracker &#x26; MicroVMs</a></li>
<li><a href="https://github.com/alexellis/firecracker-init-lab">Try my firecracker lab on GitHub - alexellis/firecracker-init-lab</a></li>
</ul>
<h2>What are people saying about actuated?</h2>
<blockquote>
<p>"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff."</p>
<p>Addison van den Hoeven - DevOps Lead, Riskfuel</p>
</blockquote>
<blockquote>
<p>"Actuated looks super cool, interested to see where you take it!"</p>
<p>Guillermo Rauch, CEO Vercel</p>
</blockquote>
<blockquote>
<p>"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here."</p>
<p>Richard Case, Principal Engineer, SUSE</p>
</blockquote>
<blockquote>
<p>"Thank you. I think actuated is amazing."</p>
<p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p>
</blockquote>
<blockquote>
<p>"Nice work, security aspects alone with shared/stale envs on self-hosted runners."</p>
<p>Matt Johnson, Palo Alto Networks</p>
</blockquote>
<blockquote>
<p>"Is there a way to pay github for runners that suck less?"</p>
<p>Darren Shepherd, Acorn Labs</p>
</blockquote>
<blockquote>
<p>"Excited to try out actuated! We use custom actions runners and I think there's something here ðŸ”¥"</p>
<p>Nick Gerace, System Initiative</p>
</blockquote>
<blockquote>
<p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running Github Actions in isolated VMs in seconds (vs minutes)."</p>
<p>Rinat Abdullin, ML &#x26; Innovation at Trustbit</p>
</blockquote>
<blockquote>
<p>This is awesome!" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p>
<p>Frederic Branczyk, Co-founder, Polar Signals</p>
</blockquote>]]></content:encoded>
        </item>
    </channel>
</rss>