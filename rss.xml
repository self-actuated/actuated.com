<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Actuated - blog</title>
        <link>https://actuated.dev</link>
        <description>Keep your team productive &amp; focused with blazing fast CI</description>
        <lastBuildDate>Fri, 17 Feb 2023 18:45:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <image>
            <title>Actuated - blog</title>
            <url>https://actuated.dev/images/actuated.png</url>
            <link>https://actuated.dev</link>
        </image>
        <item>
            <title><![CDATA[How to run a KVM guest in your GitHub Actions]]></title>
            <link>https://actuated.dev/blog/kvm-in-github-actions</link>
            <guid>https://actuated.dev/blog/kvm-in-github-actions</guid>
            <pubDate>Fri, 17 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[From building cloud images, to running NixOS tests and the android emulator, we look at how and why you'd want to run a VM in GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>GitHub's hosted runners do not support nested virtualization. This means some frequently used tools that require KVM like packer, the android emulator, etc can not be used in GitHub Actions CI pipelines.</p>
<p>We noticed there are quite a few issues for people requesting KVM support for GitHub Actions:</p>
<ul>
<li><a href="https://github.com/actions/runner-images/issues/183">Enable nested virtualization · Issue #183 · actions/runner-images</a></li>
<li><a href="https://github.com/community/community/discussions/8305">Revisiting KVM support for Hosted GitHub Actions · Discussion #8305 · community/community</a></li>
<li><a href="https://github.com/WikiWatershed/model-my-watershed/pull/3586">[Experimental] Add CI GitHub Actions workflow by rajadain · Pull Request #3586 · WikiWatershed/model-my-watershed</a></li>
</ul>
<p>As mentioned in some of these issues, an alternative would be to run your own self-hosted runner on a bare metal host. This comes with the downside that builds can conflict and cause side effects to system-level packages. On top if this self-hosted runners are considered <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">insecure for public repositories</a>.</p>
<p>Solutions like the "actions-runtime-controller" or ARC that use Kubernetes to orchestrate and run self-hosted runners in Pods are also out of scope if you need to run VMs.</p>
<p>With Actuated we make it possible to launch a Virtual Machine (VM) within a GitHub Action. Jobs are launched in isolated VMs just like GitHub hosted runners but with support for nested virtualization.</p>
<h2>Case Study: Githedgehog</h2>
<p>One of our customers Sergei Lukianov, founding engineer at <a href="https://githedgehog.com">Githedgehog</a> told us he needed somewhere to build Docker images and to test them with Kubernetes, he uses KinD for that.</p>
<p>Prior to adopting Actuated, his team used hosted runners which are considerably slower, and paid on a per minute basis. Actuated made his builds both faster, and more secure than using any of the alternatives for self-hosted runners.</p>
<p>It turned out that he also needed to launch VMs in those jobs, and that's something else that hosted runners cannot cater for right now. Actuated’s KVM guest support means he can run all of his workloads on fast hardware.</p>
<p>Some other common use cases that require KVM support on the CI runner:</p>
<ul>
<li>Running <a href="https://www.packer.io/">Packer</a> for creating Amazon Machine Images (AMI) or VM images for other cloud platforms.</li>
<li>Accelerating the <a href="https://developer.android.com/studio/run/emulator-commandline">Android Emulator</a> via KVM.</li>
<li>Running <a href="https://nixos.org/">NixOS</a> tests or builds that depend on VMs.</li>
<li>Testing software that can only be done with <a href="https://www.linux-kvm.org/page/Main_Page">KVM</a> or in a VM.</li>
</ul>
<h2>Running VMs in GitHub Actions</h2>
<p>In this section we will walk you through a couple of hands-on examples.</p>
<h3>Firecracker microVM</h3>
<p>In this example we are going to follow the Firecracker quickstart guide to boot up a Firecracker VM but instead of running it on our local machine we will run it from within a GitHub Actions workflow.</p>
<p>The workflow instals Firecracker, configures and boots a guest VM and then waits 20 seconds before shutting down the VM and exiting the workflow. The image below shows the run logs of the workflow. We see the login prompt of the running microVM.</p>
<p><img src="/images/2023-02-17-kvm-in-github-actions/nested-firecracker.png" alt="Running a firecracker microVM in a GitHub Actions job"></p>
<blockquote>
<p>Running a firecracker microVM in a GitHub Actions job</p>
</blockquote>
<p>Here is the workflow file used by this job:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">vm-run</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">vm-run:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">firecracker</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">arkade</span> <span class="hljs-string">system</span> <span class="hljs-string">install</span> <span class="hljs-string">firecracker</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">microVM</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">./run-vm.sh</span>
</code></pre>
<p>The <a href="https://github.com/alexellis/setup-arkade">setup-arkade</a> is to install arkade on the runner. Next firecracker is installed from the arkade system apps.</p>
<p>As a last step we run a firecracker microVM. The <code>run-vm.sh</code> script is based on the <a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/getting-started.md">firecracker quickstart</a> and collects all the steps into a single script that can be run in the CI pipeline.</p>
<p>It script will:</p>
<ul>
<li>Get the kernel and rootfs for the microVM</li>
<li>Start fireckracker and configure the guest kernel and rootfs</li>
<li>Start the guest machine</li>
<li>Wait for 20 seconds and kill the firecracker process so workflow finishes.</li>
</ul>
<p>The <code>run-vm.sh</code> script:</p>
<pre><code class="hljs language-sh"><span class="hljs-meta">#!/bin/bash</span>

<span class="hljs-comment"># Get a kernel and rootfs</span>
<span class="hljs-built_in">arch</span>=`<span class="hljs-built_in">uname</span> -m`
dest_kernel=<span class="hljs-string">"hello-vmlinux.bin"</span>
dest_rootfs=<span class="hljs-string">"hello-rootfs.ext4"</span>
image_bucket_url=<span class="hljs-string">"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/<span class="hljs-variable">$arch</span>"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$kernel</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_kernel</span> <span class="hljs-variable">$kernel</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$rootfs</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_rootfs</span> <span class="hljs-variable">$rootfs</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Saved kernel file to <span class="hljs-variable">$dest_kernel</span> and root block device to <span class="hljs-variable">$dest_rootfs</span>."</span>

<span class="hljs-comment"># Start firecracker</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Starting firecracker"</span>
firecracker --api-sock /tmp/firecracker.socket &#x26;
firecracker_pid=$!

<span class="hljs-comment"># Set the guest kernel and rootfs</span>
rch=`<span class="hljs-built_in">uname</span> -m`
kernel_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-vmlinux.bin"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"keep_bootcon console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

rootfs_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-rootfs.ext4"</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/drives/rootfs'</span> \
  -H <span class="hljs-string">'Accept: application/json'</span>           \
  -H <span class="hljs-string">'Content-Type: application/json'</span>     \
  -d <span class="hljs-string">"{
        \"drive_id\": \"rootfs\",
        \"path_on_host\": \"<span class="hljs-variable">${rootfs_path}</span>\",
        \"is_root_device\": true,
        \"is_read_only\": false
   }"</span>

<span class="hljs-comment"># Start the guest machine</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/actions'</span>       \
  -H  <span class="hljs-string">'Accept: application/json'</span>          \
  -H  <span class="hljs-string">'Content-Type: application/json'</span>    \
  -d <span class="hljs-string">'{
      "action_type": "InstanceStart"
   }'</span>

<span class="hljs-comment"># Kill the firecracker process to exit the workflow</span>
<span class="hljs-built_in">sleep</span> 20
<span class="hljs-built_in">kill</span> -9 <span class="hljs-variable">$firecracker_pid</span>

</code></pre>
<p>The full example can be found on <a href="https://github.com/skatolo/nested-firecracker">GitHub</a></p>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker you can watch Alex's webinar on the topic.</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h3>NixOS integration tests</h3>
<p>With nix there is the ability to provide a set of declarative configuration to define integration tests that spin up virtual machines using <a href="https://www.qemu.org/">QEMU</a> as the backend. While running these tests in CI without hardware acceleration is supported this is considerably slower.</p>
<p>For a more detailed overview of the test setup and configuration see the original article in the NixOS guides:</p>
<ul>
<li><a href="https://nixos.org/guides/integration-testing-using-virtual-machines.html">Integration testing using virtual machines (VMs)</a></li>
</ul>
<p>The workflow file for running NixOS tests on  GitHub Actions:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">nixos-tests</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">nixos-test:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-python@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">python-version:</span> <span class="hljs-string">'3.x'</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v16</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">extra_nix_config:</span> <span class="hljs-string">"system-features = nixos-test benchmark big-parallel kvm"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NixOS</span> <span class="hljs-string">test</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">nix</span> <span class="hljs-string">build</span> <span class="hljs-string">-L</span> <span class="hljs-string">.#checks.x86_64-linux.postgres</span>
</code></pre>
<p>We just install Nix using the <a href="https://github.com/cachix/install-nix-action">install-nix-action</a> and run the tests in the next step.</p>
<p>The full example is available on <a href="https://github.com/skatolo/gh-actions-nixos-tests">GitHub</a></p>
<h3>Other examples of using a VM</h3>
<p>In the previous section we showed you some brief examples for the kind of workflows you can run. Here are some other resources and tutorials that should be easy to adapt and run in CI.</p>
<ul>
<li>Create KVM virtual machine images with the <a href="https://developer.hashicorp.com/packer/plugins/builders/qemu">Packer QEMU Builder</a></li>
<li>Launching an Ubuntu cloud image with cloud-init: <a href="https://fabianlee.org/2020/02/23/kvm-testing-cloud-init-locally-using-kvm-for-an-ubuntu-cloud-image/">
KVM: Testing cloud-init locally using KVM for an Ubuntu cloud image </a></li>
</ul>
<h2>Conclusion</h2>
<p>Hosted runners do not support nested virtualization. That makes them unsuitable for running CI jobs that require KVM support.</p>
<p>For Actuated runners we provide a custom Kernel that enables KVM support. This will allow you to run Virtual Machines within your CI jobs.</p>
<p>At time of writing there is no support for aarch64 runners. Only Intel and AMD CPUs support nested virtualisation.</p>
<p>While it is possible to deploy your own self-hosted runners to run jobs that need KVM support, this is not recommended:</p>
<ul>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></li>
</ul>
<p>Want to see a demo or talk to our team? Register <a href="https://forms.gle/8XmpTTWXbZwWkfqT6">here</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Make your builds run faster with Caching for GitHub Actions]]></title>
            <link>https://actuated.dev/blog/caching-in-github-actions</link>
            <guid>https://actuated.dev/blog/caching-in-github-actions</guid>
            <pubDate>Fri, 10 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how we made a Golang project build 4x faster using GitHub's built-in caching mechanism.]]></description>
            <content:encoded><![CDATA[<p>GitHub provides a <a href="https://github.com/actions/cache">cache action</a> that allows caching dependencies and build outputs to improve workflow execution time.</p>
<p>A common use case would be to cache packages and dependencies from tools such as npm, pip, Gradle, ... . If you are using Go, caching go modules and the build cache can save you a significant amount of build time as we will see in the next section.</p>
<p>Caching can be configured manually, but a lot of setup actions already use the <a href="https://github.com/actions/cache">actions/cache</a> under the hood and provide a configuration option to enable caching.</p>
<p>We use the actions cache to speed up workflows for building the Actuated base images. As part of those workflows we build a kernel and then a rootfs. Since the kernel’s configuration is changed infrequently it makes sense to cache that output.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/build-time-comparison.png" alt="Build time comparison"></p>
<blockquote>
<p>Comparing workflow execution times with and without caching.</p>
</blockquote>
<p>Building the kernel takes around <code>1m20s</code> on our aarch-64 Actuated runner and <code>4m10s</code> for the x86-64 build so we get some significant time improvements by caching the kernel.</p>
<p>The output of the cache action can also be used to do something based on whether there was a cache hit or miss. We use this to skip the kernel publishing step when there was a cache hit.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">if:</span> <span class="hljs-string">${{</span> <span class="hljs-string">steps.cache-kernel.outputs.cache-hit</span> <span class="hljs-type">!=</span> <span class="hljs-string">'true'</span> <span class="hljs-string">}}</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">Kernel</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">make</span> <span class="hljs-string">publish-kernel-x86-64</span>
</code></pre>
<h2>Caching Go dependency files and build outputs</h2>
<p>In this minimal example we are going to setup caching for Go dependency files and build outputs. As an example we will be building <a href="https://github.com/alexellis/registry-creds">alexellis/registry-creds</a>. This is a Kubernetes operator that can be used to replicate Kubernetes ImagePullSecrets to all namespaces.</p>
<p>It has the K8s API as a dependency which is quite large so we expect to save some time by cashing the Go mod download. By also caching the Go build cache it should be possible to speed up the workflow even more.</p>
<h3>Configure caching manually</h3>
<p>We will first create the workflow and run it without any caching.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">ci</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">"alexellis/registry-creds"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</span></code></pre>
<p>The <a href="https://github.com/actions/checkout">checkout action</a> is used to check out the registry-creds repo so the workflow can access it. The next step sets up Go using the <a href="https://github.com/actions/setup-go">setup-go action</a> and as a last step we run <code>go build</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/no-cache-workflow.png" alt="No cache workflow run"></p>
<p>When triggering this workflow we see that each run takes around <code>1m20s</code>.</p>
<p>Modify the workflow and add an additional step to configure the caches using the <a href="https://github.com/actions/cache">cache action</a>:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span> <span class="hljs-string">caches</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/cache@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">path:</span> <span class="hljs-string">|
        ~/.cache/go-build
        ~/go/pkg/mod
</span>      <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-golang-${{</span> <span class="hljs-string">hashFiles('**/go.sum')</span> <span class="hljs-string">}}</span>
      <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
        ${{ runner.os }}-golang-
</span></code></pre>
<p>The <code>path</code> parameter is used to set the paths on the runner to cache or restore. The <code>key</code> parameter sets the key used when saving the cache. A hash of the go.sum file is used as part of the cache key.</p>
<p>Optionally the restore-keys are used to find and restore a cache if there was no hit for the key. In this case we always restore the cache even if there was no specific hit for the go.sum file.</p>
<p>The first time this workflow is run the cache is not populated so we see a similar execution time as without any cache of around <code>1m20s</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/workflow-cache-comparison.png" alt="Comparing workflow runs"></p>
<p>Running the workflow again we can see that it now completes in just <code>18s</code>.</p>
<h3>Use setup-go built-in caching</h3>
<p>The V3 edition of the <a href="https://github.com/actions/setup-go">setup-go</a> action has support for caching built-in. Under the hood it also uses the <a href="https://github.com/actions/cache">actions/cache</a> with a similar configuration as in the example above.</p>
<p>The advantage of using the built-in functionality is that it requires less configuration settings. Caching can be enabled by adding a single line to the workflow configuration:</p>
<pre><code class="hljs language-diff">name: ci

on: push

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          repository: "alexellis/registry-creds"
      - name: Setup Golang
        uses: actions/setup-go@v3
        with:
          go-version: ~1.19
<span class="hljs-addition">+         cache: true</span>
      - name: Build
        run: |
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</code></pre>
<p>Triggering the workflow with the build-in cache yields similar time gains as with the manual cache configuration.</p>
<h2>Conclusion</h2>
<p>We walked you through a short example to show you how to set up caching for a Go project and managed to build the project 4x faster.</p>
<p>If you are building with Docker you can use <a href="https://docs.docker.com/build/ci/github-actions/examples/#cache">Docker layer caching</a> to make your builds faster. Buildkit automatically caches the build results and allows exporting the cache to an external location. It has support for <a href="https://docs.docker.com/build/cache/backends/">uploading the build cache to GitHub Actions cache</a></p>
<p>See also: <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows">GitHub: Caching dependencies in Workflows</a></p>
<p>Keep in mind that there are some limitations to the GitHub Actions cache. Cache entries that have not been accessed in over 7 days will be removed. There is also a limit on the total cache size of 10 GB per repository.</p>
<p>Some points to take away:</p>
<ul>
<li>Using the actions cache is not limited to GitHub hosted runners but can be used with self-hosted runners as well.</li>
<li>Workflows using the cache action can be converted to run on Actuated runners without any modifications.</li>
<li>Jobs on Actuated runners start in a clean VM each time. This means dependencies need to be downloaded and build artifacts or caches rebuilt each time. Caching these files in the actions cache can improve workflow execution time.</li>
</ul>
<blockquote>
<p>Want to learn more about Go and GitHub Actions?</p>
<p>Alex's eBook <a href="https://openfaas.gumroad.com/l/everyday-golang">Everyday Golang</a> has a chapter dedicated to building Go programs with Docker and GitHub Actions.</p>
</blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The efficient way to publish multi-arch containers from GitHub Actions]]></title>
            <link>https://actuated.dev/blog/multi-arch-docker-github-actions</link>
            <guid>https://actuated.dev/blog/multi-arch-docker-github-actions</guid>
            <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to publish container images for both Arm and Intel machines from GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>In 2017, I wrote an article on <a href="https://docs.docker.com/build/building/multi-stage/">multi-stage builds with Docker</a>, and it's now part of the Docker Documentation. In my opinion, multi-arch builds were the proceeding step in the evolution of container images.</p>
<h2>What's multi-arch and why should you care?</h2>
<p>If you want users to be able to use your containers on different types of computer, then you'll often need to build different versions of your binaries and containers.</p>
<p>The <a href="https://github.com/openfaas/faas-cli">faas-cli</a> tool is how users interact with <a href="https://github.com/openfaas/faas">OpenFaaS</a>.</p>
<p>It's distributed in binary format for users, with builds for Windows, MacOS and Linux.</p>
<ul>
<li><code>linux/amd64</code>, <code>linux/arm64</code>, <code>linux/arm/v7</code></li>
<li><code>darwin/amd64</code>, <code>darwin/arm64</code></li>
<li><code>windows/amd64</code></li>
</ul>
<p>But why are there six different binaries for three Operating Systems? With the advent of Raspberry Pi, M1 Macs (Apple Silicon) and AWS Graviton servers, we have had to start building binaries for more than just Intel systems.</p>
<p>If you're curious how to build multi-arch binaries with Go, you can check out the release process for the open source arkade tool here, which is a simpler example than faas-cli: <a href="https://github.com/alexellis/arkade/blob/master/Makefile">arkade Makefile</a> and <a href="https://github.com/alexellis/arkade/blob/master/.github/workflows/publish.yml">GitHub Actions publish job</a></p>
<p>So if we have to support at least six different binaries for Open Source CLIs, what about container images?</p>
<h2>Do we need multi-arch containers too?</h2>
<p>Until recently, it was common to hear people say: "I can't find any containers that work for Arm". This was because the majority of container images were built only for Intel. Docker Inc has done a sterling job of making their "official" images work on different platforms, that's why you can now run <code>docker run -t -i ubuntu /bin/bash</code> on a Raspberry Pi, M1 Mac and your regular PC.</p>
<p>Many open source projects have also caught on to the need for multi-arch images, but there are still a few like Bitnami, haven't yet seen value. I think that is OK, this kind work does take time and effort. Ultimately, it's up to the project maintainers to listen to their users and decide if they have enough interest to add support for Arm.</p>
<p>A multi-arch image is a container that will work on two or more different combinations of operating system and CPU architecture.</p>
<p>Typically, this would be:</p>
<ul>
<li><code>linux/amd64</code> - "normal" computers made by Intel or AMD</li>
<li><code>linux/arm64</code> - 64-bit Arm servers like <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-graviton-performance-testing/what-is-aws-graviton.html">AWS Graviton</a> or <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a></li>
<li><code>linux/arm/v7</code> - the 32-bit Raspberry Pi Operating System</li>
</ul>
<p>So multi-arch is really about catering for the needs of Arm users. Arm hardware platforms like the Ampere Altra come with 80 efficient CPU cores, have a very low TDP compared to traditional Intel hardware, and are available from various cloud providers.</p>
<h2>How do we build multi-arch containers work?</h2>
<p>There are a few tools and tricks that we can combine together to take a single Dockerfile and output an image that anyone can pull, which will be right for their machine.</p>
<p>Let's take the: <code>ghcr.io/inlets-operator:latest</code> image from <a href="https://inlets.dev/">inlets</a>.</p>
<p>When a user types in <code>docker pull</code>, or deploys a Pod to Kubernetes, their local containerd daemon will fetch the manifest file and inspect it to see what SHA reference to use for to download the required layers for the image.</p>
<p><img src="/images/2023-02-multi-arch/multi-arch.png" alt="How manifests work"></p>
<blockquote>
<p>How manifests work</p>
</blockquote>
<p>Let's look at a manifest file with the crane tool. I'm going to use <a href="https://arkade.dev">arkade</a> to install crane:</p>
<pre><code class="hljs language-bash">arkade get crane

crane manifest ghcr.io/inlets/inlets-operator:latest
</code></pre>
<p>You'll see a manifests array, with a platform section for each image:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.list.v2+json"</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">"manifests"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:bae8025e080d05f1db0e337daae54016ada179152e44613bf3f8c4243ad939df"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"amd64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:3ddc045e2655f06653fc36ac88d1d85e0f077c111a3d1abf01d05e6bbc79c89f"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"arm64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<h2>How do we convert a Dockerfile to multi-arch?</h2>
<p>Instead of using the classic version of Docker, we can enable the buildx and Buildkit plugins which provide a way to build multi-arch images.</p>
<p>We'll continue with the Dockerfile from the open source inlets-operator project.</p>
<p>Within <a href="https://github.com/inlets/inlets-operator/blob/master/Dockerfile">the Dockerfile</a>, we need to make a couple of changes.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- FROM golang:1.18 as builder</span>
<span class="hljs-addition">+ FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder</span>

<span class="hljs-addition">+ ARG TARGETPLATFORM</span>
<span class="hljs-addition">+ ARG BUILDPLATFORM</span>
<span class="hljs-addition">+ ARG TARGETOS</span>
<span class="hljs-addition">+ ARG TARGETARCH</span>
</code></pre>
<p>The BUILDPLATFORM variable is the native architecture and platform of the machine performing the build, this is usually amd64.</p>
<p>The TARGETPLATFORM is important for the final step of the build, and will normally be injected based upon one each of the platforms you have specified for the build command.</p>
<p>For Go specifically, we also updated the <code>go build</code> command to tell Go to use cross-compilation based upon the TARGETOS and TARGETARCH environment variables, which are populated by Docker.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- go build -o inlets-operator</span>
<span class="hljs-addition">+ GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o inlets-operator</span>
</code></pre>
<p>Here's the full example:</p>
<pre><code>FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder

ARG TARGETPLATFORM
ARG BUILDPLATFORM
ARG TARGETOS
ARG TARGETARCH

ARG Version
ARG GitCommit

ENV CGO_ENABLED=0
ENV GO111MODULE=on

WORKDIR /go/src/github.com/inlets/inlets-operator

# Cache the download before continuing
COPY go.mod go.mod
COPY go.sum go.sum
RUN go mod download

COPY .  .

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go test -v ./...

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go build -ldflags "-s -w -X github.com/inlets/inlets-operator/pkg/version.Release=${Version} -X github.com/inlets/inlets-operator/pkg/version.SHA=${GitCommit}" \
  -a -installsuffix cgo -o /usr/bin/inlets-operator .

FROM --platform=${BUILDPLATFORM:-linux/amd64} gcr.io/distroless/static:nonroot

LABEL org.opencontainers.image.source=https://github.com/inlets/inlets-operator

WORKDIR /
COPY --from=builder /usr/bin/inlets-operator /
USER nonroot:nonroot

CMD ["/inlets-operator"]
</code></pre>
<h2>How to do you configure GitHub Actions to publish multi-arch images?</h2>
<p>Now that the Dockerfile has been configured, it's time to start working on the GitHub Action.</p>
<p>This example is taken from the Open Source <a href="https://github.com/inlets/inlets-operator">inlets-operator</a>. It builds a container image containing a Go binary and uses a Dockerfile in the root of the repository.</p>
<p>View <a href="https://github.com/inlets/inlets-operator/blob/master/.github/workflows/publish.yaml">publish.yaml</a>, adapted for actuated:</p>
<pre><code class="hljs language-diff">name: publish

on:
  push:
    tags:
      - '*'

jobs:
  publish:
<span class="hljs-addition">+    permissions:</span>
<span class="hljs-addition">+      packages: write</span>

<span class="hljs-deletion">-   runs-on: ubuntu-latest</span>
<span class="hljs-addition">+   runs-on: actuated</span>
    steps:
      - uses: actions/checkout@master
        with:
          fetch-depth: 1

<span class="hljs-addition">+     - name: Setup mirror</span>
<span class="hljs-addition">+       uses: self-actuated/hub-mirror@master</span>
      - name: Get TAG
        id: get_tag
        run: echo TAG=${GITHUB_REF#refs/tags/} >> $GITHUB_ENV
      - name: Get Repo Owner
        id: get_repo_owner
        run: echo "REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')" > $GITHUB_ENV

<span class="hljs-addition">+     - name: Set up QEMU</span>
<span class="hljs-addition">+       uses: docker/setup-qemu-action@v2</span>
<span class="hljs-addition">+     - name: Set up Docker Buildx</span>
<span class="hljs-addition">+       uses: docker/setup-buildx-action@v2</span>
      - name: Login to container Registry
        uses: docker/login-action@v2
        with:
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
          registry: ghcr.io

      - name: Release build
        id: release_build
        uses: docker/build-push-action@v3
        with:
          outputs: "type=registry,push=true"
<span class="hljs-addition">+         platforms: linux/amd64,linux/arm/v6,linux/arm64</span>
          build-args: |
            Version=${{  env.TAG }}
            GitCommit=${{ github.sha }}
          tags: |
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ env.TAG }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:latest
</code></pre>
<p>All of the images and corresponding manifest are published to GitHub's Container Registry (GHCR). The action itself is able to authenticate to GHCR using a built-in, short-lived token. This is dependent on the "permissions" section and "packages: write" being set.</p>
<p>You'll see that we added a <code>Setup mirror</code> step, this explained in the <a href="/examples/registry-mirror">Registry Mirror example</a> and is not required for Hosted Runners.</p>
<p>The <code>docker/setup-qemu-action@v2</code> step is responsible for setting up QEMU, which is used to emulate the different CPU architectures.</p>
<p>The <code>docker/build-push-action@v3</code> step is responsible for passing in a number of platform combinations such as: <code>linux/amd64</code> for cloud, <code>linux/arm64</code> for Arm servers and <code>linux/arm/v6</code> for Raspberry Pi.</p>
<h2>What if you're not using GitHub Actions?</h2>
<p>The various GitHub Actions published by the Docker team are a great way to get started, but if you look under the hood, they're just syntactic sugar for the Docker CLI.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> DOCKER_CLI_EXPERIMENTAL=enabled

<span class="hljs-comment"># Have Docker download the latest buildx plugin</span>
docker buildx install

<span class="hljs-comment"># Create a buildkit daemon with the name "multiarch"</span>
docker buildx create \
    --use \
    --name=multiarch \
    --node=multiarch

<span class="hljs-comment"># Install QEMU</span>
docker run --<span class="hljs-built_in">rm</span> --privileged \
    multiarch/qemu-user-static --reset -p <span class="hljs-built_in">yes</span>

<span class="hljs-comment"># Run a build for the different platforms</span>
docker buildx build \
    --platform=linux/arm64,linux/amd64 \
    --output=<span class="hljs-built_in">type</span>=registry,push=<span class="hljs-literal">true</span> --tag image:tag .
</code></pre>
<p>For OpenFaaS users, we do all of the above any time you type in <code>faas-cli publish</code> and the <code>faas-cli build</code> command just runs a regular Docker build, without any of the multi-arch steps.</p>
<p>If you're interested, you can checkout the code here: <a href="https://github.com/openfaas/faas-cli/blob/master/commands/publish.go">publish.go</a>.</p>
<h2>Putting it all together</h2>
<ul>
<li>CLIs are published for many different combinations of OS and CPU, but containers are usually only required for Linux with an amd64 or Arm CPU.</li>
<li>Multi-arch images work through a manifest, which then tells containerd which image is needs for the platform it is running on.</li>
<li>QEMU is a tool for emulating different CPU architectures, and is used to build the images for the different platforms.</li>
</ul>
<p>In our experience with OpenFaaS, inlets and actuated, once you have converted one or two projects to build multi-arch images, it becomes a lot easier to do it again, and make all software available for Arm servers.</p>
<p>You can learn more about <a href="https://docs.docker.com/build/building/multi-platform/">Multi-platform images</a> in the Docker Documentation.</p>
<p><em>Want more multi-arch examples?</em></p>
<p>OpenFaaS uses multi-arch Dockerfiles for all of its templates, and the examples are freely available on GitHub including Python, Node, Java and Go.</p>
<p>See also: <a href="https://github.com/openfaas/templates">OpenFaaS templates</a></p>
<p><em>A word of caution</em></p>
<p>QEMU can be incredibly slow at times when using a hosted runner, where a build takes takes 1-2 minutes can extend to over half an hour. If you do run into that, one option is to check out actuated or another solution, which can build directly on an Arm server with a securely isolated Virtual Machine.</p>
<p>In <a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a>, we showed how we decreased the build time of an open-source Go project from 30.5 mins to 1.5 mins. If this is the direction you go in, you can use a <a href="https://docs.actuated.dev/examples/matrix/">matrix-build</a> instead of a QEMU-based multi-arch build.</p>
<p>See also: <a href="https://docs.actuated.dev/provision-server/#arm64-aka-aarch64">Recommended bare-metal Arm servers</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/sbom-in-github-actions</link>
            <guid>https://actuated.dev/blog/sbom-in-github-actions</guid>
            <pubDate>Wed, 25 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions in a few easy steps.]]></description>
            <content:encoded><![CDATA[<h2>What is a Software Bill of Materials (SBOM)?</h2>
<p>In <a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">April 2022 Justin Cormack, CTO of Docker announced</a> that Docker was adding support to generate a Software Bill of Materials (SBOM) for container images.</p>
<p>An SBOM is an inventory of the components that make up a software application. It is a list of the components that make up a software application including the version of each component. The version is important because it can be cross-reference with a vulnerability database to determine if the component has any known vulnerabilities.</p>
<p>Many organisations are also required to company with certain Open Source Software (OSS) licenses. So if SBOMs are included in the software they purchase or consume from vendors, then it can be used to determine if the software is compliant with their specific license requirements, lowering legal and compliance risk.</p>
<p>Docker's enhancements to Docker Desktop and their open source Buildkit tool were the result of a collaboration with Anchore, a company that provides a commercial SBOM solution.</p>
<h2>Check out an SBOM for yourself</h2>
<p>Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free.</p>
<ul>
<li><a href="https://github.com/anchore/syft">syft</a> - a command line tool that can be used to generate an SBOM for a container image.</li>
<li><a href="https://github.com/anchore/grype">grype</a> - a command line tool that can be used to scan an SBOM for vulnerabilities.</li>
</ul>
<p>OpenFaaS Community Edition (CE) is a popular open source serverless platform for Kubernetes. It's maintained by open source developers, and is free to use.</p>
<p>Let's pick a container image from the Community Edition of <a href="https://github.com/orgs/openfaasltd/packages">OpenFaaS</a> like the container image for the OpenFaaS gateway.</p>
<p>We can browse the GitHub UI to find the latest revision, or we can use Google's crane tool:</p>
<pre><code class="hljs language-bash">crane <span class="hljs-built_in">ls</span> ghcr.io/openfaas/gateway | <span class="hljs-built_in">tail</span> -n 5
0.26.0
8e1c34e222d6c194302c649270737c516fe33edf
0.26.1
c26ec5221e453071216f5e15c3409168446fd563
0.26.2
</code></pre>
<p>Now we can introduce one of those tags to syft:</p>
<pre><code class="hljs language-bash">syft ghcr.io/openfaas/gateway:0.26.2
 ✔ Pulled image            
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [39 packages]
NAME                                              VERSION                               TYPE      
alpine-baselayout                                 3.4.0-r0                              apk        
alpine-baselayout-data                            3.4.0-r0                              apk        
alpine-keys                                       2.4-r1                                apk        
apk-tools                                         2.12.10-r1                            apk        
busybox                                           1.35.0                                binary     
busybox                                           1.35.0-r29                            apk        
busybox-binsh                                     1.35.0-r29                            apk        
ca-certificates                                   20220614-r4                           apk        
ca-certificates-bundle                            20220614-r4                           apk        
github.com/beorn7/perks                           v1.0.1                                go-module  
github.com/cespare/xxhash/v2                      v2.1.2                                go-module  
github.com/docker/distribution                    v2.8.1+incompatible                   go-module  
github.com/gogo/protobuf                          v1.3.2                                go-module  
github.com/golang/protobuf                        v1.5.2                                go-module  
github.com/gorilla/mux                            v1.8.0                                go-module  
github.com/matttproud/golang_protobuf_extensions  v1.0.1                                go-module  
github.com/nats-io/nats.go                        v1.22.1                               go-module  
github.com/nats-io/nkeys                          v0.3.0                                go-module  
github.com/nats-io/nuid                           v1.0.1                                go-module  
github.com/nats-io/stan.go                        v0.10.4                               go-module  
github.com/openfaas/faas-provider                 v0.19.1                               go-module  
github.com/openfaas/faas/gateway                  (devel)                               go-module  
github.com/openfaas/nats-queue-worker             v0.0.0-20230117214128-3615ccb286cc    go-module  
github.com/prometheus/client_golang               v1.13.0                               go-module  
github.com/prometheus/client_model                v0.2.0                                go-module  
github.com/prometheus/common                      v0.37.0                               go-module  
github.com/prometheus/procfs                      v0.8.0                                go-module  
golang.org/x/crypto                               v0.5.0                                go-module  
golang.org/x/sync                                 v0.1.0                                go-module  
golang.org/x/sys                                  v0.4.1-0.20230105183443-b8be2fde2a9e  go-module  
google.golang.org/protobuf                        v1.28.1                               go-module  
libc-utils                                        0.7.2-r3                              apk        
libcrypto3                                        3.0.7-r2                              apk        
libssl3                                           3.0.7-r2                              apk        
musl                                              1.2.3-r4                              apk        
musl-utils                                        1.2.3-r4                              apk        
scanelf                                           1.3.5-r1                              apk        
ssl_client                                        1.35.0-r29                            apk        
zlib                                              1.2.13-r0                             apk  
</code></pre>
<p>These are all the components that syft found in the container image. We can see that it found 39 packages, including the OpenFaaS gateway itself.</p>
<p>Some of the packages are Go modules, others are packages that have been installed with <code>apk</code> (Alpine Linux's package manager).</p>
<h2>Checking for vulnerabilities</h2>
<p>Now that we have an SBOM, we can use grype to check for vulnerabilities.</p>
<pre><code class="hljs language-bash">grype ghcr.io/openfaas/gateway:0.26.2
 ✔ Vulnerability DB        [no update available]
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [39 packages]
 ✔ Scanned image           [2 vulnerabilities]
NAME                        INSTALLED  FIXED-IN  TYPE       VULNERABILITY   SEVERITY 
google.golang.org/protobuf  v1.28.1              go-module  CVE-2015-5237   High      
google.golang.org/protobuf  v1.28.1              go-module  CVE-2021-22570  Medium  
</code></pre>
<p>In this instance, we can see there are only two vulnerabilities, both of which are in the <code>google.golang.org/protobuf</code> Go module, and neither of them have been fixed yet.</p>
<ul>
<li>As the maintainer of OpenFaaS CE, I could try to eliminate the dependency from the original codebase, or wait for a workaround to be published by its vendor.</li>
<li>As a consumer of OpenFaaS CE my choices are similar, and it may be worth trying to look into the problem myself to see if the vulnerability is relevant to my use case.</li>
<li>Now, for OpenFaaS Pro, a commercial distribution of OpenFaaS, where source is not available, I'd need to contact the vendor OpenFaaS Ltd and see if they could help, or if they could provide a workaround. Perhaps there would even be a paid support relationship and SLA relating to fixing vulnerabilities of this kind?</li>
</ul>
<p>With this scenario, I wanted to show that different people care about the supply chain, and have different responsibilities for it.</p>
<h2>Generate an SBOM from within GitHub Actions</h2>
<p>The examples above were all run locally, but we can also generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.</p>
<p>Imagine you have the following Dockerfile:</p>
<pre><code>FROM alpine:3.17.0

RUN apk add --no-cache curl ca-certificates

CMD ["curl", "https://www.google.com"]
</code></pre>
<p>I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use <code>3.17.1</code> to fix a specific vulnerability.</p>
<p>Now a typical workflow for this Dockerfile would look like the below:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">build</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]
  <span class="hljs-attr">pull_request:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">checks:</span> <span class="hljs-string">write</span>
  <span class="hljs-attr">contents:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">publish:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">image</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/alexellis/gha-sbom:${{ github.sha }}
</span></code></pre>
<p>Upon each commit, an image is published to GitHub's Container Registry with the image name of: <code>ghcr.io/alexellis/gha-sbom:SHA</code>.</p>
<p>To generate an SBOM, we just need to update the <code>docker/build-push-action</code> to use the <code>sbom</code> flag:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Local</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">local_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">sbom:</span> <span class="hljs-literal">true</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
</code></pre>
<p>By checking the logs from the action, we can see that the image has been published with an SBOM:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment">#16 [linux/amd64] generating sbom using docker.io/docker/buildkit-syft-scanner:stable-1</span>
<span class="hljs-comment">#0 0.120 time="2023-01-25T15:35:19Z" level=info msg="starting syft scanner for buildkit v1.0.0"</span>
<span class="hljs-comment">#16 DONE 1.0s</span>
</code></pre>
<p>The SBOM can be viewed as before:</p>
<pre><code class="hljs language-bash">syft ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a

grype ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a
 ✔ Vulnerability DB        [updated]
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [21 packages]
 ✔ Scanned image           [2 vulnerabilities]
NAME        INSTALLED  FIXED-IN  TYPE  VULNERABILITY  SEVERITY 
libcrypto3  3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High      
libssl3     3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High  
</code></pre>
<p>The image: <code>alpine:3.17.0</code> contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed.</p>
<p>We can resolve the issue by changing the Dockerfile to use <code>alpine:3.17.1</code> instead, and re-running the build.</p>
<pre><code class="hljs language-bash">grype ghcr.io/alexellis/gha-sbom:63c6952d1ded1f53b1afa3f8addbba9efa37b52b
 ✔ Vulnerability DB        [no update available]
 ✔ Pulled image            
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [21 packages]
 ✔ Scanned image           [0 vulnerabilities]
No vulnerabilities found
</code></pre>
<h2>Wrapping up</h2>
<p>There is a lot written on the topic of supply chain security, so I wanted to give you a quick overview, and how to get started wth it.</p>
<p>We looked at Anchore's two open source tools: Syft and Grype, and how they can be used to generate an SBOM and scan for vulnerabilities.</p>
<p>We then produced an SBOM for a pre-existing Dockerfile and GitHub Action, introducing a vulnerability by using an older base image, and then fixing it by upgrading it. We did this by adding additional flags to the <code>docker/build-push-action</code>. We added the sbom flag, and set the provenance flag to false. Provenance is a separate but related topic, which is explained well in an article by Justin Chadwell of Docker (linked below).</p>
<p>I maintain an Open Source alternative to brew for developer-focused CLIs called <a href="https://arkade.dev/">arkade</a>. This already includes Google's crane project, and there's a <a href="https://github.com/alexellis/arkade/issues/839">Pull Request coming shortly to add Syft and Grype to the project</a>.</p>
<p>It can be a convenient way to install these tools on MacOS, Windows or Linux:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Available now</span>
arkade get crane syft

<span class="hljs-comment"># Coming shortly</span>
arkade get grype
</code></pre>
<p>In the beginning of the article we mentioned license compliance. SBOMs generated by syft do not seem to include license information, but in my experience, corporations which take this risk seriously tend to run their own scanning infrastructure <a href="https://www.synopsys.com/software-integrity/security-testing/software-composition-analysis.html">with commercial tools like Blackduck</a> or <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/21-08/prisma-cloud-compute-edition-admin/compliance/manage_compliance">Twistlock</a>.</p>
<p>Tools like Twistlock, and certain registries like <a href="https://jfrog.com/artifactory/">JFrog Artifactory</a> and the <a href="https://goharbor.io/">CNCF's Harbor</a>, can be configured to scan images. GitHub has a free, built-in service called Dependabot that won't just scan, but will also send Pull Requests to fix issues.</p>
<p>But with the SBOM approach, the responsibility is rebalanced, with the supplier taking on an active role in security. The consumer can then use the supplier's SBOMs, or run their own scanning infrastructure - or perhaps both.</p>
<p>See also:</p>
<ul>
<li><a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">Announcing Docker SBOM: A step towards more visibility into Docker images- Justin Cormack</a></li>
<li><a href="https://www.docker.com/blog/generate-sboms-with-buildkit/">Generating SBOMs for Your Image with BuildKit - Justin Chadwell</a></li>
<li><a href="https://github.com/anchore">Anchore on GitHub</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Is the GitHub Actions self-hosted runner safe for Open Source?]]></title>
            <link>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</link>
            <guid>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</guid>
            <pubDate>Fri, 20 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub warns against using self-hosted Actions runners for public repositories - but why? And are there alternatives?]]></description>
            <content:encoded><![CDATA[<p>First of all, why would someone working on an open source project need a self-hosted runner?</p>
<p>Having contributed to dozens of open source projects, and gotten to know many different maintainers, the primary reason tends to be out of necessity. They face an 18 minute build time upon every commit or Pull Request revision, and want to make the best of what little time they can give over to Open Source.</p>
<p>Having faster builds also lowers friction for contributors, and since many contributors are unpaid and rely on their own internal drive and enthusiasm, a fast build time can be the difference between them fixing a broken test or waiting another few days.</p>
<p>To sum up, there are probably just a few main reasons:</p>
<ol>
<li>Faster builds, higher concurrency, more disk space</li>
<li>Needing to build and test Arm binaries or containers on real hardware</li>
<li>Access to services on private networks</li>
</ol>
<p>The first point is probably one most people can relate to. Simply by provisioning an AMD bare-metal host, or a high spec VM with NVMe, you can probably shave minutes off a build.</p>
<p>For the second case, some projects like <a href="https://github.com/fluxcd/flagger">Flagger</a> from the CNCF felt their only option to support users deploying to AWS Graviton, was to seek sponsorship for a large Arm server and to install a self-hosted runner on it.</p>
<p>The third option is more nuanced, and specialist. This may or may not be something you can relate to, but it's worth mentioning. VPNs have very limited speed and there may be significant bandwidth costs to transfer data out of a region into GitHub's hosted runner environment. Self-hosted runners eliminate the cost and give full local link bandwidth, even as high as 10GbE. You just won't get anywhere near that with IPSec or Wireguard over the public Internet.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<h2>But are self-hosted runners safe?</h2>
<p>The GitHub team has a stark warning for those of us who are tempted to deploy a self-hosted runner and to connect it to a public repository.</p>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs. Some of the risks include:</p>
<ul>
<li>Malicious programs running on the machine.</li>
<li>Escaping the machine's runner sandbox.</li>
<li>Exposing access to the machine's network environment.</li>
<li>Persisting unwanted or dangerous data on the machine.</li>
</ul>
</blockquote>
<p>See also: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">Self-hosted runner security</a></p>
<p>Now you may be thinking "I won't approve pull requests from bad actors", but quite often the workflow goes this way: the contributor gets approval, then you don't need to approve subsequent pull requests after that.</p>
<p>An additional risk is if that user's account is compromised, then the attacker can submit a pull request with malicious code or malware. There is no way in GitHub to enforce Multi-Factor Authentication (MFA) for pull requests, even if you have it enabled on your Open Source Organisation.</p>
<p>Here are a few points to consider:</p>
<ul>
<li>Side effects build up with every build, making it less stable over time</li>
<li>You can't enforce MFA for pull requests - so malware can be installed directly on the host - whether intentionally or not</li>
<li>The GitHub docs warn that users can take over your account</li>
<li>Each runner requires maintenance and updates of the OS and all the required packages</li>
</ul>
<p>The chances are that if you're running the Flagger or Network Service Mesh project, you are shipping code that enterprise companies will deploy in production with sensitive customer data.</p>
<p>If you are not worried, try explaining the above to them, to see how they may see the risk differently.</p>
<h2>Doesn't Kubernetes fix all of this?</h2>
<p><a href="https://kubernetes.io/">Kubernetes</a> is a well known platform built for orchestrating containers. It's especially suited to running microservices, webpages and APIs, but has support for batch-style workloads like CI runners too.</p>
<p>You could make a container image and install the self-hosted runner binary within in, then deploy that as a Pod to a cluster. You could even scale it up with a few replicas.</p>
<p>If you are only building Java code, Python or Node.js, you may find this resolves many of the issues that we covered above, but it's hard to scale, and you still get side-effects as the environment is not immutable.</p>
<p>That's where the community project "actions-runtime-controller" or ARC comes in. It's a controller that launches a pool of Pods with the self-hosted runner.</p>
<blockquote>
<p>How much work does ARC need?</p>
<p>Some of the teams I have interviewed over the past 3 months told me that ARC took them a lot of time to set up and maintain, whilst others have told us it was a lot easier for them. It may depend on your use-case, and whether you're more of a personal user, or part of a team with 10-30 people committing code several times per day.
The first customer for actuated, which I'll mention later in the article was a team of ~ 20 people who were using ARC and had grew tired of the maintenance overhead and certain reliability issues.</p>
</blockquote>
<p>Unfortunately, by default ARC uses the same Pod many times as a persistent runner, so side effects still build up, malware can still be introduced and you have to maintain a Docker image with all the software needed for your builds.</p>
<p>You may be happy with those trade-offs, especially if you're only building private repositories.</p>
<p>But those trade-offs gets a lot worse if you use Docker or Kubernetes.</p>
<p>Out of the box, you simply cannot start a Docker container, build a container image or start a Kubernetes cluster.</p>
<p>And to do so, you'll need to resort to what can only be described as dangerous hacks:</p>
<ol>
<li>You expose the Docker socket from the host, and mount it into each Pod - any CI job can take over the host, game over.</li>
<li>You run in <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker in Docker (DIND)</a> mode. DIND requires a privileged Pod, which means that any CI job can take over the host, game over.</li>
</ol>
<p>There is some early work on running Docker In Docker in user-space mode, but this is slow, tricky to set up and complicated. By default, user-space mode uses a non-root account. So you can't install software packages or run commands like apt-get.</p>
<p>See also: <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Using Docker-in-Docker for your CI or testing environment? Think twice.</a></p>
<p>Have you heard of Kaniko?</p>
<p><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> is a tool for building container images from a Dockerfile, without the need for a Docker daemon. It's a great option, but it's not a replacement for running containers, it can only build them.</p>
<p>And when it builds them, in nearly every situation it will need root access in order to mount each layer to build up the image.</p>
<p>See also: <a href="https://suraj.io/post/root-in-container-root-on-host/">The easiest way to prove that root inside the container is also root on the host</a></p>
<p>And what about Kubernetes?</p>
<p>To run a KinD, Minikube or K3s cluster within your CI job, you're going to have to sort to one of the dangerous hacks we mentioned earlier which mean a bad actor could potentially take over the host.</p>
<p>Some of you may be running these Kubernetes Pods in your production cluster, whilst others have taken some due diligence and deployed a separate cluster just for these CI workloads. I think that's a slightly better option, but it's still not ideal and requires even more access control and maintenance.</p>
<p>Ultimately, there is a fine line between overconfidence and negligence. When building code on a public repository, we have to assume that the worst case scenario will happen one day. When using DinD or privileged containers, we're simply making that day come sooner.</p>
<p>Containers are great for running internal microservices and Kubernetes excels here, but there is a reason that AWS insists on hard multi-tenancy with Virtual Machines for their customers.</p>
<blockquote>
<p>See also: <a href="https://www.amazon.science/publications/firecracker-lightweight-virtualization-for-serverless-applications">Firecracker whitepaper</a></p>
</blockquote>
<h2>What's the alternative?</h2>
<p>When GitHub cautioned us against using self-hosted runners, on public repos, they also said:</p>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<p>So using GitHub's hosted runners are probably the most secure option for Open Source projects and for public repositories - if you are happy with the build speed, and don't need Arm runners.</p>
<p>But that's why I'm writing this post, sometimes we need faster builds, or access to specialist hardware like Arm servers.</p>
<p>The Kubernetes solution is fast, but it uses a Pod which runs many jobs, and in order to make it useful enough to run <code>docker run</code>, <code>docker build</code> or to start a Kubernetes cluster, we have to make our machines vulnerable.</p>
<p>With actuated, we set out to re-build the same user experience as GitHub's hosted runners, but without the downsides of self-hosted runners or using Kubernetes Pods for runners.</p>
<p>Actuated runs each build in a microVM on servers that you alone provision and control.</p>
<p>Its centralised control-plane schedules microVMs to each server using an immutable Operating System that is re-built with automation and kept up to date with the latest security patches.</p>
<p>Once the microVM has launched, it connects to GitHub, receives a job, runs to completion and is completely erased thereafter.</p>
<p>You get all of the upsides of self-hosted runners, with a user experience that is as close to GitHub's hosted runners as possible.</p>
<p>Pictured - an Arm Server with 270 GB of RAM and 80 cores - that's a lot of builds.</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62k4gXkAMHX-B?format=jpg&#x26;name=large" alt=""></a></p>
<p>You get to run the following, without worrying about security or side-effects:</p>
<ul>
<li>Docker (<code>docker run</code>) and <code>docker build</code></li>
<li>Kubernetes - Minikube, K3s, KinD</li>
<li><code>sudo</code> / root commands</li>
</ul>
<p>Need to test against a dozen different Kubernetes versions?</p>
<p>Not a problem:</p>
<p><img src="https://actuated.dev/images/k3sup.png" alt="Testing multiple Kubernetes versions"></p>
<p>What about running the same on Arm servers?</p>
<p>Just change <code>runs-on: actuated</code> to <code>runs-on: actuated-aarch64</code> and you're good to go. We test and maintain support for Docker and Kubernetes for both Intel and Arm CPU architectures.</p>
<p>Do you need insights for your Open Source Program Office (OSPO) or for the Technical Steering Committee (TSC)?</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62kSTXgAQLzUb?format=jpg&#x26;name=medium" alt=""></a></p>
<p>We know that no open source project has a single repository that represents all of its activity. Actuated provides insights across an organisation, including total build time and the time queued - which is a reflection of whether you could do with more or fewer build machines.</p>
<p>And we are only just getting started with compiling insights, there's a lot more to come.</p>
<h2>Get involved today</h2>
<p>We've already launched 10,000 VMs for customers jobs, and are now ready to open up the platform to the wider community. So if you'd like to try out what we're offering, we'd love to hear from you. As you offer feedback, you'll get hands on support from our engineering team and get to shape the product through collaboration.</p>
<p>So what does it cost? There is a subscription fee which includes - the control plane for your organisation, the agent software, maintenance of the OS images and our support via Slack. But all the plans are flat-rate, so it may even work out cheaper than paying GitHub for the bigger instances that they offer.</p>
<p>Professional Open Source developers like the ones you see at Red Hat, VMware, Google and IBM, that know how to work in community and understand cloud native are highly sought after and paid exceptionally well. So the open source project you work on has professional full-time engineers allocated to it by one or more companies, as is often the case, then using actuated could pay for itself in a short period of time.</p>
<p>If you represent an open source project that has no funding and is purely maintained by volunteers, what we have to offer may not be suited to your current position. And in that case, we'd recommend you stick with the slower GitHub Runners. Who knows? Perhaps one day GitHub may offer sponsored faster runners at no cost for certain projects?</p>
<p>And finally, what if your repositories are private? Well, we've made you aware of the trade-offs with a static self-hosted runner, or running builds within Kubernetes. It's up to you to decide what's best for your team, and your customers. Actuated works just as well with private repositories as it does with public ones.</p>
<p>See microVMs launching in ~ 1s during a matrix build for testing a Custom Resource Definition (CRD) on different Kubernetes versions:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want to know how actuated works? <a href="https://docs.actuated.dev/faq/">Read the FAQ for more technical details</a>.</p>
<ul>
<li><a href="https://docs.actuated.dev/register/">Find a server for your builds</a></li>
<li><a href="https://docs.actuated.dev/provision-server/">Register for actuated</a></li>
</ul>
<p>Follow us on Twitter - <a href="https://twitter.com/selfactuated">selfactuated</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to make GitHub Actions 22x faster with bare-metal Arm]]></title>
            <link>https://actuated.dev/blog/native-arm64-for-github-actions</link>
            <guid>https://actuated.dev/blog/native-arm64-for-github-actions</guid>
            <pubDate>Tue, 17 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub doesn't provide hosted Arm runners, so how can you use native Arm runners safely & securely?]]></description>
            <content:encoded><![CDATA[<p>GitHub Actions is a modern, fast and efficient way to build and test software, with free runners available. We use the free runners for various open source projects and are generally very pleased with them, after all, who can argue with good enough and free? But one of the main caveats is that GitHub's hosted runners don't yet support the Arm architecture.</p>
<p>So many people turn to software-based emulation using <a href="https://www.qemu.org/">QEMU</a>. QEMU is tricky to set up, and requires specific code and tricks if you want to use software in a standard way, without modifying it. But QEMU is great when it runs with hardware acceleration. Unfortunately, the hosted runners on GitHub do not have KVM available, so builds tend to be incredibly slow, and I mean so slow that it's going to distract you and your team from your work.</p>
<p>This was even more evident when <a href="https://twitter.com/fredbrancz">Frederic Branczyk</a> tweeted about his experience with QEMU on <a href="https://github.com/features/actions">GitHub Actions</a> for his open source observability project named <a href="https://github.com/parca-dev/parca">Parca</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does anyone have a <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> actions self-hosted runner manifest for me to throw at a <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> cluster? I&#39;m tired of waiting for emulated arm64 CI runs taking ages.</p>&mdash; Frederic 🧊 Branczyk @brancz@hachyderm.io (@fredbrancz) <a href="https://twitter.com/fredbrancz/status/1582779459379204096?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I checked out his build and expected "ages" to mean 3 minutes, in fact, it meant 33.5 minutes. I know because I forked his project and ran a test build.</p>
<p>After migrating it to actuated and one of our build agents, the time dropped to 1 minute and 26 seconds, a 22x improvement for zero effort.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>You can see the results here:</p>
<p><a href="https://twitter.com/alexellisuk/status/1583089248084729856/photo/1"><img src="https://pbs.twimg.com/media/FfhC5z1XkAAoYjn?format=jpg&#x26;name=large" alt="Results from the test, side-by-side"></a></p>
<p>As a general rule, the download speed is going to be roughly the same with a hosted runner, it may even be slightly faster due to the connection speed of Azure's network.</p>
<p>But the compilation times speak for themselves - in the Parca build, <code>go test</code> was being run with QEMU. Moving it to run on the ARM64 host directly, resulted in the marked increase in speed. In fact, the team had introduced lots of complicated code to try and set up a Docker container to use QEMU, all that could be stripped out, replacing it with a very standard looking test step:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">the</span> <span class="hljs-string">go</span> <span class="hljs-string">tests</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">go</span> <span class="hljs-string">test</span> <span class="hljs-string">./...</span>
</code></pre>
<h2>Can't I just install the self-hosted runner on an Arm VM?</h2>
<p>There are relatively cheap Arm VMs available from Oracle OCI, Google and Azure based upon the Ampere Altra CPU. AWS have their own Arm VMs available in the Graviton line.</p>
<p>So why shouldn't you just go ahead and install the runner and add them to your repos?</p>
<p>The moment you do that you run into three issues:</p>
<ul>
<li>You now have to maintain the software packages installed on that machine</li>
<li>If you use KinD or Docker, you're going to run into conflicts between builds</li>
<li>Out of the box scheduling is poor - by default it only runs one build at a time there</li>
</ul>
<p>Chasing your tail with package updates, faulty builds due to caching and conflicts is not fun, you may feel like you're saving money, but you are paying with your time and if you have a team, you're paying with their time too.</p>
<p>Most importantly, GitHub say that it cannot be used safely with a public repository. There's no security isolation, and state can be left over from one build to the next, including harmful code left intentionally by bad actors, or accidentally from malware.</p>
<h2>So how do we get to a safer, more efficient Arm runner?</h2>
<p>The answer is to get us as close as possible to a hosted runner, but with the benefits of a self-hosted runner.</p>
<p>That's where actuated comes in.</p>
<p>We run a SaaS that manages bare-metal for you, and talks to GitHub upon your behalf to schedule jobs efficiently.</p>
<ul>
<li>No need to maintain software, we do that for you with an automated OS image</li>
<li>We use microVMs to isolate builds from each other</li>
<li>Every build is immutable and uses a clean environment</li>
<li>We can schedule multiple builds at once without side-effects</li>
</ul>
<p>microVMs on Arm require a bare-metal server, and we have tested all the options available to us. Note that the Arm VMs discussed above do not currently support KVM or nested virtualisation.</p>
<ul>
<li>a1.metal on AWS - 16 cores / 32GB RAM - 300 USD / mo</li>
<li>c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM - 2.5 USD / hr</li>
<li><a href="https://www.hetzner.com/dedicated-rootserver/matrix-rx">RX-Line</a> from <a href="https://hetzner.com">Hetzner</a> with 128GB / 256GB RAM, NVMe &#x26; 80 cores for approx 200-250 EUR / mo.</li>
<li><a href="https://amzn.to/3WiSDE7">Mac Mini M1</a> - 8 cores / 16GB RAM - tested with Asahi Linux - one-time payment of ~ 1500 USD</li>
</ul>
<p>If you're already an AWS customer, the a1.metal is a good place to start. If you need expert support, networking and a high speed uplink, you can't beat Equinix Metal (we have access to hardware there and can help you get started) - you can even pay per minute and provision machines via API. The Mac Mini &#x3C;1 has a really fast NVMe and we're running one of these with Asahi Linux for our own Kernel builds for actuated. The RX Line from Hetzner has serious power and is really quite affordable, but just be aware that you're limited to a 1Gbps connection, a setup fee and monthly commitment, unless you pay significantly more.</p>
<p>I even tried Frederic's Parca job <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">on my 8GB Raspberry Pi with a USB NVMe</a>. Why even bother, do I hear you say? Well for a one-time payment of 80 USD, it was 26m30s quicker than a hosted runner with QEMU!</p>
<p><a href="https://alexellisuk.medium.com/upgrade-your-raspberry-pi-4-with-a-nvme-boot-drive-d9ab4e8aa3c2">Learn how to connect an NVMe over USB-C to your Raspberry Pi 4</a></p>
<h2>What does an Arm job look like?</h2>
<p>Since I first started trying to build code for Arm in 2015, I noticed a group of people who had a passion for this efficient CPU and platform. They would show up on GitHub issue trackers, ready to send patches, get access to hardware and test out new features on Arm chips. It was a tough time, and we should all be grateful for their efforts which go largely unrecognised.</p>
<blockquote>
<p>If you're looking to make your <a href="https://twitter.com/alexellisuk">software compatible with Arm</a>, feel free to reach out to me via Twitter.</p>
</blockquote>
<p>In 2020 when Apple released their M1 chip, Arm went mainstream, and projects that had been putting off Arm support like KinD and Minikube, finally had that extra push to get it done.</p>
<p>I've had several calls with teams who use Docker on their M1/M2 Macs exclusively, meaning they build only Arm binaries and use only Arm images from the Docker Hub. Some of them even ship to project using Arm images, but I think we're still a little behind the curve there.</p>
<p>That means Kubernetes - KinD/Minikube/K3s and Docker - including Buildkit, compose etc, all work out of the box.</p>
<p>I'm going to use the arkade CLI to download KinD and kubectl, however you can absolutely find the download links and do all this manually. I don't recommend it!</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">e2e-kind-test</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">start-kind:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">and</span> <span class="hljs-string">kubectl</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">kubectl:</span> <span class="hljs-string">latest</span>
          <span class="hljs-attr">kind:</span> <span class="hljs-string">latest</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">a</span> <span class="hljs-string">KinD</span> <span class="hljs-string">cluster</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          mkdir -p $HOME/.kube/
          kind create cluster --wait 300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Wait</span> <span class="hljs-string">until</span> <span class="hljs-string">CoreDNS</span> <span class="hljs-string">is</span> <span class="hljs-string">ready</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          kubectl rollout status deploy/coredns -n kube-system --timeout=300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">nodes</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">nodes</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">pods</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">pod</span> <span class="hljs-string">-A</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
</code></pre>
<p>That's our <code>x86_64</code> build, or Intel/AMD build that will run on a hosted runner, but will be kind of slow.</p>
<p>Let's convert it to run on an actuated ARM64 runner:</p>
<pre><code class="hljs language-diff">jobs:
  start-kind:
<span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: actuated-aarch64</span>
</code></pre>
<p>That's it, we've changed the runner type and we're ready to go.</p>
<p><img src="/images/2023-native-arm64-for-oss/in-progress-dashboard.png" alt="In progress build on the dashboard"></p>
<blockquote>
<p>An in progress build on the dashboard</p>
</blockquote>
<p>Behind the scenes, actuated, the SaaS schedules the build on a bare-metal ARM64 server, the boot up takes less than 1 second, and then the standard GitHub Actions Runner talks securely to GitHub to run the build. The build is isolated from other builds, and the runner is destroyed after the build is complete.</p>
<p><img src="/images/2023-native-arm64-for-oss/arm-kind.png" alt="Setting up an Arm KinD cluster took about 49s"></p>
<blockquote>
<p>Setting up an Arm KinD cluster took about 49s</p>
</blockquote>
<p>Setting up an Arm KinD cluster took about 49s, then it's over to you to test your Arm images, or binaries.</p>
<p>If I were setting up CI and needed to test software on both Arm and x86_64, then I'd probably create two separate builds, one for each architecture, with a <code>runs-on</code> label of <code>actuated</code> and <code>actuated-aarch64</code> respectively.</p>
<p>Do you need to test multiple versions of Kubernetes? Let's face it, it changes so often, that who doesn't need to do that. You can use the <code>matrix</code> feature to test multiple versions of Kubernetes on Arm and x86_64.</p>
<p>I show 5x clusters being launched in parallel in the video below:</p>
<p><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Demo - Actuated - secure, isolated CI for containers and Kubernetes</a></p>
<p>What about Docker?</p>
<p>Docker comes pre-installed in the actuated OS images, so you can simply use <code>docker build</code>, without any need to install extra tools like Buildx, or to have to worry about multi-arch Dockerfiles. Although these are always good to have, and are <a href="https://github.com/openfaas/golang-http-template/blob/master/template/golang-middleware/Dockerfile">available out of the box in OpenFaaS</a>, if you're curious what a multi-arch Dockerfile looks like.</p>
<h2>Wrapping up</h2>
<p>Building on bare-metal Arm hosts is more secure because side effects cannot be left over between builds, even if malware is installed by a bad actor. It's more efficient because you can run multiple builds at once, and you can use the latest software with our automated Operating System image. Enabling actuated on a build is as simple as changing the runner type.</p>
<p>And as you've seen from the example with the OSS Parca project, moving to a native Arm server can improve speed by 22x, shaving off a massive 34 minutes per build.</p>
<p>Who wouldn't want that?</p>
<p>Parca isn't a one-off, I was also told by <a href="https://twitter.com/cohix">Connor Hicks from Suborbital</a> that they have an Arm build that takes a good 45 minutes due to using QEMU.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<p>So if that sounds like where you are, reach out to us and we'll get you set up.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register to pilot actuated with us</a></li>
</ul>
<p>Additional links:</p>
<ul>
<li><a href="https://docs.actuated.dev/">Actuated docs</a></li>
<li><a href="https://docs.actuated.dev/faq">FAQ &#x26; comparison to other solutions</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blazing fast CI with MicroVMs]]></title>
            <link>https://actuated.dev/blog/blazing-fast-ci-with-microvms</link>
            <guid>https://actuated.dev/blog/blazing-fast-ci-with-microvms</guid>
            <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[I saw an opportunity to fix self-hosted runners for GitHub Actions. Actuated is now in pilot and aims to solve most if not all of the friction.]]></description>
            <content:encoded><![CDATA[<p>Around 6-8 months ago I started exploring MicroVMs out of curiosity. Around the same time, I saw an opportunity to <strong>fix</strong> self-hosted runners for GitHub Actions. <a href="https://docs.actuated.dev/">Actuated</a> is now in pilot and aims to solve <a href="https://twitter.com/alexellisuk/status/1573599285362532353?s=20&#x26;t=dFcd54c4KIynk6vIGTb7QA">most if not all of the friction</a>.</p>
<p>There's three parts to this post:</p>
<ol>
<li>A quick debrief on Firecracker and MicroVMs vs legacy solutions</li>
<li>Exploring friction with GitHub Actions from a hosted and self-hosted perspective</li>
<li>Blazing fast CI with Actuated, and additional materials for learning more about Firecracker</li>
</ol>
<blockquote>
<p>We're looking for customers who want to solve the problems explored in this post.
<a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot</a></p>
</blockquote>
<h2>1) A quick debrief on Firecracker 🔥</h2>
<blockquote>
<p>Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services.</p>
</blockquote>
<p>I learned about <a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> mostly by experimentation, building bigger and more useful prototypes. This helped me see what the experience was going to be like for users and the engineers working on a solution. I met others in the community and shared notes with them. Several people asked "Are microVMs the next thing that will replace containers?" I don't think they are, but they are an important tool where hard isolation is necessary.</p>
<p>Over time, one thing became obvious:</p>
<blockquote>
<p>MicroVMs fill a need that legacy VMs and containers can't.</p>
</blockquote>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker, you can replay my deep dive session with Richard Case, Principal Engineer (previously Weaveworks, now at SUSE).</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h2>2) So what's wrong with GitHub Actions?</h2>
<p>First let me say that I think GitHub Actions is a far better experience than Travis ever was, and we have moved all our CI for OpenFaaS, inlets and actuated to Actions for public and private repos. We've built up a good working knowledge in the community and the company.</p>
<p>I'll split this part into two halves.</p>
<h3>What's wrong with hosted runners?</h3>
<p><strong>Hosted runners are constrained</strong></p>
<p>Hosted runners are incredibly convenient, and for most of us, that's all we'll ever need, especially for public repositories with fast CI builds.</p>
<p>Friction starts when the 7GB of RAM and 2 cores allocated causes issues for us - like when we're launching a KinD cluster, or trying to run E2E tests and need more power. Running out of disk space is also a common problem when using Docker images.</p>
<p>GitHub recently launched new paid plans to get faster runners, however the costs add up, the more you use them.</p>
<p>What if you could pay a flat fee, or bring your own hardware?</p>
<p><strong>They cannot be used with public repos</strong></p>
<p>From GitHub.com:</p>
<blockquote>
<p>We recommend that you only use self-hosted runners with private repositories. This is because forks of your public repository can potentially run dangerous code on your self-hosted runner machine by creating a pull request that executes the code in a workflow.</p>
</blockquote>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs.</p>
</blockquote>
<p>Read more about the risks: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners">Self-hosted runner security</a></p>
<p>Despite a stern warning from GitHub, at least one notable CNCF project runs self-hosted ARM64 runners on public repositories.</p>
<p>On one hand, I don't blame that team, they have no other option if they want to do open source, it means a public repo, which means risking everything knowingly.</p>
<p>Is there another way we can help them?</p>
<p>I spoke to the GitHub Actions engineering team, who told me that using an ephemeral VM and an immutable OS image would solve the concerns.</p>
<p><strong>There's no access to ARM runners</strong></p>
<p>Building with QEMU is incredibly slow as Frederic Branczyk, Co-founder, Polar Signals found out when his Parca project was taking 33m5s to build.</p>
<p>I forked it and changed a line: <code>runs-on: actuated-aarch64</code> and reduced the total build time to 1m26s.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><strong>They limit maximum concurrency</strong></p>
<p>On the free plan, you can only launch 20 hosted runners at once, this increases as you pay GitHub more money.</p>
<p><strong>Builds on private repos are billed per minute</strong></p>
<p>I think this is a fair arrangement. GitHub donates Azure VMs to open source users or any public repo for that matter, and if you want to build closed-source software, you can do so by renting VMs per hour.</p>
<p>There's a free allowance for free users, then Pro users like myself get a few more build minutes included. However, These are on the standard, 2 Core 7GB RAM machines.</p>
<p>What if you didn't have to pay per minute of build time?</p>
<h3>What's wrong with self-hosted runners?</h3>
<p><strong>It's challenging to get all the packages right as per a hosted runner</strong></p>
<p>I spent several days running and re-running builds to get all the software required on a self-hosted runner for the private repos for <a href="https://www.openfaas.com/pricing/">OpenFaaS Pro</a>. Guess what?</p>
<p>I didn't want to touch that machine again afterwards, and even if I built up a list of apt packages, it'd be wrong in a few weeks. I then had a long period of tweaking the odd missing package and generating random container image names to prevent Docker and KinD from conflicting and causing side-effects.</p>
<p>What if we could get an image that had everything we needed and was always up to date, and we didn't have to maintain that?</p>
<p><strong>Self-hosted runners cause weird bugs due to caching</strong></p>
<p>If your job installs software like apt packages, the first run will be different from the second. The system is mutable, rather than immutable and the first problem I faced was things clashing like container names or KinD cluster names.</p>
<p><strong>You get limited to one job per machine at a time</strong></p>
<p>The default setup is for a self-hosted Actions Runner to only run one job at a time to avoid the issues I mentioned above.</p>
<p>What if you could schedule as many builds as made sense for the amount of RAM and core the host has?</p>
<p><strong>Docker isn't isolated at all</strong></p>
<p>If you install Docker, then the runner can take over that machine since Docker runs at root on the host. If you try user-namespaces, many things break in weird and frustrating aways like Kubernetes.</p>
<p>Container images and caches can cause conflicts between builds.</p>
<p><strong>Kubernetes isn't a safe alternative</strong></p>
<p>Adding a single large machine isn't a good option because of the dirty cache, weird stateful errors you can run into, and side-effects left over on the host.</p>
<p>So what do teams do?</p>
<p>They turn to a controller called <a href="https://github.com/actions/actions-runner-controller">Actions Runtime Controller (ARC)</a>.</p>
<p>ARC is non trivial to set up and requires you to create a GitHub App or PAT (please don't do that), then to provision, monitor, maintain and upgrade a bunch of infrastructure.</p>
<p>This controller starts a number of re-usable (not one-shot) Pods and has them register as a runner for your jobs. Unfortunately, they still need to use Docker or need to run Kubernetes which leads us to two awful options:</p>
<ol>
<li>Sharing a Docker Socket (easy to become root on the host)</li>
<li>Running Docker In Docker (requires a privileged container, root on the host)</li>
</ol>
<p>There is a third option which is to use a non-root container, but that means you can't use <code>sudo</code> in your builds. You've now crippled your CI.</p>
<p>What if you don't need to use Docker build/run, Kaniko or Kubernetes in CI at all? Well ARC may be a good solution for you, until the day you do need to ship a container image.</p>
<h2>3) Can we fix it? Yes we can.</h2>
<p><a href="https://docs.actuated.dev/">Actuated</a> ("cause (a machine or device) to operate.") is a semi-managed solution that we're building at OpenFaaS Ltd.</p>
<p><img src="https://docs.actuated.dev/images/conceptual-high-level.png" alt="A semi-managed solution, where you provide hosts and we do the rest."></p>
<blockquote>
<p>A semi-managed solution, where you provide hosts and we do the rest.</p>
</blockquote>
<p>You provide your own hosts to run jobs, we schedule to them and maintain a VM image with everything you need.</p>
<p>You install our GitHub App, then change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code> or <code>runs-on: actuated-aarch64</code> for ARM.</p>
<p>Then, provision one or more VMs with nested virtualisation enabled on GCP, DigitalOcean or Azure, or a bare-metal host, and <a href="https://docs.actuated.dev/add-agent/">install our agent</a>. That's it.</p>
<p>If you need ARM support for your project, the <a href="https://aws.amazon.com/ec2/instance-types/a1/">a1.metal from AWS</a> is ideal with 16 cores and 32GB RAM, or an <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a> machine like the c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM if you really need to push things. The 2020 M1 Mac Mini also works well with <a href="https://asahilinux.org/">Asahi Linux</a>, and can be maxed out at 16GB RAM / 8 Cores. <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">I even tried Frederic's Parca job on my Raspberry Pi</a> and it was 26m30s quicker than a hosted runner!</p>
<p>Whenever a build is triggered by a repo in your organisation, the control plane will schedule a microVM on one of your own servers, then GitHub takes over from there. When the GitHub runner exits, we forcibly delete the VM.</p>
<p>You get:</p>
<ul>
<li>A fresh, isolated VM for every build, no re-use at all</li>
<li>A fast boot time of ~ &#x3C;1-2s</li>
<li>An immutable image, which is updated regularly and built with automation</li>
<li>Docker preinstalled and running at boot-up</li>
<li>Efficient scheduling and packing of builds to your fleet of hosts</li>
</ul>
<p>It's capable of running Docker and Kubernetes (KinD, kubeadm, K3s) with full isolation. You'll find some <a href="https://docs.actuated.dev/">examples in the docs</a>, but anything that works on a hosted runner we expect to work with actuated also.</p>
<p>Here's what it looks like:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want the deeply technical information and comparisons? <a href="https://docs.actuated.dev/faq/">Check out the FAQ</a></p>
<p>You may also be interested in a debug experience that we're building for GitHub Actions. It can be used to launch a shell session over SSH with hosted and self-hosted runners: <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">Debug GitHub Actions with SSH and launch a cloud shell</a></p>
<h2>Wrapping up</h2>
<p>We're piloting actuated with customers today. If you're interested in faster, more isolated CI without compromising on security, we would like to hear from you.</p>
<p><strong>Register for the pilot</strong></p>
<p>We're looking for customers to participate in our pilot.</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot 📝</a></p>
<p>Actuated is live in pilot and we've already run thousands of VMs for our customers, but we're only just getting started here.</p>
<p><img src="https://blog.alexellis.io/content/images/2022/11/vm-launch.png" alt="VM launch events over the past several days"></p>
<blockquote>
<p>Pictured: VM launch events over the past several days</p>
</blockquote>
<p>Other links:</p>
<ul>
<li><a href="https://docs.actuated.dev/faq/">Read the FAQ</a></li>
<li><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Watch a short video demo</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>
<p><strong>What about GitLab?</strong></p>
<p>We're focusing on GitHub Actions users for the pilot, but have a prototype for GitLab. If you'd like to know more, reach out using the <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Apply for the pilot form</a>.</p>
<p><strong>Just want to play with Firecracker or learn more about microVMs vs legacy VMs and containers?</strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch A cracking time: Exploring Firecracker &#x26; MicroVMs</a></li>
<li><a href="https://github.com/alexellis/firecracker-init-lab">Try my firecracker lab on GitHub - alexellis/firecracker-init-lab</a></li>
</ul>
<h2>What are people saying about actuated?</h2>
<blockquote>
<p>"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff."</p>
<p>Addison van den Hoeven - DevOps Lead, Riskfuel</p>
</blockquote>
<blockquote>
<p>"Actuated looks super cool, interested to see where you take it!"</p>
<p>Guillermo Rauch, CEO Vercel</p>
</blockquote>
<blockquote>
<p>"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here."</p>
<p>Richard Case, Principal Engineer, SUSE</p>
</blockquote>
<blockquote>
<p>"Thank you. I think actuated is amazing."</p>
<p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p>
</blockquote>
<blockquote>
<p>"Nice work, security aspects alone with shared/stale envs on self-hosted runners."</p>
<p>Matt Johnson, Palo Alto Networks</p>
</blockquote>
<blockquote>
<p>"Is there a way to pay github for runners that suck less?"</p>
<p>Darren Shepherd, Acorn Labs</p>
</blockquote>
<blockquote>
<p>"Excited to try out actuated! We use custom actions runners and I think there's something here 🔥"</p>
<p>Nick Gerace, System Initiative</p>
</blockquote>
<blockquote>
<p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running Github Actions in isolated VMs in seconds (vs minutes)."</p>
<p>Rinat Abdullin, ML &#x26; Innovation at Trustbit</p>
</blockquote>
<blockquote>
<p>This is awesome!" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p>
<p>Frederic Branczyk, Co-founder, Polar Signals</p>
</blockquote>]]></content:encoded>
        </item>
    </channel>
</rss>