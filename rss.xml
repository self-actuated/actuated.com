<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Actuated - blog</title>
        <link>https://actuated.dev</link>
        <description>Keep your team productive &amp; focused with blazing fast CI</description>
        <lastBuildDate>Wed, 03 Apr 2024 15:22:55 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <image>
            <title>Actuated - blog</title>
            <url>https://actuated.dev/images/actuated.png</url>
            <link>https://actuated.dev</link>
        </image>
        <item>
            <title><![CDATA[Run AI models with ollama in CI with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/ollama-in-github-actions</link>
            <guid>https://actuated.dev/blog/ollama-in-github-actions</guid>
            <pubDate>Thu, 25 Apr 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[With the new GPU support for actuated, we've been able to run models like llama2 from ollama in CI on consumer and datacenter grade Nvidia cards.]]></description>
            <content:encoded><![CDATA[<p>That means you can run real end to end tests in CI with the same models you may use in dev and production. And if you use OpenAI or AWS SageMaker extensively, you could perhaps swap out what can be a very expensive API endpoint for your CI or testing environments to save money.</p>
<p>If you'd like to learn more about how and why you'd want access to GPUs in CI, read my past update: <a href="/blog/gpus-for-github-actions">Accelerate GitHub Actions with dedicated GPUs</a>.</p>
<p>We'll first cover what ollama is, why it's so popular, how to get it, what kinds of fun things you can do with it, then how to access it from actuated using a real GPU.</p>
<p><img src="/images/2024-04-ollama-in-ci/logos.png" alt="ollama can run in CI with isolated GPU acceleration using actuated"></p>
<blockquote>
<p>ollama can now run in CI with isolated GPU acceleration using actuated</p>
</blockquote>
<h2>What's ollama?</h2>
<p><a href="https://ollama.com/">ollama</a> is an open source project that aims to do for AI models, what Docker did for Linux containers. Whilst Docker created a user experience to share and run containers using container images in the Open Container Initiative (OCI) format, ollama bundles well-known AI models and makes it easy to run them without having to think about Python versions or Nvidia CUDA libraries.</p>
<p>The project packages and runs various models, but seems to take its name from Meta's popular <a href="https://llama.meta.com/">llama2 model</a>, which whilst <a href="https://llama.meta.com/faq">not released under an open source license</a>, allows for a generous amount of free usage for most types of users.</p>
<p>The ollama project can be run directly on a Linux, MacOS or Windows host, or within a container. There's a server component, and a CLI that acts as a client to pre-trained models. The main use-case today is that of inference - exercising the model with input data. A more recent feature means that you can create embeddings, if you pull a model that supports them.</p>
<p>On Linux, ollama can be installed using a utility script:</p>
<pre><code class="hljs language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<p>This provides the <code>ollama</code> CLI command.</p>
<h2>A quick tour of ollama</h2>
<p>After the initial installation, you can start a server:</p>
<pre><code class="hljs language-bash">ollama serve
</code></pre>
<p>By default, its REST API will listen on port <code>11434</code> on 127.0.0.1.</p>
<p>You can find the reference for ollama's REST API here: <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">API endpoints</a> - which includes things like: creating a chat completion, pulling a model, or generating embeddings.</p>
<p>You can then browse <a href="https://ollama.com/library">available models on the official website</a>, which resembles the Docker Hub. This set currently includes: gemma (built upon Google's DeepMind), mistral (an LLM), codellama (for generating Code), phi (from Microsoft research), vicuna (for chat, based upon llama2), llava (a vision encoder), and many more.</p>
<p>Most models will download with a default parameter size that's small enough to run on most CPUs or GPUs, but if you need to access it, there are larger models for higher accuracy.</p>
<p>For instance, the <a href="https://ollama.com/library/llama2">llama2</a> model by Meta will default to the 7b model which needs around 8GB of RAM.</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Pull the default model size:</span>

ollama pull llama2

<span class="hljs-comment"># Override the parameter size</span>

ollama pull llama2:13b
</code></pre>
<p>Once you have a model, you can then either "run" it, where you'll be able to ask it questions and interact with it like you would with ChatGPT, or you can send it API requests from your own applications using REST and HTTP.</p>
<p>For an interactive prompt, give no parameters:</p>
<pre><code class="hljs language-bash">ollama run llama2
</code></pre>
<p>To get an immediate response for use in i.e. scripts:</p>
<pre><code class="hljs language-bash">ollama run llama2 <span class="hljs-string">"What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?"</span>
</code></pre>
<p>And you can use the REST API via <code>curl</code>, or your own codebase:</p>
<pre><code class="hljs language-bash">curl -s http://localhost:11434/api/generate -d <span class="hljs-string">'{
    "model": "llama2",
    "stream": false,
    "prompt":"What are the risks of running privileged Docker containers for CI workloads?"
}'</span> | jq
</code></pre>
<p>We are just scratching the surface with what ollama can do, with a focus on testing and pulling pre-built models, but you can also create and share models using a <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfile</a>, which is another homage to the Docker experience by the ollama developers.</p>
<h3>Access ollama from Python code</h3>
<p>Here's how to access the API via Python, the <code>stream</code> parameter will emit JSON progressively when set to True, block until done if set to False. With Node.js, Python, Java, C#, etc the code will be very similar, but using your own preferred HTTP client. For Golang (Go) users, ollama founder <a href="https://twitter.com/jmorgan">Jeffrey Morgan</a> maintains a <a href="https://pkg.go.dev/github.com/jmorganca/ollama/api">higher-level Go SDK</a>.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> json

url = <span class="hljs-string">"http://localhost:11434/api/generate"</span>
payload = {
    <span class="hljs-string">"model"</span>: <span class="hljs-string">"llama2"</span>,
    <span class="hljs-string">"stream"</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">"prompt"</span>: <span class="hljs-string">"What are the risks of running privileged Docker containers for CI workloads?"</span>
}
headers = {
    <span class="hljs-string">"Content-Type"</span>: <span class="hljs-string">"application/json"</span>
}

response = requests.post(url, data=json.dumps(payload), headers=headers)

<span class="hljs-comment"># Parse the JSON response</span>
response_json = response.json()

<span class="hljs-comment"># Pretty print the JSON response</span>
<span class="hljs-built_in">print</span>(json.dumps(response_json, indent=<span class="hljs-number">4</span>))
</code></pre>
<p>When you're constructing a request by API, make sure you include any tags in the <code>model</code> name, if you've used one. I.e. <code>"model": "llama2:13b"</code>.</p>
<p>I hear from so many organisations who have gone to lengths to get SOC2 compliance, doing CVE scanning, or who are running Open Policy Agent or Kyverno to enforce strict Pod admission policies in Kubernetes, but then are happy to run their CI in Pods in privileged mode. So I asked the model why that may not be a smart idea. You can run the sample for yourself or <a href="https://gist.githubusercontent.com/alexellis/4c85e5927d251153c41379c4cac1a6c8/raw/257a02df0e01dff966c68509baf299bc32d3a11e/security.txt">see the response here</a>. We also go into detail in the <a href="https://docs.actuated.dev/faq/">actuated FAQ</a>, the security situation around self-hosted runners and containers is the main reason we built the solution.</p>
<h3>Putting it together for a GitHub Action</h3>
<p>The following GitHub Action will run on for customers who are enrolled for GPU support for actuated. If you'd like to gain access, contact us via the form on the <a href="https://actuated.dev/pricing">Pricing page</a>.</p>
<p>The <code>self-actuated/nvidia-run</code> installs either the consumer or datacenter driver for Nvidia, depending on what you have in your system. This only takes about 30 seconds and could be cached if you like. The ollama models could also be <a href="https://docs.actuated.dev/tasks/local-github-cache/">cached using a local S3 bucket</a>.</p>
<p>Then, we simply run the equivalent bash commands from the previous section to:</p>
<ul>
<li>Install ollama</li>
<li>Start serving the REST API</li>
<li>Pull the llama2 model from Meta</li>
<li>Run an inference via CLI</li>
<li>Run an inference via REST API using curl</li>
</ul>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">ollama-e2e</span>

<span class="hljs-attr">on:</span>
    <span class="hljs-attr">workflow_dispatch:</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">ollama-e2e:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">ollama-e2e</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/nvidia-run@master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">Ollama</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            curl -fsSL https://ollama.com/install.sh | sudo -E sh
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Start</span> <span class="hljs-string">serving</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              # Run the background, there is no way to daemonise at the moment
              ollama serve &#x26;
</span>
              <span class="hljs-comment"># A short pause is required before the HTTP port is opened</span>
              <span class="hljs-string">sleep</span> <span class="hljs-number">5</span>

              <span class="hljs-comment"># This endpoint blocks until ready</span>
              <span class="hljs-string">time</span> <span class="hljs-string">curl</span> <span class="hljs-string">-i</span> <span class="hljs-string">http://localhost:11434</span>

        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Pull</span> <span class="hljs-string">llama2</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              ollama pull llama2
</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Invoke</span> <span class="hljs-string">via</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              ollama run llama2 "What are the pros of MicroVMs for continous integrations, especially if Docker is the alternative?"
</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Invoke</span> <span class="hljs-string">via</span> <span class="hljs-string">API</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            curl -s http://localhost:11434/api/generate -d '{
              "model": "llama2",
              "stream": false,
              "prompt":"What are the risks of running privileged Docker containers for CI workloads?"
            }' | jq
</span></code></pre>
<p>There is no built-in way to daemonise the ollama server, so for now we run it in the background using bash. The readiness endpoint can then be accessed which blocks until the server has completed its initialisation.</p>
<h3>Interactive access with SSH</h3>
<p>By modifying your CI job, you can drop into a remote SSH session and run interactive commands at any point in the workflow.</p>
<p>That's how I came up with the commands for the Nvidia driver installation, and for the various ollama commands I shared.</p>
<p>Find out more about SSH for GitHub Actions <a href="https://docs.actuated.dev/tasks/debug-ssh/">in the actuated docs</a>.</p>
<p><img src="/images/2024-04-ollama-in-ci/ssh.png" alt="Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM"></p>
<blockquote>
<p>Pulling one of the larger llama2 models interactively in an SSH session, directly to the runner VM</p>
</blockquote>
<h2>Wrapping up</h2>
<p>Within a very short period of time ollama helped us pull a popular AI model that can be used for chat and completions. We were then able to take what we learned and run it on a GPU at an accelerated speed and accuracy by using actuated's <a href="/blog/gpus-for-github-actions">new GPU support for GitHub Actions and GitLab CI</a>. Most hosted CI systems provide a relatively small amount of disk space for jobs, with actuated you can customise this and that may be important if you're going to be downloading large AI models. You can also easily customise the amount of RAM and vCPU using the <code>runs-on</code> label to any combination you need.</p>
<p>ollama isn't the only way to find, download and run AI models, just like Docker wasn't the only way to download and install Nginx or Postgresql, but it provides a useful and convenient interface for those of us who are still learning about AI, and are not as concerned with the internal workings of the models.</p>
<p>Over on the OpenFaaS blog, in the tutorial <a href="https://www.openfaas.com/blog/openai-streaming-responses/">Stream OpenAI responses from functions using Server Sent Events</a>, we covered how to stream a response from a model to a function, and then back to a user. There, we used the <a href="https://github.com/c0sogi/llama-api">llama-api</a> open source project, which is a single-purpose HTTP API for simulating llama2.</p>
<p>One of the benefits of ollama is <a href="https://github.com/ollama/ollama/tree/main/examples">the detailed range of examples</a> in the docs, and the ability to run other models that may include computer vision such as with the <a href="https://llava-vl.github.io/">LLaVA: Large Language and Vision Assistant model</a> or generating code with <a href="https://codellama.dev/about">Code Llama</a>.</p>
<p>Right now, many of us are running and tuning models in development, some of us are using OpenAI's API or self-hosted models in production, but there's very little talk about doing thorough end to end testing or exercising models in CI. That's where actuated can help.</p>
<p>Feel free to <a href="/pricing/">reach out for early access, or to see if we can help your team with your CI needs</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Accelerate GitHub Actions with dedicated GPUs]]></title>
            <link>https://actuated.dev/blog/gpus-for-github-actions</link>
            <guid>https://actuated.dev/blog/gpus-for-github-actions</guid>
            <pubDate>Tue, 12 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[You can now accelerate GitHub Actions with dedicated GPUs for machine learning and AI use-cases.]]></description>
            <content:encoded><![CDATA[<p>With the surge of interest in AI and machine learning models, it's not hard to think of reasons why people want GPUs in their workstations and production environments. They make building &#x26; training, fine-tuning and serving (inference) from a machine learning model just that much quicker than running with a CPU alone.</p>
<p>So if you build and test code for CPUs in CI pipelines like GitHub Actions, why wouldn't you do the same with code built for GPUs? Why exercise only a portion of your codebase?</p>
<h2>GPUs for GitHub Actions</h2>
<p>One of our earliest customers moved all their GitHub Actions to actuated for a team of around 30 people, but since Firecracker has no support for GPUs, they had to keep a few self-hosted runners around for testing their models. Their second hand Dell servers were racked in their own datacentre, with 8x 3090 GPUs in each machine.</p>
<p>Their request for GPU support in actuated predated the hype around OpenAI, and was the catalyst for us doing this work.</p>
<p>They told us how many issues they had keeping drivers in sync when trying to use self-hosted runners, and the security issues they ran into with mounting Docker sockets or running privileged containers in Kubernetes.</p>
<p>With a microVM and actuated, you'll be able to test out different versions of drivers as you see fit, and know there will never be side-effects between builds. You can read more in <a href="https://docs.actuated.dev/faq/">our FAQ</a> on how actuated differs from other solutions which rely on the poor isolation afforded by containers. Actuated is the closest you can get to a hosted runner, whilst having full access to your own hardware.</p>
<p>I'll tell you a bit more about it, how to build your own workstation with commodity hardware, or where to rent a powerful bare-metal host with a capable GPU for less than 200 USD / mo that you can use with actuated.</p>
<h3>Available in early access</h3>
<p>So today, we're announcing early access to actuated for GPUs. Whether your machine has one GPU, two, or ten, you can allocate them directly to a microVM for a CI job, giving strong isolation, and the same ephemeral environment that you're used to with GitHub's hosted runners.</p>
<p><img src="/images/2024-03-gpus/3060.jpg" alt="Our test rig with 2x 3060s"></p>
<blockquote>
<p>Our test rig has 2x Nvidia 3060 GPUs and is available for customer demos and early testing.</p>
</blockquote>
<p>We've compiled a list of vendors that provide access to fast, bare-metal compute, but at the moment, there are only a few options for bare-metal with GPUs.</p>
<ul>
<li>Self-build a workstation, you'll recover the total cost in &#x3C; 1 month vs hosted</li>
<li>Use a European bare-metal host like Hetzner or OVH Cloud</li>
</ul>
<p>We have a full bill of materials available for anyone who wants to build a workstation with 2x Nvidia 3060 graphics cards, giving 24GB of usage RAM at a relatively low maximum power consumption of 170W. It's ideal for CI and end to end testing.</p>
<p>If you'd like to go even more premium, the Nvidia RTX 4000 card comes with 20GB of RAM, so two of those would give you 40GB of RAM available for Large Language Models (LLMs).</p>
<p>For Hetzner, you can get started with an i5 bare-metal host with 14 cores, 64GB RAM and a dedicated Nvidia RTX 4000 for around 184 EUR / mo (less than 200 USD / mo). If that sounds like ridiculously good value, it's because it is.</p>
<h2>What does the build look like?</h2>
<p>Once you've installed the actuated agent, it's the same process as a regular bare-metal host.</p>
<p>It'll show up on your actuated dashboard, and you can start sending jobs to it immediately.</p>
<p><img src="/images/2024-03-gpus/runner.png" alt="The server with 2x GPUs showing up in the dashboard"></p>
<blockquote>
<p>The server with 2x GPUs showing up in the dashboard</p>
</blockquote>
<p>Here's how we install the Nvidia driver for a consumer-grade card. The process is very similar for the datacenter range of GPUs found in enterprise servers.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">nvidia-smi</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">nvidia-smi:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">nvidia-smi</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Download</span> <span class="hljs-string">Nvidia</span> <span class="hljs-string">install</span> <span class="hljs-string">package</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
           curl -s -S -L -O https://us.download.nvidia.com/XFree86/Linux-x86_64/525.60.11/NVIDIA-Linux-x86_64-525.60.11.run \
              &#x26;&#x26; chmod +x ./NVIDIA-Linux-x86_64-525.60.11.run
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">Nvidia</span> <span class="hljs-string">driver</span> <span class="hljs-string">and</span> <span class="hljs-string">Kernel</span> <span class="hljs-string">module</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
              sudo ./NVIDIA-Linux-x86_64-525.60.11.run \
                  --accept-license \
                  --ui=none \
                  --no-questions \
                  --no-x-check \
                  --no-check-for-alternate-installs \
                  --no-nouveau-check
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">nvidia-smi</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            nvidia-smi
</span></code></pre>
<p>This is a very similar approach to installing a driver on your own machine, just without any interactive prompts. It took around 38s which is not very long considering how much time AI and ML operations can run for when doing end to end testing. The process installs some binaries like <code>nvidia-smi</code> and compiles a Kernel module to load the graphics driver, these could easily be cached with GitHub Action's built-in caching mechanism.</p>
<p>For convenience, we created a composite action that reduces the duplication if you have lots of workflows with the Nvidia driver installed.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">gpu-job</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">gpu-job:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">gpu-job</span>
        <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-8cpu-16gb</span>, <span class="hljs-string">gpu</span>]
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/nvidia-run@master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">nvidia-smi</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            nvidia-smi
</span></code></pre>
<p>Of course, if you have an AMD graphics card, or even an ML accelerator like a PCIe Google Corale, that can also be passed through into a VM in a dedicated way.</p>
<p>The mechanism being used is called VFIO, and allows a VM to take full, dedicated, isolated control over a PCI device.</p>
<h2>A quick example to get started</h2>
<p>To show the difference between using a GPU and CPU, I ran <a href="https://github.com/openai/whisper">OpenAI's Whisper project</a>, which transcribes audio or video to a text file.</p>
<p>With the <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">following demo video of Actuated's SSH gateway</a>, running with the tiny model.</p>
<ul>
<li>A AMD Ryzen 9 5950X 16-Core Processor took 39 seconds</li>
<li>The same host with a 3060 GPU mounted via actuated took 16 seconds</li>
</ul>
<p>That's over 2x quicker, for a 5:34 minute video. If you process a lot of clips, or much longer clips then the difference may be even more marked.</p>
<p>The tiny model is really designed for demos, and in production you'd use the medium or large model which is much more resource intensive.</p>
<p>Here's a screenshot showing what this looks like with the medium model, which is much larger and more accurate:</p>
<p><a href="/images/2024-03-gpus/gpu-medium.png">Medium model running on a GPU via actuated</a></p>
<blockquote>
<p>Medium model running on a GPU via actuated</p>
</blockquote>
<p>With a CPU, even with 16 vCPU, all of them get pinned at 100%, and then it takes a significantly longer time to process.</p>
<p><a href="https://twitter.com/alexellisuk/status/1767252171978871195/"><img src="/images/2024-03-gpus/cpu-medium.png" alt="You can run the medium model on CPU, but would you want to?"></a></p>
<blockquote>
<p>You can run the medium model on CPU, but would you want to?</p>
</blockquote>
<p>With the medium model:</p>
<ul>
<li>The CPU took 8 minutes 54 seconds, pinning 16 cores at 100%</li>
<li>The GPU took only 55s with very little CPU consumption</li>
</ul>
<p>The GPU increased the speed by 9x, imagine how much quicker it'd be if you used an Nvidia 3090, 4090, or even an RTX 4000.</p>
<p>If you want to just explore the system, and run commands interactively, you can use <a href="https://docs.actuated.dev/tasks/debug-ssh/">actuated's SSH feature</a> to get a shell. Once you know the commands you want to run, you can copy them into your workflow YAML file for GitHub Actions.</p>
<p>We took the SSH debug session for a test-drive. We installed <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">the NVIDIA Container Toolkit</a>, then ran the ollama tool to test out some Large Language Models (LLMs).</p>
<p><a href="https://ollama.com/">Ollama</a> is an open source tool for downloading and testing prepackaged models like Mistral or Llama2.</p>
<p><a href="https://twitter.com/alexellisuk/status/1767899463471796278/"><img src="https://pbs.twimg.com/media/GIjXEXZWkAAiuKc?format=png&#x26;name=medium" alt="Our experiment with ollama within a GitHub Actions runner"></a></p>
<blockquote>
<p>Our experiment with ollama within a GitHub Actions runner</p>
</blockquote>
<h2>The technical details</h2>
<p>Since launch, actuated powered by Firecracker has securely isolated over 220k CI jobs for GitHub Actions users. Whilst it's a complex project to integrate, it has been very reliable in production.</p>
<p>Now in order to bring GPUs to actuated, we needed to add support for a second Virtual Machine Manager (VMM), and we picked cloud-hypervisor.</p>
<p>cloud-hypervisor was originally a fork from Firecracker and shares a significant amount of code. One place it diverged was adding support for PCI devices, such as GPUs. Through VFIO, cloud-hypervisor allows for a GPU to be passed through to a VM in a dedicated way, so it can be used in isolation.</p>
<p>Here's the first demo that I ran when we had everything working, showing the output from <code>nvidia-smi</code>:</p>
<p><a href="https://twitter.com/alexellisuk/status/1765706313068130695/photo/1"><img src="https://pbs.twimg.com/media/GIEMJZUWMAAS0A8?format=jpg&#x26;name=large" alt="The first run of nvidia-smi"></a></p>
<blockquote>
<p>The first run of nvidia-smi</p>
</blockquote>
<h2>Reach out for more</h2>
<p>In a relatively short period of time, we were able to update our codebase to support both Firecracker and cloud-hypervisor, and to enable consumer-grade GPUs to be passed through to VMs in isolation.</p>
<p>You can rent a really powerful and capable machine from Hetzner for under 200 USD / mo, or build your own workstation with dual graphics cards like our demo rig, for less than 2000 USD and then you own that and can use it as much as you want, plugged in under your desk or left in a cabinet in your office.</p>
<p><strong>A quick recap on use-cases</strong></p>
<p>Let's say you want to run end to end tests for an application that uses a GPU? Perhaps it runs on Kubernetes? You can do that.</p>
<p>Do you want to fine-tune, train, or run a batch of inferences on a model? You can do that. GitHub Actions has a 6 hour timeout, which is plenty for many tasks.</p>
<p>Would it make sense to run Stable Diffusion in the background, with different versions, different inputs, across a matrix? GitHub Actions makes that easy, and actuated can manage the GPU allocations for you.</p>
<p>Do you run inference from OpenFaaS functions? We have a tutorial on <a href="https://www.openfaas.com/blog/transcribe-audio-with-openai-whisper/">OpenAI Whisper within a function with GPU acceleration here</a> and a separate one on how to <a href="https://www.openfaas.com/blog/openai-streaming-responses/">serve Server Sent Events (SSE) from OpenAI or self-hosted models</a>, which is popular for chat-style interfaces to AI models.</p>
<p>If you're interested in GPU support for GitHub Actions, then reach out to talk to us with <a href="https://actuated.dev/pricing">this form</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The state of Arm CI for the CNCF]]></title>
            <link>https://actuated.dev/blog/cncf-arm-march-update</link>
            <guid>https://actuated.dev/blog/cncf-arm-march-update</guid>
            <pubDate>Mon, 04 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[After running over 70k build minutes for top-tier CNCF projects, we give an update on the sponsored Arm CI program.]]></description>
            <content:encoded><![CDATA[<p>It's now been 4 months since we <a href="https://actuated.dev/blog/arm-ci-cncf-ampere">kicked off the sponsored program with the Cloud Native Computing Foundation (CNCF) and Ampere to manage CI for the foundation's open source projects</a>. But even before that, <a href="https://calyptia.com/blog/scaling-builds-with-actuated">Calyptia, the maintainer of Fluent approached us to run Arm CI for the open source fluent repos</a>, so we've been running CI for CNCF projects since June 2023.</p>
<p>Over that time, we've got to work directly with some really bright, friendly, and helpful maintainers, who wanted to have a safe, fast and secure way to create release artifacts, test PRs, and to run end to end tests. Their alternative until this point was either to go against GitHub's own advice, and to <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">run an unsafe, self-hosted runner on an open source repo</a>, or <a href="https://actuated.dev/blog/how-to-run-multi-arch-builds-natively">to use QEMU</a> that in the case of Fluent meant their 5 minute build took over 6 hours before failing.</p>
<p>You can find out more about why we put this program together in the original announcement: <a href="https://actuated.dev/blog/arm-ci-cncf-ampere">Announcing managed Arm CI for CNCF projects</a></p>
<h2>Measuring the impact</h2>
<p>When we started out, Chris Aniszczyk, the CNCF's CTO wanted to create a small pilot to see if there'd be enough demand for our service. The CNCF partnered with Ampere to co-fund the program, Ampere sell a number of Arm based CPUs - which they brand as "Cloud Native" because they're so dense in cores and highly power efficient. Equinix Metal provide the credits and the hosting via the <a href="https://www.cncf.io/blog/2021/10/13/announcing-the-cloud-native-credits-program/">Cloud Native Credits program</a>.</p>
<p>In a few weeks, not only did we fill up all available slots, but we personally hand-held and onboarded each of the project maintainers one by one, over Zoom, via GitHub, and Slack.</p>
<p>Why would maintainers of top-tier projects need our help? Our team and community has extensive experience porting code to Arm, and building for multiple CPUs. We were able to advise on best practices for splitting up builds, how to right-size VMs, were there to turn on esoteric Kernel modules and configurations, and to generally give them a running start.</p>
<p>Today, our records show that the CNCF projects enrolled have run a total of 70.3k minutes. That's almost the equivalent of a computer running tasks 24/7 for a total of 2 months solid, without a break.</p>
<p>Here's a list of the organisations we've onboarded so far, ordered by the total amount of build minutes. We added the date of their first actuated build to help add some context. As I mentioned in the introduction, fluent have been a paying customer since June 2023.</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Organisation</th>
<th>Date of first actuated build</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>etcd-io (etcd, boltdb)</td>
<td>2023-10-24</td>
</tr>
<tr>
<td>2</td>
<td>fluent</td>
<td>2023-06-07</td>
</tr>
<tr>
<td>3</td>
<td>falcosecurity</td>
<td>2023-12-06</td>
</tr>
<tr>
<td>4</td>
<td>containerd</td>
<td>2023-12-02</td>
</tr>
<tr>
<td>5</td>
<td>cilium (tetragon, cilium, ebpf-go)</td>
<td>2023-10-31</td>
</tr>
<tr>
<td>6</td>
<td>cri-o</td>
<td>2023-11-27</td>
</tr>
<tr>
<td>7</td>
<td>open-telemetry</td>
<td>2024-02-14</td>
</tr>
<tr>
<td>8</td>
<td>opencontainers (runc)</td>
<td>2023-12-15</td>
</tr>
<tr>
<td>9</td>
<td>argoproj</td>
<td>2024-01-30</td>
</tr>
</tbody>
</table>
<p><em>Ranked by build minutes consumed</em></p>
<p>Some organisations have been actuated on multiple projects like <a href="https://github.com/etcd-io/etcd">etcd-io</a>, with <a href="https://github.com/etcd-io/bbolt">boltdb</a> adding to their minutes, and <a href="https://github.com/cilium">cilium</a> where <a href="https://isovalent.com/blog/post/can-i-use-tetragon-without-cilium-yes/">tetragon</a> and <a href="https://github.com/cilium/ebpf">ebpf-go</a> are also now running Arm builds.</p>
<p>It's tempting to look at build minutes as the only metric, however, now that <a href="https://github.com/containerd/containerd">containerd</a>, <a href="https://github.com/opencontainers/runc">runc</a>, cilium, etcd, and various other core projects are built by actuated, the security of the supply chain has become far more certain.</p>
<p><strong>From 10,000ft</strong></p>
<p>Here's what we aimed for and have managed to achieve in a very short period of time:</p>
<ul>
<li>Maintainers have no infrastructure to maintain - in the case of etcd, 5 people are now off the hook for being on call to manually maintain servers</li>
<li>OSS projects can run their CI jobs securely on ephemeral runners, with no risk of side-effects</li>
<li>Equinix Metal that donates credits has reduced the total amount of servers allocated and greatly increased utilization</li>
</ul>
<p><strong>Making efficient use of shared resources</strong></p>
<p>After fluent, etcd was the second project to migrate off self-managed runners. They had the impression that one of their jobs needed 32 vCPU and 32GB of RAM, and when we monitored the shared server pool, we noticed barely any load on the servers. That led me to build a quick Linux profiling tool called vmmeter. When they ran the profiler, it turned out the job used a maximum of 1.3 vCPU and 3GB of RAM, that's not just a rounding error - that's a night and day difference.</p>
<p>You can learn how to try out vmmeter to right-size your jobs on actuated, or on GitHub's hosted runners.</p>
<p><a href="https://actuated.dev/blog/right-sizing-vms-github-actions">Right sizing VMs for GitHub Actions</a></p>
<p>The projects have had a fairly stable, steady-state of CI jobs throughout the day and night as contributors from around the globe send PRs and end to end tests run.</p>
<p>But with etcd-io in particular we started to notice on Monday or Tuesday that there was a surge of up to 200 jobs all at once. When we asked them about this, they told us Dependabot was the cause. It would send a number of PRs to bump dependencies and that would in turn trigger dozens of jobs.</p>
<p><img src="/images/2024-03-cncf-update/etcd-dependabot.png" alt="Thundering herd problem from dependabot"></p>
<blockquote>
<p>Thundering herd problem from dependabot</p>
</blockquote>
<p>It would clear itself down in time, but we spent a little time to automate adding in 1-2 extra servers for this period of the week, and we managed to get the queue cleared several times quicker. When the machines are no longer needed, they drain themselves and get deleted. This is important for efficient use of the CNCF's credits and Equinix Metal's fleet of Ampere Altra Q80 Arm servers.</p>
<p><strong>Giving insights to maintainers</strong></p>
<p>I got to meet up with <a href="https://twitter.com/estesp">Phil Estes</a> from the containerd project at FOSDEM. We are old friends and used to be Docker Captains together.</p>
<p>We looked at the daily usage stats, looked at the total amount of contributors that month and how many builds they'd had.</p>
<p><a href="https://twitter.com/alexellisuk/status/1753769476871405804/photo/3"><img src="https://pbs.twimg.com/media/GFakCn-WsAAMlR9?format=jpg&#x26;name=medium" alt="Phil checking out the actuated dashboard"></a></p>
<p>Then we opened up the organisation insights page and found that containerd had accounted for 14% of the total build minutes having only been onboarded in Dec 2023.</p>
<p><a href="https://twitter.com/alexellisuk/status/1753769476871405804/photo/2"><img src="https://pbs.twimg.com/media/GFakCn3XwAACA3b?format=jpg&#x26;name=medium" alt="Detailed usage"></a></p>
<p>We saw that there was a huge peak in jobs last month compared to this month, so he went off to the containerd Slack to ask about what had happened.</p>
<p><strong>Catching build time increases early</strong></p>
<p>Phil also showed me that he used to have a jimmy-rigged dashboard of his own to track build time increases, and at FOSDEM, my team did a mini hackathon to release our own way to show people their job time increases.</p>
<p>We call it "Job Outliers" and it can be used to track increases going back as far as 120 days from today.</p>
<p><img src="/images/2024-03-cncf-update/outliers-top.png" alt="Job Outliers for actuated&#x27;s VM builds"></p>
<p>Clicking "inspect" on any of the workflows will open up a separate plot link with deep links to the longest job seen on each day of that period of time.</p>
<p>So what changed for our own actuated VM builds in that week, to add 5+ minutes of build time?</p>
<p><img src="/images/2024-03-cncf-update/detail-plot.png" alt="Maximum build times per day across the period"></p>
<p>We started building eBPF into the Kernel image, and the impact was 2x 2.5 minutes of build time.</p>
<p>This feature was originally requested by Toolpath, a commercial user of actuated with very intensive Julia builds, and they have been using it to keep their build times in check. We're pleased to be able to offer every enhancement to the CNCF project maintainers too.</p>
<h2>Wrapping up</h2>
<p><strong>What are the project maintainers saying?</strong></p>
<p><a href="https://github.com/atoulme">Antoine Toulme</a>, maintainer of OpenTelemetry collectors:</p>
<blockquote>
<p>The OpenTelemetry project has been looking for ways to test arm64 to support it as a top tier distribution. Actuated offers a path for us to test on new operating systems, especially arm64, without having to spend any time setting up or maintaining runners. We were lucky to be the recipient of a loan from Ampere that gave us access to a dedicated ARM server, and it took us months to navigate setting up dedicated runners and has significant maintenance overhead. With Actuated, we just set a tag in our actions and everything else is taken care of.</p>
</blockquote>
<p><a href="https://github.com/LucaGuerra">Luca Guerra</a>, maintainer of Falco:</p>
<blockquote>
<p>Falco users need to deploy to ARM64 as a platform, and we as maintainers, need to make sure that this architecture is treated as a first class citizen. Falco is a complex piece of software that employs kernel instrumentation and so it is not trivial to properly test. Thanks to Actuated, we were able to quickly add ARM64 to our GitHub Actions CI/CD pipeline making it much easier to maintain, freeing up engineering time from infrastructure work.</p>
</blockquote>
<p><a href="https://github.com/saschagrunert">Sascha Grunert</a>, maintainer of Cri-o:</p>
<blockquote>
<p>The CRI-O project was able to seamlessly integrate Arm based CI with the support of Actuated. We basically had to convert our existing tests to a GitHub Actions matrix utilizing their powerful Arm runners. Integration and unit testing on Arm is another big step for CRI-O to provide a generally broader platform support. We also had to improve the test suite itself for better compatibility with other architectures than x86_64/arm64. This makes contributing to CRI-O on those platforms even simpler. I personally don’t see any better option than Actuated right now, because managing our own hardware is something we’d like to avoid to mainly focus on open source software development. The simplicity of the integration using Actuated helped us a lot, and our future goal is to extend the CRI-O test scenarios for that.</p>
</blockquote>
<h3>Summing up the program so far</h3>
<p>Through the sponsored program, actuated has now run over 70k build minutes for around 10 CNCF projects, and we've heard from a growing number of projects who would like access.</p>
<p>We've secured the supply chain by removing unsafe runners that GitHub says should definitely not be used for open source repositories, and we've lessened the burden of server management on already busy maintainers.</p>
<p>Whilst the original pilot program is now full, we have the capacity to onboard many other projects and would love to work with you. We are happy to offer a discounted subscription if your employer that sponsors your time on the said CNCF project will pay for it. Otherwise, contact us anyway, and we'll put you into email contact with <a href="https://www.linkedin.com/in/caniszczyk/">Chris Aniszczyk</a> so you can let him know how this would help you.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Right sizing VMs for GitHub Actions]]></title>
            <link>https://actuated.dev/blog/right-sizing-vms-github-actions</link>
            <guid>https://actuated.dev/blog/right-sizing-vms-github-actions</guid>
            <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[How do you pick the right VM size for your GitHub Actions runners? We wrote a custom tool to help you find out.]]></description>
            <content:encoded><![CDATA[<p>When we <a href="https://actuated.dev/blog/arm-ci-cncf-ampere">onboarded the etcd project from the CNCF</a>, they'd previously been using a self-hosted runner for their repositories on a bare-metal host. There are several drawbacks to this approach, <a href="/blog/is-the-self-hosted-runner-safe-github-actions">including potential security issues, especially when using Docker</a>.</p>
<p>actuated VM sizes can be configured by a label, and you can pick any combination of vCPU and RAM, there's no need to pick a pre-defined size.</p>
<p>At the same time, it can be hard to know what size to pick, and if you make the VM size too large, then you won't be able to run as many jobs at once.</p>
<p>There's three main things to consider:</p>
<ul>
<li>The number of vCPUs - if there are not enough for the job, it'll be slower, and may hit timeouts which cause a failure</li>
<li>The amount of RAM - if there's not enough, all the RAM could be consumed, and the VM will crash. You may not even get a helpful error message</li>
<li>The amount of disk space per VM - if you make this too high, you'll be limited on the amount of jobs you can run at once, if you make it too low, jobs can fail, sometimes with non-obvious error messages</li>
</ul>
<p>We wrote a tool called vmmeter which takes samples of resource consumption over the duration of a build, and will then report back with the peak and average values.</p>
<p>vmmeter is written in Go, and is available to use as a pre-built binary. We may consider open-sourcing it in the future. The information you gather still needs to be carefully considered and some experimentation will be required to get the right balance between VM size and performance.</p>
<p>The tool can be run in an action by adding some YAML, however, it can also be run on any Linux system using bash, or potentially within a different CI/CD system. See the note at the end if you're interested in trying that out.</p>
<h2>Running vmmeter inside GitHub Actions</h2>
<p>This action will work with a Linux VM environment, so with a hosted runner or with actuated. It may not work when used within the <code>containers:</code> section of a workflow, or with a Kubernetes-based runner.</p>
<p>Add to the top of your GitHub action:</p>
<pre><code class="hljs language-yaml">
<span class="hljs-attr">steps:</span>

<span class="hljs-comment"># vmmeter start</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">vmmeter</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            sudo -E arkade oci install ghcr.io/openfaasltd/vmmeter:latest --path /usr/local/bin/
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/vmmeter-action@master</span>
<span class="hljs-comment"># vmmeter end</span>
</code></pre>
<p>The first set installs arkade, which we then use to extract vmmeter from a container image to the host.</p>
<p>Then <code>self-actuated/vmmeter-action</code> is used to run the tool in the background, and also runs a post-setup setup to stop the measurements, and upload the results to the workflow run.</p>
<p>To show you how the tool works, I ran a simple <a href="https://github.com/actuated-samples/kernel-builder-linux-6.0/blob/master/.github/workflows/microvm-kernel.yml">build of the Linux Kernel</a> without any additional modules or options added in.</p>
<p>Here's the summary text that was uploaded to the workflow run:</p>
<pre><code>Total RAM: 61.2GB
Total vCPU: 32
Load averages:
Max 1 min: 5.63 (17.59%)
Max 5 min: 1.25 (3.91%)
Max 15 min: 0.41 (1.28%)

RAM usage (10 samples):
Max RAM usage: 2.528GB

Max 10s avg RAM usage: 1.73GB
Max 1 min avg RAM usage: 1.208GB
Max 5 min avg RAM usage: 1.208GB

Disk read: 374.2MB
Disk write: 458.2MB
Max disk I/O inflight: 0
Free: 45.57GB	Used: 4.249GB	(Total: 52.52GB)

Egress adapter RX: 271.4MB
Egress adapter TX: 1.535MB

Entropy min: 256
Entropy max: 256

Max open connections: 125
Max open files: 1696
Processes since boot: 18081

Run time: 45s
</code></pre>
<p>The main thing to look for is the peak load on the system. This roughly corresponds to the amount of vCPUs used at peak. If the number is close to the amount you allocated, then try allocating more and measuring the effect in build time and peak usage.</p>
<p>We've found that some jobs are RAM hungry, and others use a lot of CPU. So if you find that the RAM requested is much higher than the peak or average usage, the chances are that you can safely reduce it.</p>
<p>Disk usage is self-explanatory, if you've allocated around 30GB per VM, and a job is getting close to that limit, it may need increasing to avoid future failures.</p>
<p>Disk, network read/write and open files are potential indicators of I/O contention. if a job reads or writes a large amount of data over the network interface, then that may become a bottleneck. <a href="/blog/local-caching-for-github-actions">Caching</a> is one of the ways to work around that, whether you set up your workflow to use GitHub's hosted cache, or one running in the same datacenter or region as your CI servers.</p>
<h2>Wrapping up</h2>
<p>In one case, a build on the etcd-io project was specified with 16 vCPU and 32GB of RAM, but when running vmmeter, they found that less than 2 vCPU was used at peak and less than 3GB of RAM. That's a significant difference.</p>
<p>Toolpath is a commercial customer, and we were able to help them reduce their wall time per pull request from 6 hours to 60 minutes. Or from 6x 1 hour jobs to 6x 15-20 minute jobs running in parallel. Jason Gray told me during a product market fit interview that "the level of expertise and support pays for itself". We'd noticed that his teams jobs were requesting far too much CPU, but not enough RAM and were able to make recommendations. We then saw that disk space was running dangerously low, and were able to reconfigure their dedicated build servers for them, remotely, without them having to even think about it.</p>
<p>If you'd like to try out vmmeter, it's free to use on GitHub's hosted runners and on actuated runners. We wouldn't recommend making it a permanent fixture in your workflow, because if it were to fail or exit early for any reason, it may mark the whole build as a failure.</p>
<p>Instead, we recommend you use it learn and explore, and fine-tune your VM sizes. Getting the numbers closer to a right-size could reduce your costs with hosted runners and your efficiency with actuated runners.</p>
<p>The source-code for the action is available here: <a href="https://github.com/self-actuated/vmmeter-action">self-actuated/vmmeter-action</a>.</p>
<ul>
<li><a href="https://github.com/self-actuated/vmmeter-action/blob/master/index.js">index.js</a> is used to setup the tool and start taking measurements</li>
<li><a href="https://github.com/self-actuated/vmmeter-action/blob/master/post.js">post.js</a> communicates to vmmeter over a TCP port and tells it to dump its response to <code>/tmp/vmmeter.log</code>, then to exit</li>
</ul>
<p><em>What if you're not using GitHub Actions?</em></p>
<p>You can run vmmeter with bash on your own system, and may also able to use vmmeter in GitLab CI or Jenkins. We've got <a href="https://gist.github.com/alexellis/1f33e581c75e11e161fe613c46180771#running-vmmeter-inside-github-actions">manual instructions for vmmeter here</a>. You can even just start it up right now, do some work and then call the collect endpoint to see what was used over that period of time, a bit like a generic profiler.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Testing the impact of a local cache for building Discourse]]></title>
            <link>https://actuated.dev/blog/local-caching-for-github-actions</link>
            <guid>https://actuated.dev/blog/local-caching-for-github-actions</guid>
            <pubDate>Fri, 23 Feb 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[We compare the impact of switching Discourse's GitHub Actions from self-hosted runners and a hosted cache, to a local cache with S3.]]></description>
            <content:encoded><![CDATA[<p>We heard from the <a href="https://github.com/discourse/discourse">Discourse</a> project last year because they were looking to speed up their builds. After trying out a couple of solutions that automated self-hosted runners, they found out that whilst faster CPUs were nice, reliability was a problem and the cache hosted on GitHub's network became the new bottleneck. We ran some tests to compare the hosted cache with hosted runners, to self-hosted with a local cache running with S3. This post will cover what we found.</p>
<img src="/images/2024-02-local-caching-for-github-actions/discourse-readme-logo.png" width="200" alt="Discourse logo">
<blockquote>
<p>Discourse is the online home for your community. We offer a 100% open source community platform to those who want complete control over how and where their site is run.</p>
</blockquote>
<h2>Set up a local cache</h2>
<p>Hosted runners are placed close to the cache which means the latency is very low. Self-hosted runners can also make good use of this cache but the added latency can negate the advantage of switching to these faster runners. Running a local S3 cache with <a href="https://github.com/minio/minio">Minio</a> or <a href="https://github.com/seaweedfs/seaweedfs">Seaweedfs</a> on the self hosted runner or in the same region/network can solve this problem.</p>
<p>For this test we ran the cache on the runner host. <a href="https://docs.actuated.dev/tasks/local-github-cache/">Instructions</a> to set up a local S3 cache with Seaweedfs can be found in our docs.</p>
<p>The Discourse repo is already using the <code>actions/cache</code>in their tests workflow which makes it easy to switch out the official <a href="https://github.com/actions/cache">actions/cache</a> with <a href="https://github.com/tespkg/actions-cache">tespkg/actions-cache</a>.</p>
<p>The S3 cache is not directly compatible with the official <a href="https://github.com/actions/cache">actions/cache</a> and some changes to the workflows are required to start using the cache.</p>
<p>The <code>tespkg/actions-cache</code> supports the same properties as the actions cache and only requires some additional parameters to configure the S3 connection.</p>
<pre><code class="hljs language-diff"> - name: Bundler cache
<span class="hljs-deletion">-  uses: actions/cache@v3</span>
<span class="hljs-addition">+  uses: tespkg/actions-cache@v1</span>
   with:
<span class="hljs-addition">+    endpoint: "192.168.128.1"</span>
<span class="hljs-addition">+    port: 8333</span>
<span class="hljs-addition">+    insecure: true</span>
<span class="hljs-addition">+    accessKey: ${{ secrets.ACTIONS_CACHE_ACCESS_KEY }}</span>
<span class="hljs-addition">+    secretKey: ${{ secrets.ACTIONS_CACHE_SECRET_KEY }}</span>
<span class="hljs-addition">+    bucket: actuated-runners</span>
<span class="hljs-addition">+    region: local</span>
<span class="hljs-addition">+    use-fallback: false</span>

     path: vendor/bundle
     key: ${{ runner.os }}-${{ matrix.ruby }}-gem-${{ hashFiles('**/Gemfile.lock') }}-cachev2
</code></pre>
<p>The endpoint could also be a HTTPS URL to a S3 server hosted within the same network as the self-hosted runners.</p>
<p>If you are relying on the built-in cache support that is included in some actions like <a href="https://github.com/actions/setup-node#caching-global-packages-data">setup-node</a> and <a href="https://github.com/actions/setup-go">setup-go</a> you will need to add an additional caching step to your workflow as they are not directly compatible with the self-hosted S3 cache.</p>
<h2>The impact of switching to a local cache</h2>
<p>The <a href="https://github.com/discourse/discourse/actions/workflows/tests.yml">Tests workflow</a> from the <a href="https://github.com/discourse/discourse">Discourse repository</a> was used to test the impact of switching to a local cache. We ran the workflow on a self-hosted Actuated runner, both with the S3 local cache and with the GitHub cache.</p>
<p>Next we looked at the time required to restore the caches in our two environments and compared it with the times we saw on GitHub hosted runners:</p>
<table>
<thead>
<tr>
<th></th>
<th>Bundler cache<br>(±273MB)</th>
<th>Yarn cache<br>(±433MB)</th>
<th>Plugins gems cache<br>(±51MB)</th>
<th>App state cache<br>(±1MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actuated with local cache</strong></td>
<td>5s</td>
<td>11s</td>
<td>1s</td>
<td>0s</td>
</tr>
<tr>
<td><strong>Actuated with hosted cache</strong></td>
<td>13s</td>
<td>19s</td>
<td>3s</td>
<td>2s</td>
</tr>
<tr>
<td><strong>Runner &#x26; cache hosted on GitHub</strong></td>
<td>6s</td>
<td>11s</td>
<td>3s</td>
<td>2s</td>
</tr>
</tbody>
</table>
<p>While the GitHub runner and the self-hosted runner with a local cache perform very similarly, cache restores on the self-hosted runner that uses the GitHub cache take a bit longer.</p>
<p>If we take a look at the yarn cache, which is the biggest cache, we can see that switching to the local S3 cache saved 8s for the cache size in this test vs using GitHub's cache from a self-hosted runner. This is a 42% improvement.</p>
<p>Depending on your workflow and the cache size this can add up quickly. If a pipeline has multiple steps or when you are running matrix builds a cache step may need to run multiple times. In the case of the Discourse repo this cache step runs nine times which adds up to 1m12s that can be saved per workflow run.</p>
<p>When Discourse approached us, we found that they had around a dozen jobs running for each pull request, all with varying sizes of caches. At busy times of the day, their global team could have 10 or more of those pull requests running, so these savings could add up to a significant amount.</p>
<p><strong>What if you also cached git checkout</strong></p>
<p>If your repository is a monorepo or has lots of large artifacts, you may get a speed boost caching the <code>git checkout</code> step too. Depending on where your runners are hosted, pulling from GitHub can take some time vs. restoring the same files from a local cache.</p>
<p>We demonstrated what impact that had for Settlemint's CTO in <a href="/blog/faster-self-hosted-cache">this case study</a>. They saw a cached checkout using a GitHub's hosted cache from from 2m40s to 11s.</p>
<p><strong>How we improved testpkg's custom action</strong></p>
<p>During our testing we noticed that every cache restore took a minimum of 10 seconds regardless of the cache size. It turned out to be an issue with timeouts in the <code>tespkg/actions-cache</code> action when listing objects in S3. We reported it and sent them <a href="https://github.com/tespkg/actions-cache/pull/44">a pull request with a fix</a>.</p>
<p>With the fix in place restoring small caches from the local cache dropped from 10s to sub 1s.</p>
<h2>The impact of switching to faster runners</h2>
<p>The Discourse repo uses the larger GitHub hosted runners to run tests. The jobs we are going to compare are part of the <a href="https://github.com/discourse/discourse/actions/workflows/tests.yml">Tests workflow</a>. They are using runners with 8 CPUs and 32GB of ram so we replaced the <code>runs-on</code> label with an actuated label <code>actuated-8cpu-24gb</code> to run the jobs on similar sized microVMs.</p>
<p>All jobs ran on the same <a href="https://www.hetzner.com/dedicated-rootserver/ax102/">Hetzner AX102</a> bare metal host.</p>
<p>This table compares the time it took to run each job on the hosted runner and on our Actuated runner.</p>
<table>
<thead>
<tr>
<th>Job</th>
<th>GitHub hosted runner</th>
<th>Actuated runner</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>core annotations</strong></td>
<td>3m23s</td>
<td>1m22s</td>
<td>59%</td>
</tr>
<tr>
<td><strong>core backend</strong></td>
<td>7m11s</td>
<td>6m0s</td>
<td>16%</td>
</tr>
<tr>
<td><strong>plugins backend</strong></td>
<td>7m42s</td>
<td>5m54s</td>
<td>23%</td>
</tr>
<tr>
<td><strong>plugins frontend</strong></td>
<td>5m28s</td>
<td>4m3s</td>
<td>26%</td>
</tr>
<tr>
<td><strong>themes frontend</strong></td>
<td>4m20s</td>
<td>2m46s</td>
<td>36%</td>
</tr>
<tr>
<td><strong>chat system</strong></td>
<td>9m37s</td>
<td>6m33s</td>
<td>32%</td>
</tr>
<tr>
<td><strong>core system</strong></td>
<td>7m12s</td>
<td>5m24s</td>
<td>25%</td>
</tr>
<tr>
<td><strong>plugin system</strong></td>
<td>5m32s</td>
<td>3m56s</td>
<td>29%</td>
</tr>
<tr>
<td><strong>themes system</strong></td>
<td>4m32s</td>
<td>2m41</td>
<td>41%</td>
</tr>
</tbody>
</table>
<p>The first thing we notice is that all jobs completed faster on the Actuated runner. On average we see an improvement of around 1m40s seconds for each individual job.</p>
<h2>Conclusion</h2>
<p>While switching to faster self-hosted runners is the most obvious way to speed up your builds, the cache hosted on GitHub's network can become a new bottleneck if you use caching in your actions. After switching to a local S3 cache we saw a very significant improvement in the cache latency. Depending on how heavily the cache is used in your workflow and the size of your cache artifacts, switching to a local S3 cache might even have a bigger impact on build times.</p>
<p>Both Seaweedfs and Minio were tested in our setup and they performed in a very similar way. Both have different open source licenses, so we'd recommend reading those before picking one or the other. Of course you could also use AWS S3, Google Cloud Storage, or another S3 compatible hosted service.</p>
<p>In addition to the reduced latency, switching to a self hosted cache has a couple of other benefits.</p>
<ul>
<li>No limit on the cache size. The GitHub cache has a size limit of 10GB before the oldest cache entries start to get pruned.</li>
<li>No egress costs for pushing data to the cache from self-hosted runners.</li>
</ul>
<p>GitHub's caching action does not yet support using a custom S3 server, so we had to make some minor adjustments to the Discourse's workflow files. For this reason, if you use something like setup-go or setup-node, you won't be able to just set <code>cache: true</code>. Instead you'll need an independent caching step with the <code>testpkg/actions-cache</code> action.</p>
<p>If you'd like to reach out to us and see if we can advise you on how to optmise your builds, you can <a href="https://actuated.dev/pricing">set up a call with us here.</a>.</p>
<p>If you want to learn more about caching for GitHub Actions checkout some of our other blog posts:</p>
<p>You may also like:</p>
<ul>
<li><a href="https://actuated.dev/blog/caching-in-github-actions">Make your builds run faster with Caching for GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/faster-self-hosted-cache">Fixing the cache latency for self-hosted GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is GitHub's self-hosted runner safe for open source?</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[December Boost: Custom Job Sizes, eBPF Support & KVM Acceleration]]></title>
            <link>https://actuated.dev/blog/custom-sizes-bpf-kvm</link>
            <guid>https://actuated.dev/blog/custom-sizes-bpf-kvm</guid>
            <pubDate>Mon, 04 Dec 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[You can now request custom amounts of RAM and vCPU for jobs, run eBPF within jobs, and use KVM acceleration.]]></description>
            <content:encoded><![CDATA[<p>In this December update, we've got three new updates to the platform that we think you'll benefit from. From requesting custom vCPU and RAM per job, to eBPF features, to spreading your plan across multiple machines dynamically, it's all new and makes actuated better value.</p>
<h2>New eBPF support and 5.10.201 Kernel</h2>
<p>And as part of our work to provide hosted <a href="/blog/arm-ci-cncf-ampere">Arm CI for CNCF projects</a>, including <a href="https://github.com/cilium/tetragon">Tetragon</a> and <a href="https://github.com/cilium/cilium">Cilium</a>, we've now enabled eBPF and BTF features within the Kernel.</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Berkeley_Packet_Filter">Berkley Packet Filter (BPF)</a> is an advanced way to integrate with the Kernel, for observability, security and networking. You'll see it included in various CNCF projects like Cilium, <a href="https://github.com/falcosecurity/falco">Falco</a>, <a href="https://www.cncf.io/projects/kepler/">Kepler</a>, and others.</p>
</blockquote>
<p>Whilst BPF is powerful, it's also a very fast moving space, and was particularly complicated to patch to Firecracker's minimal Kernel configuration. We want to say a thank you to <a href="https://twitter.com/mtardy_?lang=en">Mahé Tardy
</a> who maintains Tetragon and to <a href="https://www.linkedin.com/in/mauilion">Duffie Coolie</a> both from <a href="https://isovalent.com/">Isovalent</a> for pointers and collaboration.</p>
<p>We've made a big jump in the supported Kernel version from 5.10.77 up to 5.10.201, with newer revisions being made available on a continual basis.</p>
<p>To update your servers, log in via SSH and edit <code>/etc/default/actuated</code>.</p>
<p>For amd64:</p>
<pre><code>AGENT_IMAGE_REF="ghcr.io/openfaasltd/actuated-ubuntu22.04:x86_64-latest"
AGENT_KERNEL_REF="ghcr.io/openfaasltd/actuated-kernel:x86_64-latest"
</code></pre>
<p>For arm64:</p>
<pre><code>AGENT_IMAGE_REF="ghcr.io/openfaasltd/actuated-ubuntu22.04:aarch64-latest"
AGENT_KERNEL_REF="ghcr.io/openfaasltd/actuated-kernel:aarch64-latest"
</code></pre>
<p>Once you have the new images in place, reboot the server. Updates to the Kernel and root filesystem will be delivered Over The Air (OTA) automatically by our team.</p>
<h2>Request custom vCPU and RAM per job</h2>
<p>Our initial version of actuated aimed to set a specific vCPU and RAM value for each build, designed to slice up a machine equally for the best mix of performance and concurrency. We would recommend it to teams during their onboarding call, then mostly leave it as it was. For a machine with 128GB RAM and 32 threads, you may have set it up for 8 jobs with 4x vCPU and 16GB RAM each, or 4 jobs with 8x vCPU and 32GB RAM.</p>
<p>However, whilst working with Justin Gray, CTO at Toolpath, we found that their build needed increasing amounts of RAM to avoid an Out Of Memory (OOM) crash, and so implemented custom labels.</p>
<p>These labels do not have any predetermined values, so you can change them to any value you like, independently. You're not locked into a set combinations.</p>
<p>Small tasks, automation, publishing Helm charts?</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-2cpu-8gb</span>
</code></pre>
<p>Building a large application, or training an AI model?</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-32cpu-128gb</span>
</code></pre>
<h2>Spreading your plan across all available hosts</h2>
<p>Previously, if you had a plan with 10 concurrent builds and both an Arm server and an amd64 server, we'd split your plan statically 50/50. So you could run a maximum of 5 Arm and 5 amd64 builds at the same time.</p>
<p>Now, we've made this dynamic, all of your 10 builds can start on the Arm or amd64 server. Or, 1 could start on the Arm server, then 9 on the amd64 server, and so on.</p>
<p>The change makes the product better value for money, and we had always wanted it to work this way.</p>
<p>Thanks to <a href="https://uk.linkedin.com/in/patrickjkstephens">Patrick Stephens at Fluent/Calyptia</a> for the suggestion and for helping us test it out.</p>
<h2>KVM acceleration aka running VMs in your CI pipeline</h2>
<p>When we started actuated over 12 months ago, there was no support for using KVM acceleration, or running a VM within a GitHub Actions job within GitHub's infrastructure. We made it available for our customers first, with a custom Kernel configuration for <em>x86_64</em> servers. Arm support for launching VMs within VMs is not currently available in the current generation of Ampere servers, but may be available within the next generation of chips and Kernels.</p>
<p>We have several tutorials including how to run Firecracker itself within a CI job, Packer, Nix and more.</p>
<p>When you run Packer in a VM, instead of with one of the cloud drivers, you save on time and costs, by not having to fire up cloud resources on AWS, GCP, Azure, and so forth. Instead, you can run a local VM to build the image, then convert it to an AMI or another format.</p>
<p>One of our customers has started exploring launching a VM during a CI job in order to test air-gapped support for enterprise customers. This is a great example of how you can use nested virtualisation to test your own product in a repeatable way.</p>
<p>Nix benefits particularly from being able to create a clean, isolated environment within a CI pipeline, to get a repeatable build. <a href="https://twitter.com/grhmc">Graham Christensen</a> from <a href="https://github.com/determinateSystems/nix-installer-action">Determinate Systems reached out to collaborate on testing their Nix installer</a> in actuated.</p>
<p>He didn't expect it to run, but when it worked first time, he remarked: "Perfect! I'm impressed and happy that our action works out of the box."</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">specs:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">ci</span>
    <span class="hljs-attr">runs-on:</span> [<span class="hljs-string">actuated-16cpu-32gb</span>]
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">DeterminateSystems/nix-installer-action@main</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">run:</span> <span class="hljs-string">|
            nix-build '&#x3C;nixpkgs/nixos/tests/doas.nix>'
</span></code></pre>
<ul>
<li><a href="https://actuated.dev/blog/kvm-in-github-actions">How to run KVM guests in your GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/automate-packer-qemu-image-builds">Automate Packer Images with QEMU and Actuated</a></li>
<li><a href="https://actuated.dev/blog/faster-nix-builds">Faster Nix builds with GitHub Actions and actuated</a></li>
<li><a href="https://docs.actuated.dev/examples/kvm-guest">Docs: Run a KVM guest</a></li>
</ul>
<h2>Wrapping up</h2>
<p>We've now released eBPF/BTF support as part of onboarding CNCF projects, updated to the latest Kernel revision, made scheduling better value for money &#x26; easier to customise, and have added a range of tutorials for getting the most out of nested virtualisation.</p>
<p>If you'd like to try out actuated, you can get started same day.</p>
<p><a href="/pricing">Talk to us.</a>.</p>
<p>You may also like:</p>
<ul>
<li><a href="/blog/arm-ci-cncf-ampere">Announcing managed Arm CI for CNCF projects</a></li>
<li><a href="https://docs.actuated.dev/">Actuated docs &#x26; FAQ</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing managed Arm CI for CNCF projects]]></title>
            <link>https://actuated.dev/blog/arm-ci-cncf-ampere</link>
            <guid>https://actuated.dev/blog/arm-ci-cncf-ampere</guid>
            <pubDate>Wed, 25 Oct 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Ampere Computing and The Cloud Native Computing Foundation are sponsoring a pilot of actuated's managed Arm CI for CNCF projects.]]></description>
            <content:encoded><![CDATA[<p>In this post, we'll cover why <a href="https://amperecomputing.com/">Ampere Computing</a> and <a href="https://www.cncf.io/">The Cloud Native Computing Foundation (CNCF)</a> are sponsoring a pilot of actuated for open source projects, how you can get involved.</p>
<p>We'll also give you a quick one year recap on actuated, if you haven't checked in with us for a while.</p>
<h2>Managed Arm CI for CNCF projects</h2>
<p>At KubeCon EU, I spoke to <a href="https://www.linkedin.com/in/caniszczyk/">Chris Aniszczyk</a>, CTO at the <a href="https://cncf.io">Cloud Native Computing Foundation (CNCF)</a>, and told him about some of the results we'd been seeing with actuated customers, including Fluent Bit, which is a CNCF project. Chris told me that many teams were either putting off Arm support all together, were suffering with the slow builds that come from using QEMU, or were managing their own infrastructure which was underutilized.</p>
<p><a href="https://deploy.equinix.com/">Equinix</a> provides a generous amount of credits to the CNCF under <a href="https://github.com/cncf/cluster">CNCF Community Infrastructure Lab (CIL)</a>, including access to powerful Ampere Q80 Arm servers (<a href="https://deploy.equinix.com/product/servers/c3-large-arm64/">c3.large.arm64</a>), that may at times be required by Equinix customers for their own Arm workloads.</p>
<p>You can find out more about <a href="https://amperecomputing.com/products/processors">Ampere's Altra platform here</a>, which is being branded as a "Cloud Native" CPU, due to its low power consumption, high core count, and ubiquitous availability across Google Cloud, Oracle Cloud Platform, Azure, Equinix Metal, Hetzner Cloud, and Alibaba Cloud.</p>
<p>As you can imagine, over time, different projects have deployed 1-3 of their own runner servers, each with 256GB of RAM and 80 Cores, which remain idle most of the time, and are not available to other projects or Equinix customers when they may need them suddenly. So, if actuated can reduce this number, whilst also improving the experience for maintainers, then that's a win-win.</p>
<p>Around the same time as speaking to Chris, Ampere reached out and asked how they could help secure actuated for a number of CNCF projects.</p>
<p>Together, Ampere and the CNCF are now sponsoring an initial 1-year pilot of managed Arm CI provided by actuated, for CNCF projects, with the view to expand it, if the pilot is a success.</p>
<p><a href="https://www.linkedin.com/in/edwardvielmetti/">Ed Vielmetti</a>, Developer Partner Manager at Equinix said:</p>
<blockquote>
<p>I'm really happy to see this all come together. If all goes according to plan, we'll have better runner isolation, faster builds, and a smaller overall machine footprint.</p>
</blockquote>
<p><a href="https://www.linkedin.com/in/dneary/">Dave Neary</a>, Director of Developer Relations at Ampere Computing added:</p>
<blockquote>
<p>Actuated offers a faster, more secure way for projects to run 64-bit Arm builds, and will also more efficiently use the Ampere Altra-based servers being used by the projects.</p>
<p>We're happy to support CNCF projects running their CI on Ampere Computing's Cloud Native Processors, hosted by Equinix.</p>
</blockquote>
<h2>One year recap on actuated</h2>
<p>In case you are hearing about actuated for the first time, I wanted to give you a quick one year recap.</p>
<p>Just over 12 months ago, we <a href="https://actuated.dev/blog/blazing-fast-ci-with-microvms">announced the work we'd been doing with actuated</a> to improve self-hosted runner security and management. We were pleasantly surprised with the amount of people that responded who'd had a common experience with slow builds, running out of RAM, limited disk space, and a lack of an easy and secure way to run self-hosted runners.</p>
<p>Fast forward to today, and we have run over 140,000 individual Firecracker VMs for customers on their own hardware. Rather than the fully managed service that GitHub offers, we believe that you should be able to bring your own hardware, and pay a flat-rate fee for the service, rather than being charged per-minute.</p>
<p>The CNCF project brings about 64-bit Arm support, but we see a good mix of <code>x86_64</code> and Arm builds from customers, with both closed and open-source repositories being used.</p>
<p>The main benefits are having access to bigger, faster and more specialist hardware.</p>
<ul>
<li>For <code>x86_64</code> builds, we see about a 3x speed-up vs. using GitHub's hosted runners, in addition to being able to add more RAM and disk space to builds.</li>
<li>For Arm64, we see builds which take several hours or which fail to complete within a 6 hour window using QEMU, go down to 3-5 minutes.</li>
</ul>
<p>Vendors and consumers are becoming increasingly aware of the importance of the supply chain, GitHub's self-hosted runner is <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">not recommended for open source repos</a>. Why? Due to the way side-effects can be left over between builds. Actuated uses a fresh, immutable, Firecracker VM for every build which boots up in less than 1 second and is destroyed after the build completes, which removes this risk.</p>
<p>If you're wanting to know more about why we think microVMs are the only tool that makes sense for secure CI, then I'd recommend my talk from Cloud Native Rejekts earlier in the year: <a href="https://www.youtube.com/watch?v=pTQ_jVYhAoc">Face off: VMs vs. Containers vs Firecracker</a>.</p>
<h2>What are maintainers saying?</h2>
<p><a href="https://www.linkedin.com/in/ellmh">Ellie Huxtable</a> is the maintainer of <a href="https://atuin.sh/">Atuin</a>, a popular open-source tool to sync, search and backup shell history. Her Rust build for the CLI took 90 minutes with QEMU, but was reduced to just 3 minutes with actuated, and a native Arm server.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Thanks to <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a>, Atuin now has very speedy ARM docker builds in our GitHub actions! Thank you <a href="https://twitter.com/alexellisuk?ref_src=twsrc%5Etfw">@alexellisuk</a> 🙏<br><br>Docker builds on QEMU: nearly 90 mins<br>Docker builds on ARM with Actuated: ~3 mins</p>&mdash; Ellie Huxtable (@ellie_huxtable) <a href="https://twitter.com/ellie_huxtable/status/1715261549172936776?ref_src=twsrc%5Etfw">October 20, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>For Fluent Bit, one of their Arm builds was taking over 6 hours, which meant it always failed with a timed-out on a hosted runner. <a href="https://www.linkedin.com/in/patrickjkstephens/?originalSubdomain=uk">Patrick Stephens</a>, Tech Lead of Infrastructure at Calyptia reached out to work with us. We got the time down to 5 minutes by changing <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated-arm64-4cpu-16gb</code>, and if you need more or less RAM/CPU, you can tune those numbers as you wish.</p>
<p>Patrick shares about the experience on the Calyptia blog, including the benefits to their <code>x86_64</code> builds for the commercial Calyptia product: <a href="https://calyptia.com/blog/scaling-builds-with-actuated">Scaling ARM builds with Actuated</a>.</p>
<p>A number of CNCF maintainers and community leaders such as <a href="https://www.linkedin.com/in/davanum/">Davanum Srinivas (Dims)</a>, Principal Engineer at AWS have come forward with project suggestions, and we're starting to work through them, with the first two being Fluent Bit and etcd.</p>
<p><a href="https://fluentbit.io/">Fluent Bit</a> describes itself as:</p>
<blockquote>
<p>..a super fast, lightweight, and highly scalable logging and metrics processor and forwarder. It is the preferred choice for cloud and containerized environments.</p>
</blockquote>
<p><a href="https://etcd.io">etcd</a> is a core component of almost every Kubernetes installation and is responsible for storing the state of the cluster.</p>
<blockquote>
<p>A distributed, reliable key-value store for the most critical data of a distributed system</p>
</blockquote>
<p>In the case of etcd, there were <a href="https://github.com/etcd-io/etcd/pull/16801/files#diff-b8f5f4d0db4fb959d72d74463e2fd2637feb69f6b9e1dce61ad47ee031806dbd">two servers being maintained by five individual maintainers</a>, all of that work goes away by adopting actuated.</p>
<p>We even sent etcd <a href="https://github.com/etcd-io/etcd/pull/16801">a minimal Pull Request</a> to make the process smoother.</p>
<p><a href="https://github.com/jmhbnz">James Blair, Specialist Solution Architect at Red Hat</a>, commented:</p>
<blockquote>
<p>I believe managed on demand arm64 CI hosts will definitely be a big win for the project. Keen to trial this.</p>
</blockquote>
<p>Another maintainer also commented that they will <a href="https://github.com/etcd-io/etcd/pull/16801#issuecomment-1774024719">no longer need to worry about "leaky containers"</a>.</p>
<p><a href="https://github.com/etcd-io/etcd/actions/runs/6635037153"><img src="/images/2023-10-cncf/etcd-nightly.png" alt="One of the first nightly jobs running within an isolated Firecracker VM"></a></p>
<blockquote>
<p>One of the first nightly workflows running within 4x separate isolated Firecracker VMs, one per job</p>
</blockquote>
<p>Prior to adopting actuated, the two servers were only configured to run one job at a time, afterwards, the jobs are scheduled by the control-plane, according to the amount of available RAM and CPU in the target servers.</p>
<h2>How do we get access?</h2>
<p>If you are working on a CNCF project and would like access, <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">please contact us via this form</a>. If your project gets selected for the pilot, there are a couple of things you may need to do.</p>
<ol>
<li>If you are already using the GitHub Actions runner hosted on Equinix, then change the <code>runs-on: self-hosted</code> label to: <code>runs-on: actuated-arm64-8cpu-16gb</code>. Then after you've seen a build or two pass, delete the old runner.</li>
<li>If you are using QEMU, the best next step is to "split" the build into two jobs which can run on <code>x86_64</code> and <code>arm64</code> natively, that's what Ellie did and it only took her a few minutes.</li>
</ol>
<p>The label for <code>runs-on:</code> allows for dynamic configuration of vCPU and GBs of RAM, just edit the label to match your needs, for etcd, the team asked for 8vCPU and 32GB of RAM, so they used <code>runs-on: actuated-arm64-8cpu-32gb</code>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I had to split the docker build so that the ARM half would build on ARM, and x86 on x86, and then a step to combine the two - overall this works out to be a very significant improvement<a href="https://t.co/69cIxjYRcW">https://t.co/69cIxjYRcW</a></p>&mdash; Ellie Huxtable (@ellie_huxtable) <a href="https://twitter.com/ellie_huxtable/status/1715266936592904446?ref_src=twsrc%5Etfw">October 20, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>We have full instructions for 2, in the following tutorial: <a href="https://actuated.dev/blog/how-to-run-multi-arch-builds-natively">How to split up multi-arch Docker builds to run natively</a>.</p>
<p><strong>Is there access for AMD64?</strong></p>
<p>This program is limited to CNCF projects and Arm CI only. That said, most actuated customers run AMD64 builds with us.</p>
<p>GitHub already provides access to AMD64 runners for free for open source projects, that should cover most OSS project's needs.</p>
<p>So why would you want dedicated AMD64 support from actuated? Firstly, our recommended provider makes builds up to 3x quicker, secondly, you can run on private repos if required, without accuring a large bill.</p>
<p><strong>What are all the combinations of CPU and RAM?</strong></p>
<p>We get this question very often, but have tried to be as clear as possible in this blog post and <a href="https://docs.actuated.dev/examples/custom-vm-size/">in the docs</a>. There are no set combinations. You can come up with what you need.</p>
<p>That helps us make best use of the hardware, you can even have just a couple of cores, and max out to 256GB of RAM, if that's what your build needs.</p>
<p><strong>What if the sponsored program is full?</strong></p>
<p>The program has been very popular and there is a limit to the budget and number of projects that Ampere and the CNCF agreed to pay for. If you contact us and we tell you the limit has been reached, then your employer could sponsor the subscription, and we'll give you a special discount - you could get started immediately. Or you'll need to contact <a href="https://www.linkedin.com/in/caniszczyk/">Chris Aniszczyk</a> and tell him why it would be of value to the OSS project you represent to have native Arm CI. If you get in touch with us, we can introduce you to him via email if needed.</p>
<h2>Learn more about actuated</h2>
<p>We're initially offering access to managed Arm CI for CNCF projects, but if you're working for a company that is experiencing friction with CI, please reach out to us to talk <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">using this form</a>.</p>
<p>Ampere who are co-sponsoring our service with the CNCF have their own announcement here: <a href="https://amperecomputing.com/blogs/ampere-computing-and-cncf-supporting-arm-native-ci-for-ncf-projects">Ampere Computing and CNCF Supporting Arm Native CI for CNCF Projects</a>.</p>
<ul>
<li>Read one of our <a href="https://actuated.dev/blog">recent blog posts</a></li>
<li>Learn more about <a href="https://docs.actuated.dev">actuated in the docs</a></li>
<li><a href="https://actuated.dev">Read what our customers are saying about our service</a></li>
</ul>
<blockquote>
<p>Did you know? <a href="https://actuated.dev/blog/secure-microvm-ci-gitlab">Actuated for GitLab CI is now in technical preview, watch a demo here</a>.</p>
</blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Grab your lab coat - we're building a microVM from a container]]></title>
            <link>https://actuated.dev/blog/firecracker-container-lab</link>
            <guid>https://actuated.dev/blog/firecracker-container-lab</guid>
            <pubDate>Tue, 05 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[No more broken tutorials, build a microVM from a container, boot it, access the Internet]]></description>
            <content:encoded><![CDATA[<p>When I started learning <a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a>, I ran into frustration after frustration with broken tutorials that were popular in their day, but just hadn't been kept up to date. Almost nothing worked, or was far too complex for the level of interest I had at the time. Most recently, one of the Firecracker maintainers in an effort to make the quickstart better, made it even harder to use. (You can still get a copy of the <a href="https://actuated.dev/blog/kvm-in-github-actions">original Firecracker quickstart in our tutorial on nested virtualisation</a>)</p>
<p>So I wrote a lab that takes a container image and converts it to a microVM. You'll get your hands dirty, you'll run a microVM, you'll be able to use <code>curl</code> and <code>ssh</code>, even expose a HTTP server to the Internet via inlets, if (like me), you find that kind of thing fun.</p>
<p>Why would you want to explore Firecracker? A friend of mine, <a href="https://iximiuz.com/en/about/">Ivan Velichko</a> is a prolific writer on containers, and Docker. He is one of the biggest independent evangelists for containers and Kubernetes that I know.</p>
<p>So when he wanted to build an <a href="https://labs.iximiuz.com/">online labs and training environment</a>, why did he pick Firecracker instead? Simply put, he told us that containers don't cut it. He needed something that would mirror the type of machine that you'd encounter in production, when you provision an EC2 instance or a GCP VM. Running Docker, Kubernetes, and performing are hard to do securely within a container, and he knew that was important for his students.</p>
<p>For us - we had very similar reasons for picking Firecracker for a secure CI solution. Too often the security issues around running privileged containers, and the slow speed of <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker In Docker's (DIND)</a> Virtual Filesystem Driver (VFS), are ignored. Heads are put into the sand. We couldn't do that and <a href="https://actuated.dev/blog/blazing-fast-ci-with-microvms">developed actuated.dev</a> as a result. Since we launched the pilot, we've now run over 110k VMs for customer CI jobs on GitHub Actions, and have a <a href="https://actuated.dev/blog/secure-microvm-ci-gitlab">tech preview for GitLab CI</a> where a job can be running within 1 second of pushing a "commit".</p>
<p>So let's get that microVM running for you?</p>
<h2>How it works 🔬</h2>
<p>How to build a microVM from a container</p>
<p><img src="/images/2023-09-firecracker-lab/conceptual.png" alt="/images/2023-09-firecracker-lab/conceptual.png"></p>
<blockquote>
<p>Conceptual archicture of the lab</p>
</blockquote>
<p>Here's what we'll be doing:</p>
<ul>
<li>Build a root-filesystem from a container image from the Docker Hub</li>
<li>Download a pre-built Linux Kernel from the Firecracker team</li>
<li>Build an init system written in Go</li>
<li>Build a disk image with the init system and root-filesystem</li>
<li>Configure a networking tap device and IP masquerading</li>
<li>Start a Firecracker process in the background</li>
<li>Configure the Firecracker process via curl statements to a UNIX socket</li>
<li>Finally, issue a boot command and try it out</li>
</ul>
<p>Let's look at why we need a init, instead of just running the entrypoint of a container.</p>
<p>Whilst in theory, you can start a microVM where the first process (PID 1) is your workload, in the same way as Docker, it will leave you with a system which is not properly initialised with things like a /proc/ filesystem, tempfs, hostname, and other things that you'd expect to find in a Linux system.</p>
<p>For that reason, you'll need to either install systemd into the container image you want to use, or build your own basic init system, which sets up the machine, then starts your workload.</p>
<p>We're doing the latter here.</p>
<p>In the below program, you'll see key devices and files mounted, to make a functional system. The hostname is then set by using a syscall, and finally <code>/bin/sh</code> is started. You could also start a specific binary, or build an agent into the init for Remote Procedure Calls (RPC) to start and stop your workload, and to query metrics.</p>
<p>The team at <a href="https://fly.io">Fly.io</a> built their own init and agent combined, and opened-sourced a very early version: <a href="https://github.com/superfly/init-snapshot">github.com/superfly/init-snapshot</a>.</p>
<p>You'll find my init in: <code>./init/main.go</code>:</p>
<pre><code class="hljs language-go"><span class="hljs-comment">// Copyright Alex Ellis 2023</span>

<span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> (
	<span class="hljs-string">"fmt"</span>
	<span class="hljs-string">"log"</span>
	<span class="hljs-string">"os"</span>
	<span class="hljs-string">"os/exec"</span>

	<span class="hljs-string">"syscall"</span>
)

<span class="hljs-keyword">const</span> paths = <span class="hljs-string">"PATH=/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin"</span>

<span class="hljs-comment">// main starts an init process that can prepare an environment and start a shell</span>
<span class="hljs-comment">// after the Kernel has started.</span>
<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
	fmt.Printf(<span class="hljs-string">"Lab init booting\nCopyright Alex Ellis 2022, OpenFaaS Ltd\n"</span>)

	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/proc"</span>, <span class="hljs-string">"proc"</span>, <span class="hljs-number">0</span>)
	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/dev/pts"</span>, <span class="hljs-string">"devpts"</span>, <span class="hljs-number">0</span>)
	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/dev/mqueue"</span>, <span class="hljs-string">"mqueue"</span>, <span class="hljs-number">0</span>)
	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/dev/shm"</span>, <span class="hljs-string">"tmpfs"</span>, <span class="hljs-number">0</span>)
	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/sys"</span>, <span class="hljs-string">"sysfs"</span>, <span class="hljs-number">0</span>)
	mount(<span class="hljs-string">"none"</span>, <span class="hljs-string">"/sys/fs/cgroup"</span>, <span class="hljs-string">"cgroup"</span>, <span class="hljs-number">0</span>)

	setHostname(<span class="hljs-string">"lab-vm"</span>)

	fmt.Printf(<span class="hljs-string">"Lab starting /bin/sh\n"</span>)

	cmd := exec.Command(<span class="hljs-string">"/bin/sh"</span>)

	cmd.Env = <span class="hljs-built_in">append</span>(cmd.Env, paths)
	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr

	err := cmd.Start()
	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
		<span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">"could not start /bin/sh, error: %s"</span>, err))
	}

	err = cmd.Wait()
	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
		<span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">"could not wait for /bin/sh, error: %s"</span>, err))
	}
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">setHostname</span><span class="hljs-params">(hostname <span class="hljs-type">string</span>)</span></span> {
	err := syscall.Sethostname([]<span class="hljs-type">byte</span>(hostname))
	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
		<span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">"cannot set hostname to %s, error: %s"</span>, hostname, err))
	}
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">mount</span><span class="hljs-params">(source, target, filesystemtype <span class="hljs-type">string</span>, flags <span class="hljs-type">uintptr</span>)</span></span> {

	<span class="hljs-keyword">if</span> _, err := os.Stat(target); os.IsNotExist(err) {
		err := os.MkdirAll(target, <span class="hljs-number">0755</span>)
		<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
			<span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">"error creating target folder: %s %s"</span>, target, err))
		}
	}

	err := syscall.Mount(source, target, filesystemtype, flags, <span class="hljs-string">""</span>)
	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
		log.Printf(<span class="hljs-string">"%s"</span>, fmt.Errorf(<span class="hljs-string">"error mounting %s to %s, error: %s"</span>, source, target, err))
	}
}
</code></pre>
<h2>What you'll need</h2>
<p><a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> is a Virtual Machine Monitor (VMM) that leans on Linux's KVM functionality to run VMs. Its beauty is in its simplicity, however even though it doesn't need a lot, you will need KVM to be available. If you have a bare-metal machine, like your own PC, or an old server or laptop, you're all set. There's also plenty of options for bare-metal in the cloud - billed either on a per minute/hour basis or per month.</p>
<p>And finally, for quick testing, <a href="https://m.do.co/c/8d4e75e9886f">DigitalOcean</a>, GCP, and Azure all support what is known as "Nested Virtualization". That's where you obtain a VM, which itself can start further VMs, it's not as fast as bare-metal, but it's cheap and works.</p>
<p>Finally, whilst Firecracker and actuated (our CI product) both support Arm, and Raspberry Pi, this tutorial is only available for `x86_64`` to keep the instructions simple.</p>
<h2>Provision the machine</h2>
<p>I'd recommend you use Ubuntu 22.04, so that you can copy and paste instructions from this tutorial.</p>
<p>Install Docker CE</p>
<pre><code class="hljs language-bash">curl -fsSL https://get.docker.com  | sudo sh
</code></pre>
<p>Docker will be used to fetch an initial Operating System, to build the init system, and to customise the root filesystem.</p>
<p>Install arkade, which gives you an easy way to install Firecracker:</p>
<pre><code class="hljs language-bash">curl -sLS https://get.arkade.dev | sudo sh
</code></pre>
<p>Install Firecracker:</p>
<pre><code class="hljs language-bash">sudo arkade system install firecracker
</code></pre>
<h2>Clone the lab</h2>
<p>Clone the lab onto the machine:</p>
<pre><code class="hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/alexellis/firecracker-init-lab --depth=1

<span class="hljs-built_in">cd</span> firecracker-init-lab
</code></pre>
<h3>Update the networking script</h3>
<p>Find out what the primary interface is on the machine using <code>ip addr</code> or <code>ifconfig</code>.</p>
<p>Edit <code>./setup-networking.sh</code>:</p>
<pre><code>IFNAME=enp8s0
</code></pre>
<p>The script will configure a TAP device which bridges microVMs to your host, then sets up IP forwarding and masquerading so that the microVMs can access the Internet.</p>
<p>Run <code>./setup-networking.sh</code> to setup the TAP device.</p>
<h3>Download the Kernel</h3>
<p>Add <code>make</code> via <code>build-essential</code>:</p>
<pre><code class="hljs language-bash">sudo apt update &#x26;&#x26; sudo apt install -y build-essential
</code></pre>
<p>Run <code>make kernel</code> to download the quickstart Kernel made available by the Firecracker team. Of course, you can build your own, but bear in mind that Firecracker does not have PCI support, so many of the ones you'll find on the Internet will not be appropriate.</p>
<p>This Makefile target will not actually build a new Kernel, but wil download one that the Firecracker team have pre-built and uploaded to S3.</p>
<h3>Make the container image</h3>
<p>Here's the Dockerfile we'll use to build the init system in a multi-stage build, then derive from Alpine Linux for the runtime, this could of course be anything like Ubuntu 22.04, Python, or Node.</p>
<p><code>./Dockerfile</code>:</p>
<pre><code>FROM golang:1.20-alpine as build

WORKDIR /go/src/github.com/alexellis/firecracker-init-lab/init

COPY init .

RUN go build --tags netgo --ldflags '-s -w -extldflags "-lm -lstdc++ -static"' -o init main.go

FROM alpine:3.18 as runtime

RUN apk add --no-cache curl ca-certificates htop

COPY --from=build /go/src/github.com/alexellis/firecracker-init-lab/init/init /init
</code></pre>
<p>I've added in a few extra packages to play with.</p>
<p>Run <code>make root</code>, and you'll see an image in your library:</p>
<pre><code>docker images | grep alexellis2/custom-init

REPOSITORY                                                          TAG                                                                      IMAGE ID       CREATED         SIZE
alexellis2/custom-init                                              latest                                                                   f89aa7f3dd27   20 hours ago    13.7MB
</code></pre>
<h3>Build the disk image</h3>
<p>Firecracker needs a disk image, or an existing block device as its boot drive. You can make this dynamically as required, run <code>make extract</code> to extract the container image into the local filesystem as <code>rootfs.tar</code>.</p>
<p>This step uses <code>docker create</code> followed by <code>docker export</code> to create a temporary container, and then to save its filesystem contents into a tar file.</p>
<p>Run <code>make extract</code></p>
<p>If you want to see what a filesystem looks like, you could extract <code>rootfs.tar</code> into <code>/tmp</code> and have a poke around. This is not a required step.</p>
<p>Then run <code>make image</code>.</p>
<p>Here, a loopback file allocated with 5GB, then formatted as ext4, under the name <code>rootfs.img</code>. The script mounts the drive and then extracts the contents of the <code>rootfs.tar</code> file into it before unmounting the file.</p>
<h3>Start a Firecracker process</h3>
<p>Now, this may feel a little odd or different to Docker users. For each Firecracker VM you want to launch, you'll need to start a process, configure it via curl over a UNIX socket, then issue a boot command.</p>
<p>To run multiple Firecracker microVMs at once, configure a different socket path for each.</p>
<pre><code>make start
</code></pre>
<h3>Boot the microVM</h3>
<p>In another window, issue the boot command:</p>
<pre><code>make boot
</code></pre>
<h3>Explore the system</h3>
<p>You're now booted into a serial console, this isn't a fully functional TTY, so some things won't work like Control + C. The serial console is really just designed for showing boot-up information, not interactive use. For proper remote administration, you should install an OpenSSH server and then connect to the VM using its IP address.</p>
<p>That said, you can now explore a little.</p>
<p>Add a DNS server to <code>/etc/resolv.conf</code>:</p>
<pre><code>echo "nameserver 8.8.8.8" > /etc/resolv.conf
</code></pre>
<p>Then try to reach the Internet:</p>
<pre><code class="hljs language-bash">ping -c 1 8.8.8.8

ping -c 4 google.com

curl --connect-timeout 1 -4 -i http://captive.apple.com/

curl --connect-timeout 1 -4 -i https://inlets.dev
</code></pre>
<p>Check out the system specifications:</p>
<pre><code>free -m
cat /proc/cpuinfo
ip addr
ip route
</code></pre>
<p>When you're done, kill the firecracker process with <code>sudo killall firecracker</code>, or type in <code>halt</code> to the serial console.</p>
<h2>Wrapping up</h2>
<p>I was frustrated by the lack of a simple guide for tinkering with Firecracker, and so that's why I wrote this lab and am keeping it up to date.</p>
<p>For production use, you could use a HTTP client to make the API requests to the UNIX socket, or an SDK, which abstracts away some of the complexity. There's an <a href="https://github.com/firecracker-microvm/firecracker-go-sdk">official SDK for Go</a> and several <a href="https://lib.rs/crates/firec">unofficial ones for Rust</a>. If you look at the sample code for either, you'll see that they are doing the same things we did in the lab, so you should find it relatively easy to convert the lab to use an SDK instead.</p>
<p>Did you enjoy the lab? Have you got a use-case for Firecracker? Let me know on Twitter <a href="https://x.com/alexellisuk">@alexellisuk</a></p>
<p>If you'd like to see how we've applied Firecracker to bring fast and secure CI to teams, check out our product <a href="https://actuated.dev/">actuated.dev</a></p>
<p>Here's a quick demo of our control-plane, scheduler and bare-metal agent in action:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w?si=xAt7YG4YCJ_ZleCA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to develop a great CLI with Go]]></title>
            <link>https://actuated.dev/blog/develop-a-great-go-cli</link>
            <guid>https://actuated.dev/blog/develop-a-great-go-cli</guid>
            <pubDate>Tue, 22 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Alex shares his insights from building half a dozen popular Go CLIs. Which can you apply to your projects?]]></description>
            <content:encoded><![CDATA[<p>Is your project's CLI growing with you? I'll cover some of the lessons learned writing the OpenFaaS, actuated, actions-usage, arkade and k3sup CLIs, going as far back as 2016. I hope you'll find some ideas or inspiration for your own projects - either to start them off, or to improve them as you go along.</p>
<blockquote>
<p>Just starting your journey, or want to go deeper?</p>
<p>You can master the fundamentals of Go (also called Golang) with my <a href="https://store.openfaas.com/l/everyday-golang">eBook Everyday Golang</a>, which includes chapters on Go routines, HTTP clients and servers, text templates, unit testing and crafting a CLI. If you're on a budget, I would recommend checkout out the official <a href="https://go.dev/tour/">Go tour</a>, too.</p>
</blockquote>
<h2>Introduction</h2>
<p>The earliest CLI I wrote was for OpenFaaS, called <a href="https://github.com/openfaas/faas-cli">faas-cli</a>. It's a client for a REST API exposed over HTTP, and I remember how it felt to add the first command <em>list functions</em>, then one more, and one more, until it was a fully working CLI with a dozen commands.</p>
<p>But it started with one command - something that was useful to us at the time, that was to list the available functions.</p>
<p>The initial version used Go's built-in flags parser, which is rudimentary, but perfectly functional.</p>
<pre><code>faas-cli -list
faas-cli -describe
faas-cli -deploy
</code></pre>
<p>Over time, you may outgrow this simple approach, and drift towards wanting sub-commands, each with their own set of options.</p>
<p>An early contributor <a href="https://github.com/johnmccabe">John McCabe</a> introduced me to <a href="https://github.com/spf13/cobra">Cobra</a> and asked if he could convert everything over.</p>
<pre><code>faas-cli list
faas-cli describe
faas-cli deploy
</code></pre>
<p>Now each sub-command can have its set of flags, and even sub-commands in the case of <code>faas-cli secret list/create/delete</code></p>
<p><a href="https://github.com/self-actuated/actions-usage">actions-usage</a> is a free analytics tool we wrote for GitHub Actions users to iterate GitHub's API and summarise your usage over a certain period of time. It's also written in Go, but because it's mostly single-purpose, it'll probably never need sub-commands.</p>
<pre><code>actions-usage -days 28 \
    -token-file ~/pat.txt \
    -org openfaasltd
</code></pre>
<p>Shortly after launching the tool for teams an open-source organisations, we had a feature request to run it on individual user accounts.</p>
<p>That meant switching up some API calls and adding new CLI flags:</p>
<pre><code>actions-usage -days 7 \
    -token-file ~/pat.txt \
    -user alexellis
</code></pre>
<p>We then got a bit clever and started adding some extra reports and details, you can see what it looks in the article <a href="https://actuated.dev/blog/github-actions-usage-cli">Understand your usage of GitHub Actions</a></p>
<h2>What's new for actuated-cli</h2>
<p>I'm very much a believer in a Minimal Viable Product (MVP). If you can create some value or utility to users, you should ship it as early as possible, especially if you have a good feedback loop with them.</p>
<p>A quick note about the <a href="https://github.com/self-actuated/actuated-cli">actuated-cli</a>, it's main use-cases are to:</p>
<ul>
<li>List the runners for an organisation</li>
<li>List the queued or in-progress jobs for an organisation</li>
<li>Update a remote server, or get the logs from a VM or the agent service</li>
</ul>
<h3>No more owner flags</h3>
<p>The <code>actuated-cli</code> was designed to work on a certain organisation, but it meant extra typing, so wherever possible, we've removed the flag completely.</p>
<pre><code>actuated-cli runners --owner openfaasltd
</code></pre>
<p>becomes:</p>
<pre><code>actuated-cli runners
</code></pre>
<p>How did we do this? We determine the intersection of organisations for which your account is authorized, and which are enrolled for actuated. It's much less typing and it's more intuitive.</p>
<h3>The host flag became a positional argument</h3>
<p>This was another exercise in reducing typing. Let's say we wanted to upgrade the agent for a certain host, we'd have to type:</p>
<pre><code>actuated-cli upgrade --owner openfaasltd --host server-1
</code></pre>
<p>By looking at the "args" slice, instead of for a specific command, we can assume that any text after the flags is always the server name:</p>
<pre><code>actuated-cli upgrade --owner openfaasltd server-1
</code></pre>
<h3>Token management</h3>
<p>The actuated CLI uses a GitHub personal access token to authenticate with the API. This is a common pattern, but it's not always clear how to manage the token.</p>
<p>We took inspiration from the <code>gh</code> CLI, which is a wrapper around the GitHub API.</p>
<p>The <code>gh</code> CLI has a <code>gh auth</code> command which can be used to obtain a token, and save it to a local file, then any future usage of the CLI will use that token.</p>
<p>Before, you had to create a Personal Access Token in the GitHub UI, then copy and paste it into a file, and decide where to put it, and what to name it. What's more, if you missed a permission, then the token wouldn't work.</p>
<pre><code>actuated-cli --token ~/pat.txt
</code></pre>
<p>Now, you simply run:</p>
<pre><code>actuated-cli auth
</code></pre>
<p>And as you saw from the previous commands, there's no longer any need for the <code>--token</code> flag. Unless of course, you want to supply it, then you can.</p>
<p>A good way to have a default for a flag, and then an override, is to use the Cobra package's <code>Changed()</code> function. Read the default, unless <code>.Changed()</code> on the <code>--token</code> or <code>--token-value</code> flags return <code>true</code>.</p>
<h3>The <code>--json</code> flag</h3>
<p>From early on, I knew that I would want to be able to pipe output into .jq, or perhaps even do some scripting. I've seen this in <code>docker</code>, <code>kubectl</code> and numerous other CLI tools written in Go.</p>
<pre><code>actuated-cli runners --json | jq '.[] | .name'

"m1m1"
"m1m2"
"of-epyc-lon1"
</code></pre>
<p>The JSON format also allows you to get access to certain fields which the API call returns, which may not be printed by the default command's text-based formatter:</p>
<pre><code>|         NAME         |  CUSTOMER   |   STATUS    | VMS  | PING  |   UP    | CPUS |   RAM   | FREE RAM | ARCH  |                 VERSION                  |
|----------------------|-------------|-------------|------|-------|---------|------|---------|----------|-------|------------------------------------------|
| of-epyc-lon1         | openfaasltd | running     | 0/5  | 7ms   | 6 days  |   48 | 65.42GB | 62.85GB  | amd64 | 5f702001a952e496a9873d2e37643bdf4a91c229 |
</code></pre>
<p>Instead, we get:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">[</span>  <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"of-epyc-lon1"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"customer"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"openfaasltd"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"pingNano"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">30994998</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"uptimeNano"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">579599000000000</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"cpus"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">48</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"memory"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">65423184000</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"memoryAvailable"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">62852432000</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"vms"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"maxVms"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"reachable"</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"status"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"running"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"agentSHA"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"5f702001a952e496a9873d2e37643bdf4a91c229"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"arch"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"amd64"</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">]</span>
</code></pre>
<h3>SSH commands and doing the right thing</h3>
<p>Actuated has a <a href="https://docs.actuated.dev/tasks/debug-ssh/">built-in SSH gateway</a>, this means that any job can be debugged - whether running on a hosted or self-hosted runner, just by editing the workflow YAML.</p>
<p>Add the following to the <code>- steps:</code> section, and the <code>id_token: write</code> permission, and your workflow will pause, and then you can connect over SSH using the CLI or the UI.</p>
<pre><code class="hljs language-yaml">    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/connect-ssh@master</span>
</code></pre>
<p>There are two sub-commands:</p>
<ul>
<li><code>actuated-cli ssh list</code> - list the available SSH sessions</li>
<li><code>actuated-cli ssh connect</code> - connect to an available session</li>
</ul>
<p>Here's an example of having only one connection:</p>
<pre><code>actuated-cli ssh list
| NO  |   ACTOR   |   HOSTNAME    | RX | TX | CONNECTED |
|-----|-----------|---------------|----|----|-----------|
|   1 | alexellis | fv-az1125-168 |  0 |  0 | 32s       |
</code></pre>
<p>Now how do you think the <code>ssh connect</code> command should work?</p>
<p>Here's the most obvious way:</p>
<pre><code>actuated-cli ssh connect --hostname fv-az1125-168
</code></pre>
<p>This is a little obtuse, since we only have one server to connect to, we can improve it for the user, with:</p>
<pre><code>actuated-cli ssh connect
</code></pre>
<p>That's right, we do the right thing, the obvious thing.</p>
<p>Then when there is more than one connection, instead of adding two flags <code>--no</code> or <code>--hostname</code>, we can simply take the positional argument:</p>
<pre><code>actuated-cli ssh connect 1
actuated-cli ssh connect fv-az1125-168
</code></pre>
<p>Are there any places where you could simplify your own CLI?</p>
<p>Read the source code here: <a href="https://github.com/self-actuated/actuated-cli/blob/master/cmd/ssh_connect.go">ssh_connect.go</a></p>
<h3>The <code>--verbose</code> flag</h3>
<p>We haven't made any use of the <code>--verbose</code> flag yet in the CLI, but it's a common pattern which has been used in <code>faas-cli</code> and various others. Once your output gets to a certain width, it can be hard to view in a terminal, like the output from the previous command.</p>
<p>To implement <code>--verbose</code>, you should reduce the columns to the absolute minimum to be useful, so maybe we could give up the Version, customer, ping, and CPUs columns in the standard view, then add them back in with <code>--verbose</code>.</p>
<h3>Table printing</h3>
<p>As you can see from the output of the commands above, we make heavy usage of a table printer.</p>
<p>You don't necessarily need a 3rd-party table printer, Go has a fairly good "tab writer" which can create nicely formatted code:</p>
<pre><code>faas-cli list -g https://openfaas.example.com
Function                        Invocations     Replicas
bcrypt                          9               1    
figlet                          0               1    
inception                       0               1    
nodeinfo                        2               1    
ping-url                        0               1  
</code></pre>
<p>You can find the standard <a href="https://pkg.go.dev/text/tabwriter">tabwriter package</a> here.</p>
<p>Or try out the <a href="https://github.com/olekukonko/tablewriter">tablewriter</a> package by Olekukonko. We've been able to make use of it in <a href="https://arkade.dev">arkade</a> too - a free marketplace for developer tools.</p>
<p>See usage in arkade here: <a href="https://github.com/alexellis/arkade/blob/master/pkg/get/table.go#L19">table.go</a></p>
<p>See usage in actuated-cli's SSH command here: <a href="https://github.com/self-actuated/actuated-cli/blob/master/cmd/ssh_ls.go">ssh_ls.go</a></p>
<h2>Progress bars</h2>
<p>One thing that has been great about having open-source CLIs, is that other people make suggestions and help you learn about new patterns.</p>
<p>For arkade, <a href="https://twitter.com/rberrelleza">Ramiro from Okteto</a> sent a PR to add a progress bar to show how long remained to download a big binary like the Kubernetes CLI.</p>
<pre><code class="hljs language-bash">arkade get kubectl
Downloading: kubectl
Downloading: https://storage.googleapis.com/kubernetes-release/release/v1.24.2/bin/linux/amd64/kubectl

15.28 MiB / 43.59 MiB [------------------------>____________________________________] 35.05%
</code></pre>
<p>It's simple, but gives enough feedback to stop you from thinking the program is stuck. In my Human Design Interaction course at university, I learned that anything over 7s triggers uncertainty in an end-user.</p>
<p>See how it's implemented: <a href="https://github.com/alexellis/arkade/blob/master/pkg/get/download.go#L191">download.go</a></p>
<h2>HTTP and REST are not the only option</h2>
<p>When I wrote <a href="https://github.com/alexellis/k3sup">K3sup</a>, a tool to install K3s on remote servers, I turned to SSH to automate the process. So rather than making HTTP calls, a Go library for SSH is used to open a connection and run remote commands.</p>
<p>It also simplifies an annoying post-installation task - managing the kubeconfig file. By default this is a protected file on the initial server you set up, k3sup will download the file and merge it with your local kubeconfig.</p>
<pre><code>k3sup install \
    --host HOST1 \
    --user ubuntu \
    --merge \
    --local-path ~/.kube/config
</code></pre>
<p>I'd recommend trying out <a href="https://pkg.go.dev/golang.org/x/crypto/ssh">golang.org/x/crypto/ssh</a> in your own CLIs and tools. It's great for automation, and really simple to use.</p>
<h2>Document everything as best as you can</h2>
<p>Here's an example of a command with good documentation:</p>
<pre><code>Schedule additional VMs to repair the build queue.
Use sparingly, check the build queue to see if there is a need for 
more VMs to be launched. Then, allow ample time for the new VMs to 
pick up a job by checking the build queue again for an in_progress
status.

Usage:
  actuated-cli repair [flags]

Examples:
  ## Launch VMs for queued jobs in a given organisation
  actuated repair OWNER

  ## Launch VMs for queued jobs in a given organisation for a customer
  actuated repair --staff OWNER


Flags:
  -h, --help    help for repair
  -s, --staff   List staff repair

Global Flags:
  -t, --token string         File to read for Personal Access Token (default "$HOME/.actuated/PAT")
      --token-value string   Personal Access Token
</code></pre>
<p>Not only does it show example usage, so users can understand <em>what can be done</em>, but it has a detailed explanation of when to use the command.</p>
<pre><code class="hljs language-golang">	cmd := &#x26;cobra.Command{
		Use:   <span class="hljs-string">"repair"</span>,
		Short: <span class="hljs-string">"Schedule additional VMs to repair the build queue"</span>,
		Long: <span class="hljs-string">`Schedule additional VMs to repair the build queue.
Use sparingly, check the build queue to see if there is a need for 
more VMs to be launched. Then, allow ample time for the new VMs to 
pick up a job by checking the build queue again for an in_progress
status.`</span>,
		Example: <span class="hljs-string">`  ## Launch VMs for queued jobs in a given organisation
  actuated repair OWNER

  ## Launch VMs for queued jobs in a given organisation for a customer
  actuated repair --staff OWNER
`</span>
    }
</code></pre>
<p>Browse the source code: <a href="https://github.com/self-actuated/actuated-cli/blob/master/cmd/repair.go">repair.go</a></p>
<h2>Wrapping up</h2>
<p>I covered just a few of the recent changes - some were driven by end-user feedback, others were open source contributions, and in some cases, we just wanted to make the CLI easier to use. I've been writing CLIs for a long time, and I still have a lot to learn.</p>
<p>What CLIs do you maintain? Could you apply any of the above to them?</p>
<p>Do you want to learn how to master the fundamentals of Go? Check out my eBook: <a href="https://store.openfaas.com/l/everyday-golang">Everyday Go</a>.</p>
<p>If you're on a budget, I would recommend checkout out the official <a href="https://go.dev/tour/">Go tour</a>, too. It'll help you understand some of the basics of the language and is a good primer for the e-book.</p>
<p>Read the source code of the CLIs we mentioned:</p>
<ul>
<li><a href="https://github.com/self-actuated/actions-usage">actions-usage</a> - free analytics tool for GitHub Actions</li>
<li><a href="https://github.com/self-actuated/actuated-cli">actuated-cli</a> - CLI client for actuated customers</li>
<li><a href="https://github.com/openfaas/faas-cli">faas-cli</a> - CLI client for OpenFaaS</li>
<li><a href="https://github.com/alexellis/k3sup">k3sup</a> - Install K3s over SSH</li>
<li><a href="https://github.com/alexellis/arkade">arkade</a> - Download CLI tools from GitHub releases</li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Calyptia fixed its Arm builds whilst saving money]]></title>
            <link>https://actuated.dev/blog/calyptia-case-study-arm</link>
            <guid>https://actuated.dev/blog/calyptia-case-study-arm</guid>
            <pubDate>Fri, 11 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how Calyptia fixed its failing Arm builds for open-source Fluent Bit and accelerated our commercial development by adopting Actuated and bare-metal runners.]]></description>
            <content:encoded><![CDATA[<p>This is a case-study, and guest article by <a href="https://www.linkedin.com/in/patrickjkstephens/">Patrick Stephens, Tech Lead of Infrastructure at Calyptia</a>.</p>
<h2>Introduction</h2>
<p>Different architecture builds can be slow using the Github Actions hosted runners due to emulation of the non-native architecture for the build. This blog shows a simple way to make use of self-hosted runners for dedicated builds but in a secure and easy to maintain fashion.</p>
<p><a href="https://calyptia.io">Calyptia</a> maintains the OSS and <a href="https://cncf.io">Cloud Native Computing Foundation (CNCF)</a> graduated Fluent projects including <a href="https://docs.fluentbit.io">Fluent Bit</a>. We then add value to the open-source core by providing commercial services and enterprise-level features.</p>
<blockquote>
<p>Fluent Bit is a Fast and Lightweight Telemetry Agent for Logs, Metrics, and Traces for Linux, macOS, Windows, and BSD family operating systems. It has been made with a strong focus on performance to allow the collection and processing of telemetry data from different sources without complexity.</p>
<p>It was originally created by <a href="https://twitter.com/edsiper">Eduardo Silva</a> and is now an independent project.</p>
</blockquote>
<p>To learn about Fluent Bit, the Open Source telemetry agent that Calyptia maintains, <a href="https://docs.fluentbit.io/manual/about/what-is-fluent-bit">check out their docs</a>.</p>
<h3>The Problem</h3>
<p>One of the best things about Fluent Bit is that we provide native packages (RPMs and DEBs) for a myriad of supported targets (various Linux, macOS and Windows), however to do this is also one of the hardest things to support due to the complexity of building and testing across all these targets.</p>
<p>When PRs are provided we would like to ensure they function across the targets but doing so can take a very long time (hours) and consume a lot of resources (that must be paid for). This means that these long running jobs are only done via exception (manually labelling a PR or on full builds for releases) leading to issues only discovered when a full build &#x26; test is done, e.g. during the release process so blocking the release until it is fixed.</p>
<p>The long build time problem came to a head when we discovered we could no longer build for Amazon Linux 2023 (AL2023) because the build time exceeded the 6 hour limit for a single job on Github. We had to disable the AL2023 target for releases which means users cannot then update to the latest release leading to missing features or security problems: <a href="https://github.com/fluent/fluent-bit/issues/6978">See the issue here</a></p>
<p>In addition to challenges in the OSS, there are also challenges on the commercial side. Here, we are seeing issues with extended build times for ARM64 targets because our CI is based on Github Actions and currently only AMD64 (also called x86-64 or x64) runners are provided for builds. This slows down development and can mean bugs are not caught as early as possible.</p>
<h3>Why not use self-hosted runners?</h3>
<p>One way to speed up builds is to provide self-hosted ARM64 runners.</p>
<p>Unfortunately, runners pose security implications, particularly for public repositories. In fact, Github recommends against using self-hosted runners: About self-hosted runners - GitHub Docs</p>
<p>In addition to security concerns, there are also infrastructure implications for using self-hosted runners. We have to provide the infrastructure around deploying and managing the self-hosted runners, installing an agent, configuring it for jobs, etc. From a perspective of OSS we want anything we do to be simple and easy for maintenance purposes.</p>
<p>Any change we make needs to be compatible with downstream forks as well. We do not want to break builds for existing users, particularly for those who are contributors as well to the open source project. Therefore we need a solution that does not impact them.</p>
<p>There are various tools that can help with managing self-hosted runners, <a href="https://jonico.github.io/awesome-runners/">https://jonico.github.io/awesome-runners/</a> provides a good curated list. I performed an evaluation of some of the recommended tools but the solution would be non-trivial and require some effort to maintain.</p>
<h3>Our considerations</h3>
<p>We have the following high level goals in a rough priority order:</p>
<ul>
<li>Speed up the build.</li>
<li>Keep costs minimal.</li>
<li>Keep the process as secure as possible.</li>
<li>Make it simple to deploy and manage.</li>
<li>Minimise impact to OSS forks and users.</li>
</ul>
<h3>The solution</h3>
<p>At Kubecon EU 2023 I met up with Alex Ellis from Actuated (and of <a href="https://openfaas.com/">OpenFaaS</a> fame) in-person and we wanted to put Alex and his technology to the test, to see if the Actuated technology could fix the problems we see with our build process.</p>
<p>To understand what Actuated is then it is best to refer to their documentation with this specific blog post being a good overview of why we considered adopting it. We're not the only CNCF project that Alex's team was able to help. He describes how he helped Parca and Network Service Mesh to slash their build teams by using native Arm hardware.</p>
<p>A quick TLDR; though would be that Actuated provides an agent you install which then automatically creates ephemeral VMs on the host for each build job. Actuated seemed to tick the various boxes (see the considerations above) we had for it but never trust a vendor until you’ve tried it yourself!</p>
<p>Quote from Alex:</p>
<blockquote>
<p>"Actuated aims to give teams the closest possible experience to managed runners, but with native arm support flat rate billing, and secure VM-level isolation. Since Calyptia adopted actuated, we’ve also shipped an SSH debug experience (like you’d find with CircleCI) and detailed reports and insights on usage across repos, users and organisations."</p>
</blockquote>
<p>To use Actuated, you have to provision a machine with the Actuated agent, which is trivial and well documented: <a href="https://docs.actuated.dev/install-agent/">https://docs.actuated.dev/install-agent/</a>.</p>
<p>We deployed an Ampere Altra Q80 server with 256GB of RAM and 80 cores ARM64 machine via Equinix (Equinix donates resources to the CNCF which we use for Fluent Bit so this satisfies the cost side of things) and installed the Actuated agent on it per the Actuated docs.</p>
<p>The update required to start using Actuated in OSS Fluent Bit is a one-liner. (Thanks in part to my excellent work refactoring the CI workflows, or so I like to think. You can see the actual PR here for the change: <a href="https://github.com/fluent/fluent-bit/pull/7527">https://github.com/fluent/fluent-bit/pull/7527</a>.)</p>
<p>The following is the code required to start using Actuated:</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: ${{ (contains(matrix.distro, 'arm' ) &#x26; 'actuated-arm64') || 'ubuntu-latest' }}</span>
</code></pre>
<p>For most people, the change will be much simpler:</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: actuated</span>
</code></pre>
<p>In Github Actions parlance, the code above translates to “if we are doing an ARM build, then use the Actuated runner; otherwise, use the default Github Hosted (AMD64) Ubuntu runner”.</p>
<p>In the real code, I added an extra check so that we only use Actuated runners for the official source repo which means any forks will also carry on running as before on the Github Hosted runner.</p>
<p>With this very simple change, all the ARM64 builds that used to take hours to complete now finish in minutes. In addition, we can actually build the AL2023 ARM64 target to satisfy those users too. <strong>A simple change gave us a massive boost to performance and also provided a missing target.</strong></p>
<p>To demonstrate this is not specific to Equinix hosts or in some fashion difficult to manage in heterogeneous infrastructure (e.g. various hosts/VMs from different providers), we also replicated this for all our commercial offerings using a bare-metal Hetzner host. The process was identical: install the agent and make the runs-on code change as above to use Actuated. Massive improvements in build time were seen again as expected.</p>
<p>The usage of bare-metal (or cloud) hosts providers is invisible and only a choice of which provider you want to put the agent on. In our case we have a mixed set up with no difference in usage or maintenance.</p>
<h3>Challenges building containers</h3>
<p>The native package (RPM/DEB) building described above was quite simple to integrate via the existing workflows we had.</p>
<p>Building the native packages is done via a process that runs a target-specific container for each of the builds, e.g. we run a CentOS container to build for that target. This allows a complete build to be run on any Linux-compatible machine with a container runtime either in CI or locally. For ARM builds, we were using QEMU emulation for ARM builds hence the slowdown as this has to emulate instructions between architectures.</p>
<p>Container builds are the primary commercial area for improvement as we provide a SAAS solution running on K8S. Container builds were also a trickier proposition for OSS as we were using a single job to build all architectures using the docker/build-push-action. The builds were incredibly slow for ARM and also atomic, which means if you received a transient issue in one of the architecture builds, you would have to repeat the whole lot.</p>
<p>As an example: <a href="https://github.com/fluent/fluent-bit/blob/master/.github/workflows/call-build-images.yaml">https://github.com/fluent/fluent-bit/blob/master/.github/workflows/call-build-images.yaml</a></p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">the</span> <span class="hljs-string">production</span> <span class="hljs-string">images</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">build_push</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./dockerfiles/Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">${{</span> <span class="hljs-string">steps.meta.outputs.tags</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">labels:</span> <span class="hljs-string">${{</span> <span class="hljs-string">steps.meta.outputs.labels</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/amd64,</span> <span class="hljs-string">linux/arm64,</span> <span class="hljs-string">linux/arm/v7</span>
          <span class="hljs-attr">target:</span> <span class="hljs-string">production</span>
          <span class="hljs-comment"># Must be disabled to provide legacy format images from the registry</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">push:</span> <span class="hljs-literal">true</span>
          <span class="hljs-attr">load:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            FLB_NIGHTLY_BUILD=${{ inputs.unstable }}
            RELEASE_VERSION=${{ inputs.version }}
</span></code></pre>
<p>The build step above is a bit more complex to tease out into separate components: we need to run single architecture builds for each target then provide a multi-arch manifest that links them together.</p>
<p>We reached out to Alex on a good way to modify this to work within a split build per architecture approach. The Actuated team has been very responsive on these types of questions along with proactive monitoring of our build queue and runners.</p>
<p>Within Calyptia we have followed the approach Docker provided here and suggested by the Actuated team: <a href="https://docs.docker.com/build/ci/github-actions/multi-platform/#distribute-build-across-multiple-runners">https://docs.docker.com/build/ci/github-actions/multi-platform/#distribute-build-across-multiple-runners</a></p>
<p>Based on what we learned, we recommend the following process is followed:</p>
<p>Build each architecture and push by digest in a set of parallel matrix jobs.
Capture the output digest of each build.
Create the multi-arch manifest made up of each digest we have pushed in step 1 using the artefact from step 2.</p>
<p>This approach provides two key benefits. First, it allows us to run on dedicated runners per-arch. Second, if a job fails we only need to repeat the single job, instead of having to rebuild all architectures.</p>
<p>The new approach reduced the time for the release process for the Calyptia Core K8S Operator from more than an hour to minutes. Additionally, because we can do this so quickly, we now build all architectures for every change rather than just on release. This helps developers who are running ARM locally for development as they have containers always available.</p>
<p>The example time speed up for the Calyptia Core K8S operator process was replicated across all the other components. A very good bang for your buck!</p>
<p>For us, the actuated subscription fee has been of great value. Initially we tested the waters on the Basic Plan, but soon upgraded when we saw more areas where we could use it. The cost for us has been offset against a massive improvement in CI time and development time plus reducing the infrastructure costs of managing the self-hosted runners.</p>
<h2>Lessons learned</h2>
<p>The package updates were seamless really, however we did encounter some issues with the ecosystem (not with actuated), when refactoring and updating our container builds. The issues with the container builds are covered below to help anyone else with the same problems.</p>
<h3>Provenance is now enabled by default</h3>
<p>We were using v3 of Docker’s docker/build-push-action, but they made a breaking change which caused us a headache. They changed the default in v4 to create the various extra artifacts for provenance (e.g. SBOMs) which did have a few extra side effects both at the time and even now.</p>
<p>If you do not disable this then it will push manifest lists rather than images so you will subsequently get an error message when you try to create a manifest list of another manifest list.</p>
<p>Separately this also causes issues for older docker clients or organisations that need the legacy Docker schema format from a registry: using it means only OCI format schemas are pushed. This impacted both OSS and our commercial offerings: <a href="https://github.com/fluent/fluent-bit/issues/7748">https://github.com/fluent/fluent-bit/issues/7748</a>.</p>
<p>It meant people on older OS’s or with requirements on only consuming Docker schema (e.g. maybe an internal mirror registry only supports that) could not pull the images.</p>
<h3>Invalid timestamps for gcr.io with manifests</h3>
<p>A funny problem found with our cloud-run deployments for Calyptia Core SAAS offering was that pushing the manifests to (Google Container Registry) gcr.io meant they ended up with a zero-epoch timestamp. This messed up some internal automation for us when we tried to get the latest version.</p>
<p>To resolve this we just switched back to doing a single architecture build as we do not need multi-arch manifests for cloud-run. Internally we still have multi-arch images in ghcr.io for internal use anyway, this is purely the promotion to gcr.io.</p>
<h3>Manifests cannot use sub-paths</h3>
<p>This was a fun one: when specifying images to make up your manifest they must be in the same registry of course!</p>
<p>Now, we tend to use sub-paths a lot to handle specific use cases for ghcr.io but unfortunately you cannot use them when trying to construct a manifest.</p>
<p>OK: ghcr.io/calyptia/internal/product:tag --> ghcr.io/calyptia/internal/product:tag-amd64
NOK: ghcr.io/calyptia/internal/product:tag --> ghcr.io/calyptia/internal/amd64/product:tag</p>
<p>As with all good failures, the tooling let me make a broken manifest at build time but unfortunately trying to pull it meant a failure at runtime.</p>
<h3>Actuated container registry mirror</h3>
<p>All Github hosted runners provide default credentials to authenticate with docker.io for pulling public images. When running on a self-hosted runner you need to authenticate for this otherwise you will hit rate limits and builds may fail as they cannot download required base images.</p>
<p>Actuated provide a registry mirror and Github Action to simplify this so make sure you set it up: <a href="https://docs.actuated.dev/tasks/registry-mirror/">https://docs.actuated.dev/tasks/registry-mirror/</a></p>
<p>As part of this, ensure it is set up for anything that uses images (e.g. we run integration tests on KIND that failed as the cluster could not download its images) and that it is done after any buildx config as it creates a dedicated buildx builder for the mirror usage.</p>
<h3>Actuated support</h3>
<p>The Actuated team helped us in two ways: the first was that we were able to enable Arm builds for our OSS projects and our commercial products, when they timed out with hosted runners. The second way was where our costs were getting out of hand on GitHub’s larger hosted runners: Actuated not only reduced the build time, but the billing model is flat-rate, meaning our costs are now fixed, rather than growing.</p>
<p>As we made suggestions or collaborated with the Actuated team, they updated the documentation, including our suggestions on smoothing out the onboarding of new build servers and new features for the CLI.</p>
<p>The more improvements we’ve made, the more we’ve seen. Next on our list is getting the runtime of a Go release down from 26 minutes by bringing it over to actuated.</p>
<h2>Conclusion</h2>
<p>Alex Ellis: We've learned a lot working with Patrick and Calyptia and are pleased to see that they were able to save money, whilst getting much quicker, and safer Open Source and commercial builds.</p>
<p>We value getting feedback and suggestions from customers, and Patrick continues to provide plenty of them.</p>
<p>If you'd like to learn more about actuated, reach out to speak to our team by clicking "Sign-up" and filling out the form. We'll be in touch to arrange a call.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Update your AMD hosts now to mitigate the Zenbleed exploit]]></title>
            <link>https://actuated.dev/blog/amd-zenbleed-update-now</link>
            <guid>https://actuated.dev/blog/amd-zenbleed-update-now</guid>
            <pubDate>Mon, 31 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to update the microcode on your AMD CPU to avoid the Zenbleed exploit.]]></description>
            <content:encoded><![CDATA[<p>On 24th July 2023, <a href="https://www.theregister.com/2023/07/24/amd_zenbleed_bug/">The Register covered a new exploit</a> for certain AMD CPUs based upon the Zen architecture. The exploit, dubbed Zenbleed, allows an attacker to read arbitrary physical memory locations on the host system. It works by allowing memory to be read after it's been set to be freed up aka "use-after-free". This is a serious vulnerability, and you should update your AMD hosts as soon as possible.</p>
<p>The Register made the claim that "any level of emulation such as QEMU" would prevent the exploit from working. This is misleading because QEMU only makes sense in production when used with hardware acceleration (KVM). We were able to run the exploit with a GitHub Action using actuated on an AMD Epyc server from Equinix Metal using Firecracker and KVM.</p>
<blockquote>
<p>"If you stick any emulation layer in between, such as Qemu, then the exploit understandably fails."</p>
</blockquote>
<p>The editors at The Register have since reached out and updated their article.</p>
<p>Even Firecracker with its isolated guest Kernel is vulnerable, which shows how serious the bug is, it's within the hardware itself. Of course it goes without saying that this also affects containerd, Docker (and by virtue Kubernetes) which share the host Kernel.</p>
<p>To test this, we ran a GitHub Actions matrix build that creates many VMs running different versions of K3s. About the same time, we triggered a build which runs a Zenbleed exploit PoC written by <a href="https://twitter.com/taviso">Tavis Ormandy</a>, a security researcher at Google.</p>
<p>We found that the exploit was able to read the memory of the host system, and that the exploit was able to read the memory of other VMs running on the same host.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">build</span>

<span class="hljs-attr">on:</span>
    <span class="hljs-attr">pull_request:</span>
        <span class="hljs-attr">branches:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">'*'</span>
    <span class="hljs-attr">push:</span>
        <span class="hljs-attr">branches:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">main</span>
    <span class="hljs-attr">workflow_dispatch:</span>

<span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">build:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">specs</span>
        <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Download</span> <span class="hljs-string">exploit</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            curl -L -S -s -O https://lock.cmpxchg8b.com/files/zenbleed-v5.tar.gz
            tar -xvf zenbleed-v5.tar.gz
            sudo apt install -qy build-essential make nasm
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">exploit</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            cd zenbleed
            make
            chmod +x ./zenbleed
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">exploit</span> <span class="hljs-string">for</span> <span class="hljs-number">1000 </span><span class="hljs-string">pieces</span> <span class="hljs-string">of</span> <span class="hljs-string">data</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            ./zenbleed/zenbleed -m 1000
</span></code></pre>
<p>Full details of the exploit can be found on a microsite created by the security researcher who discovered the vulnerability. The <code>-m 1000</code> flag reads 1000 pieces of memory and then exits.</p>
<p><a href="https://lock.cmpxchg8b.com/zenbleed.html#vulnerability">Zenbleed by Tavis Ormandy</a></p>
<p>We didn't see any secrets printed out during the scan, but we did see part of a public SSH key, console output from etcd running within K3s, and instructions from containerd. So we can assume anything that was within memory within one of the other VMs on the host, or even the host itself, could be read by the exploit.</p>
<p><img src="/images/2023-07-zenbleed/gha.png" alt="GHA output from the zenbleed exploit"></p>
<blockquote>
<p>GHA output from the zenbleed exploit</p>
</blockquote>
<h2>Mitigation</h2>
<p>AMD has already released a mitigation for the Zenbleed exploit, which requires an update to the CPU's microcode.</p>
<p><a href="https://www.linkedin.com/in/edwardvielmetti/">Ed Vielmetti</a>, Developer Partner Manager at Equinix told us that mitigation is three-fold:</p>
<ol>
<li>There is an incoming BIOS update from certain vendors that will update the microcode.</li>
<li>Updating some OSes like the Ubuntu 20.04 and 22.04 will upgrade the microcode of the CPU.</li>
<li>There is a <a href="https://www.reddit.com/r/linux/comments/15903hr/zenbleed_a_useafterfree_in_amd_zen2_processors/">"chicken bit"</a> that can be enabled which prevents the exploit from working.</li>
</ol>
<p>I probably don't need to spell this out, but a system update looks like the following, and the reboot is required:</p>
<pre><code class="hljs language-bash">sudo apt update -qy &#x26;&#x26; \
    sudo apt upgrade -yq &#x26;&#x26; \
    sudo reboot 
</code></pre>
<p><img src="/images/2023-07-zenbleed/microcode.png" alt="An update to the CPU microcode is required"></p>
<p>For some unknown reason, both of the Equinix AMD hosts that we use internally broke after running the OS upgrade, so I had to reinstall Ubuntu 22.04 using the dashboard. If for whatever reason the machine won't come up after the microcode update, then you should reinstall the Operating System (OS) using your vendor's rescue system or out of band console, both Equinix Metal and Hetzner have an "easy button" that you can click for this. If there is still an issue after that, reach out to your vendor's support team.</p>
<p>New machines provisioned after this date should already contain the microcode fix or have the "chicken bit" enabled. We provisioned a new AMD Epyc server on Equinix Metal to make sure, and as expected, thanks to their hard work - it was not vulnerable.</p>
<p>We offer 500 USD of free credit for new Equinix Metal customers to use with actuated, and Equinix Metal have also written up their own guide on workaround here:</p>
<ul>
<li><a href="https://deploy.equinix.com/blog/an-update-on-the-amd-zen-2-cpu-vulnerability/">An Update On the AMD Zen 2 CPU Vulnerability</a></li>
</ul>
<h2>Verifying the mitigation</h2>
<p>Since actuated VMs use Firecracker, you should run the above workflow before and after to verify the exploit was a) present and b) mitigated.</p>
<p><img src="/images/2023-07-zenbleed/mitigated.png" alt="What it looks like when the mitigation is in place"></p>
<blockquote>
<p>Above: What it looks like when the mitigation is in place</p>
</blockquote>
<p>You can also run the exploit on the host by copying and pasting the commands from the GitHub Action above.</p>
<p>My workstation uses a Ryzen 9 CPU, so when I ran the exploit I just saw a blocking message instead of memory regions:</p>
<pre><code class="hljs language-bash">$ grep <span class="hljs-string">"model name"</span> /proc/cpuinfo |<span class="hljs-built_in">uniq</span>
model name	: AMD Ryzen 9 5950X 16-Core Processor

./zenbleed -m 100
*** EMBARGOED SECURITY ISSUE --  DO NOT DISTRIBUTE! ***
ZenBleed Testcase -- taviso@google.com

NOTE: Try -h to see configuration options

Spawning 32 Threads...
Thread 0x7fd0d40e8700 running on CPU 0
Thread 0x7fd0d38e7700 running on CPU 1
...
</code></pre>
<p>You can also run the following command to print out the microcode version. This is the output from the Equinix Metal server (c3.medium) that ran an OS update on:</p>
<pre><code class="hljs language-bash">$ grep <span class="hljs-string">'microcode'</span> /proc/cpuinfo
microcode	: 0x830107a
</code></pre>
<h2>Wrapping up</h2>
<p>Actuated uses <a href="https://firecracker-microvm.github.io/">Firecracker</a>, an open source Virtual Machine Manager (VMM) that works with Linux KVM to run isolated systems on a host. We have verified that the exploit works on Firecracker, and that the mitigation works too. So whilst VM-level isolation and an immutable filesystem is much more appropriate than a container for CI, this is an example of why we must still be vigilant and ready to respond to security vulnerabilities.</p>
<p>This is an unfortunate, and serious vulnerability. It affects bare-metal, VMs and containers, which is why it's important to update your systems as soon as possible.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automate Packer Images with QEMU and Actuated]]></title>
            <link>https://actuated.dev/blog/automate-packer-qemu-image-builds</link>
            <guid>https://actuated.dev/blog/automate-packer-qemu-image-builds</guid>
            <pubDate>Tue, 25 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to automate Packer images using QEMU and nested virtualisation through actuated.]]></description>
            <content:encoded><![CDATA[<p>One of the most popular tools for creating images for virtual machines is <a href="https://www.packer.io/">Packer</a> by <a href="https://hashicorp.com">Hashicorp</a>. Packer automates the process of building images for a variety of platforms from a single source configuration. Different builders can be used to create machines and generate images from those machines.</p>
<p>In this tutorial we will use the <a href="https://developer.hashicorp.com/packer/plugins/builders/qemu">QEMU builder</a> to create a <a href="https://www.linux-kvm.org/page/Main_Page">KVM</a> virtual machine image.</p>
<p>We will see how the Packer build can be completely automated by integrating Packer into a continuous integration (CI) pipeline with GitHub Actions. The workflow will automatically trigger image builds on changes and publish the resulting images as GitHub release artifacts.</p>
<p>Actuated supports nested virtualsation where a VM can make use of KVM to launch additional VMs within a GitHub Action. This makes it possible to run the Packer QEMU builder in GitHub Action workflows. Something that is not possible with GitHub's default hosted runners.</p>
<blockquote>
<p>See also: <a href="https://actuated.dev/blog/kvm-in-github-actions">How to run KVM guests in your GitHub Actions</a></p>
</blockquote>
<h2>Create the Packer template</h2>
<p>We will be starting from a <a href="https://cloud-images.ubuntu.com/">Ubuntu Cloud Image</a> and modify it to suit our needs. If you need total control of what goes into the image you can start from scratch using the ISO.</p>
<p>Variables are used in the packer template to set the <code>iso_url</code> and <code>iso_checksum</code>. In addition to these we also use variables to configure the <code>disk_size</code>, <code>ram</code>, <code>cpu</code>, <code>ssh_password</code> and <code>ssh_username</code>:</p>
<pre><code>variable "cpu" {
  type    = string
  default = "2"
}

variable "disk_size" {
  type    = string
  default = "40000"
}

variable "headless" {
  type    = string
  default = "true"
}

variable "iso_checksum" {
  type    = string
  default = "sha256:d699ae158ec028db69fd850824ee6e14c073b02ad696b4efb8c59d37c8025aaa"
}

variable "iso_url" {
  type    = string
  default = "https://cloud-images.ubuntu.com/jammy/20230719/jammy-server-cloudimg-amd64.img"
}

variable "name" {
  type    = string
  default = "jammy"
}

variable "ram" {
  type    = string
  default = "2048"
}

variable "ssh_password" {
  type    = string
  default = "ubuntu"
}

variable "ssh_username" {
  type    = string
  default = "ubuntu"
}

variable "version" {
  type    = string
  default = ""
}

variable "format" {
  type    = string
  default = "qcow2"
}
</code></pre>
<p>The Packer source configuration:</p>
<pre><code>source "qemu" "jammy" {
  accelerator      = "kvm"
  boot_command     = []
  disk_compression = true
  disk_interface   = "virtio"
  disk_image       = true
  disk_size        = var.disk_size
  format           = var.format
  headless         = var.headless
  iso_checksum     = var.iso_checksum
  iso_url          = var.iso_url
  net_device       = "virtio-net"
  output_directory = "artifacts/qemu/${var.name}${var.version}"
  qemuargs = [
    ["-m", "${var.ram}M"],
    ["-smp", "${var.cpu}"],
    ["-cdrom", "cidata.iso"]
  ]
  communicator           = "ssh"
  shutdown_command       = "echo '${var.ssh_password}' | sudo -S shutdown -P now"
  ssh_password           = var.ssh_password
  ssh_username           = var.ssh_username
  ssh_timeout            = "10m"
}
</code></pre>
<p>Some notable settings in the source configuration:</p>
<ul>
<li>We set <code>disk_image=true</code> since we are starting from an Ubuntu Cloud Image. If you wanted to launch an ISO based installation this would have to be false.</li>
<li>Notice how the variables are used to configure different VM settings like disk size: <code>disk_size=var.disk_size</code>, image output format: <code>format=var.format</code> and the RAM and CPU for the vm through <code>qemuargs</code>.</li>
<li>The <code>ssh_username</code> and <code>ssh_password</code> that Packer can use to establish an ssh connection to the VM are also configured.</li>
</ul>
<p>In the next section we will see how <a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init</a> is used to setup user account with the correct password that Packer needs for provisioning.</p>
<p>The full example of the packer file is available on <a href="https://github.com/skatolo/actuated-packer-qemu">GitHub</a>.</p>
<h3>Create the user-data file</h3>
<p>Cloud images provided by Canonical do not have users by default. The Ubuntu images use <a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init</a> to pre-configure the system during boot.</p>
<p>Packer uses provisioners to install and configure the machine image after booting. To run these provisioners Packer needs to be able to communicate with the machine. By default this happens by establishing an ssh connection to the machine.</p>
<p>Create a user-data file that sets the password of the default user so that it can be used by Packer to connect over ssh:</p>
<pre><code>#cloud-config
password: ubuntu
ssh_pwauth: true
chpasswd:
  expire: false
</code></pre>
<p>Next create an ISO that can be referenced by our Packer template and presented to the VM:</p>
<pre><code class="hljs language-bash">genisoimage -output cidata.iso -input-charset utf-8 -volid cidata -joliet -r \
</code></pre>
<p>The ISO can be mounted by QEMU to provide the configuration data to <code>cloud-init</code> while the VM boots.</p>
<p>The <code>-cdrom</code> flag is used in the <code>qemuargs</code> field to mount the <code>cidata.iso</code> file:</p>
<pre><code>  qemuargs = [
    ["-m", "${var.ram}M"],
    ["-smp", "${var.cpu}"],
    ["-cdrom", "cidata.iso"]
  ]
</code></pre>
<h3>Provision the image</h3>
<p>The build section of the Packer template is used to define provisioners that can run scripts and commands to install software and configure the machine.</p>
<p>In this example we are installing python3 but you can run any script you want or use tools like <a href="https://www.ansible.com/">Ansible</a> to automate the configuration.</p>
<pre><code>build {
  sources = ["source.qemu.jammy"]

  provisioner "shell" {
    execute_command = "{{ .Vars }} sudo -E bash '{{ .Path }}'"
    inline          = ["sudo apt update", "sudo apt install python3"]
  }

  post-processor "shell-local" {
    environment_vars = ["IMAGE_NAME=${var.name}", "IMAGE_VERSION=${var.version}", "IMAGE_FORMAT=${var.format}"]
    script           = "scripts/prepare-image.sh"
  }
}
</code></pre>
<h3>Prepare the image for publishing.</h3>
<p>Packer supports post-processors. They only run after Packer saves an instance as an image. Post-processors are commonly used to compress artifacts, upload them into a cloud, etc. See the <a href="https://developer.hashicorp.com/packer/tutorials/docker-get-started/docker-get-started-post-processors">Packer docs</a> for more use-cases and examples.</p>
<p>We will add a post processing step to the packer template to run the <code>prepare-image.sh</code> script. This script renames the image artifacts and calculates the shasum to prepare them to be uploaded as release artifacts on GitHub.</p>
<pre><code>  post-processor "shell-local" {
    environment_vars = ["IMAGE_NAME=${var.name}", "IMAGE_VERSION=${var.version}", "IMAGE_FORMAT=${var.format}"]
    script           = "scripts/prepare-image.sh"
  }
</code></pre>
<h3>Launch the build locally</h3>
<p>If your local system is setup correctly, it has the packer binary and qemu installed, you can build with just:</p>
<pre><code class="hljs language-bash">packer build .
</code></pre>
<p>The artifacts folder will contain the resulting machine image and shasum file after the build completes.</p>
<pre><code>artifacts
└── qemu
    └── jammy
        ├── jammy.qcow2
        └── jammy.qcow2.sha256sum
</code></pre>
<h2>Automate image releases with GitHub Actions.</h2>
<p>For the QEMU builder to run at peak performance it requires hardware acceleration. This is not always possible in CI runners. GitHub's hosted runners do not support nested virtualization. With Actuated we added support for launching Virtual Machines in GitHub Action pipelines. This makes it possible to run the Packer QEMU builder in your workflows.</p>
<p>Support for KVM is not enabled by default on Actuated and there are some prerequisites:</p>
<ul>
<li><code>arm64</code> runners are not supported at the moment</li>
<li>A bare-metal host that supports nested virtualization is required for the Agent.</li>
</ul>
<p>To configure your Actuated Agent for KVM support follow the <a href="https://docs.actuated.dev/examples/kvm-guest/">instructions in the docs</a>.</p>
<h3>The GitHub actions workflow</h3>
<p>The default GitHub hosted runners come with Packer pre-installed. On self-hosted runners you will need a step to install the Packer binary. The official [setup-packer][<a href="https://github.com/hashicorp/setup-packer">https://github.com/hashicorp/setup-packer</a>] action can be used for this.</p>
<p>We set <code>runs-on</code> to <code>actuated</code> so that the build workflow will run on an Actuated runner:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">tags:</span> [<span class="hljs-string">"v[0-9].[0-9]+.[0-9]+"</span>]
    <span class="hljs-attr">branches:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">"main"</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build-image:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-comment">##...</span>
</code></pre>
<p>The build job runs the following steps:</p>
<ol>
<li>
<p>Retrieve the Packer configuration by checking out the GitHub repository.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Checkout</span> <span class="hljs-string">Repository</span>
  <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
</code></pre>
</li>
<li>
<p>Install QEMU to ensure Packer is able to launch kvm/qemu virtual machines.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">qemu</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">update</span> <span class="hljs-string">&#x26;&#x26;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">install</span> <span class="hljs-string">qemu-system</span> <span class="hljs-string">-y</span>
</code></pre>
</li>
<li>
<p>Setup packet to ensure the binary is available in the path.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">packer</span>
  <span class="hljs-attr">uses:</span> <span class="hljs-string">hashicorp/setup-packer@main</span>
</code></pre>
</li>
<li>
<p>Initialize the packer template and install all plugins referenced by the template.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Packer</span> <span class="hljs-string">Init</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">packer</span> <span class="hljs-string">init</span> <span class="hljs-string">.</span>
</code></pre>
</li>
<li>
<p>Build the images defined in the root directory. Before we run the <code>packer build</code> command we make <code>/dev/kvm</code> world read-writable so that the QEMU builder can use it.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Packer</span> <span class="hljs-string">Build</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">|
    sudo chmod o+rw /dev/kvm
    packer build .
</span></code></pre>
</li>
<li>
<p>Upload the images as GitHub release artifacts. This job only runs for tagged commits.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Upload</span> <span class="hljs-string">images</span> <span class="hljs-string">and</span> <span class="hljs-string">their</span> <span class="hljs-string">SHA</span> <span class="hljs-string">to</span> <span class="hljs-string">Github</span> <span class="hljs-string">Release</span>
  <span class="hljs-attr">if:</span> <span class="hljs-string">startsWith(github.ref,</span> <span class="hljs-string">'refs/tags/v'</span><span class="hljs-string">)</span>
  <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/upload-assets@0.4.0</span>
  <span class="hljs-attr">env:</span>
    <span class="hljs-attr">GITHUB_TOKEN:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
  <span class="hljs-attr">with:</span>
    <span class="hljs-attr">asset_paths:</span> <span class="hljs-string">'["./artifacts/qemu/*/*"]'</span>
</code></pre>
</li>
</ol>
<h2>Taking it further</h2>
<p>We created a GitHub actions workflow that can run a Packer build with QEMU to create a custom Ubuntu image. The resulting <code>qcow2</code> image is automatically uploaded to the GitHub release assets on each release.</p>
<p>The released image can be downloaded and used to spin up a VM instance on your private hardware or on different cloud providers.</p>
<p>We exported the image in <code>qcow2</code> format but you might need a different image format. The QEMU builder also supports outputting images in <code>raw</code> format. In our Packer template the output format can be changed by setting the <code>format</code> variable.</p>
<p>Additional tools like the <a href="https://www.qemu.org/docs/master/tools/qemu-img.html">qemu disk image utility</a> can also be used to convert images between different formats. A <a href="https://developer.hashicorp.com/packer/docs/templates/hcl_templates/blocks/build/post-processor">post-processor</a> would be the ideal place for these kinds of extra processing steps.</p>
<p>AWS also supports importing VM images and converting them to an AMI so they can be used to launch EC2 instances. See: <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html">Create an AMI from a VM image</a></p>
<p>If you'd like to know more about nested virtualisation support, check out: <a href="https://actuated.dev/blog/kvm-in-github-actions">How to run KVM guests in your GitHub Actions</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Understand your usage of GitHub Actions]]></title>
            <link>https://actuated.dev/blog/github-actions-usage-cli</link>
            <guid>https://actuated.dev/blog/github-actions-usage-cli</guid>
            <pubDate>Fri, 16 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how you or your team is using GitHub Actions across your personal account or organisation.]]></description>
            <content:encoded><![CDATA[<p>Whenever GitHub Actions users get in touch with us to ask about actuated, we ask them a number of questions. What do you build? What pain points have you been running into? What are you currently spending? And then - how many minutes are you using?</p>
<p>That final question is a hard one for many to answer because the GitHub UI and API will only show billable minutes. Why is that a problem? Some teams only use open-source repositories with free runners. Others may have a large free allowance of credit for one reason or another and so also don't really know what they're using. Then you have people who already use some form of self-hosted runners - they are also excluded from what GitHub shows you.</p>
<p>So we built an Open Source CLI tool called <a href="https://github.com/self-actuated/actions-usage">actions-usage</a> to generate a report of your total minutes by querying GitHub's REST API.</p>
<p>And over time, we had requests to break-down per day - so for our customers in the Middle East like <a href="https://kubiya.ai/">Kubiya</a>, it's common to see a very busy day on Sunday, and not a lot of action on Friday. Given that some teams use mono-repos, we also added the ability to break-down per repository - so you can see which ones are the most active. And finally, we added the ability to see hot-spots of usage like the longest running repo or the most active day.</p>
<p>You can run the tool in three ways:</p>
<ul>
<li>Against an organisation</li>
<li>Against a personal user account</li>
<li>Or in a GitHub Action</li>
</ul>
<p>I'll show you each briefly, but the one I like the most is the third option because it's kind of recursive.</p>
<p>Before we get started, download <a href="https://arkade.dev">arkade</a>, and use it to install the tool:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Move the binary yourself into $PATH</span>
curl -sLS https://get.arkade.dev | sh

<span class="hljs-comment"># Have sudo move it for you</span>
curl -sLS https://get.arkade.dev | sudo sh
arkade install actions-usage
</code></pre>
<p>Or if you prefer - you can add my <a href="https://github.com/alexellis/homebrew-alexellis">brew tap</a>, or head over to the <a href="https://github.com/alexellis/arkade/releases">arkade releases page</a>.</p>
<p>Later on, I'll also show you how to use the <code>alexellis/arkade-get</code> action to install the tool for CI.</p>
<h2>Finding out about your organisation</h2>
<p>If you want to find out about your organisation, you can run the tool like this:</p>
<pre><code class="hljs language-bash">actions-usage \
    -org <span class="hljs-variable">$GITHUB_REPOSITORY_OWNER</span> \
    -days 28 \
    -by-repo \
    -punch-card \
    -token-file ~/PAT.txt
</code></pre>
<p>You'll need a Personal Access Token, there are instructions on how to create this in the <a href="https://github.com/self-actuated/actions-usage">actions-usage README file</a></p>
<p>There are many log lines printed to stderr during the scan of repositories and the workflows. You can omit all of this by adding <code>2> /dev/null </code> to the command.</p>
<p>First off we show the totals:</p>
<pre><code>Fetching last 28 days of data (created>=2023-05-19)

Generated by: https://github.com/self-actuated/actions-usage
Report for actuated-samples - last: 28 days.

Total repos: 24
Total private repos: 0
Total public repos: 24

Total workflow runs: 107
Total workflow jobs: 488

Total users: 1
</code></pre>
<p>Then break down on success/failure and cancelled jobs overall, plus the biggest and average build time:</p>
<pre><code>Success: 369/488
Failure: 45/488
Cancelled: 73/488

Longest build: 29m32s
Average build time: 1m26s
</code></pre>
<p>Next we have the day by day breakdown. You can see that we try to focus on our families on Sunday at OpenFaaS Ltd, instead of working:</p>
<pre><code>Day            Builds
Monday         61
Tuesday        50
Wednesday      103
Thursday       110
Friday         153
Saturday       10
Sunday         0
Total          488
</code></pre>
<p>Our customers in the Middle East work to a different week, and so you'd see Saturday with no builds or nothing, and Sunday like a normal working day.</p>
<p>Then we have the repo-by-repo breakdown with some much more granular data:</p>
<pre><code>Repo                                      Builds         Success        Failure        Cancelled      Skipped        Total          Average        Longest
actuated-samples/k3sup-matrix-test        355            273            20             62             0              2h59m1s        30s            1m29s
actuated-samples/discourse                49             38             7              4              0              6h37m21s       8m7s           20m1s
actuated-samples/specs                    35             31             1              3              0              10m20s         18s            32s
actuated-samples/cypress-test             17             4              13             0              0              6m23s          23s            49s
actuated-samples/cilium-test              9              4              2              3              0              1h10m41s       7m51s          29m32s
actuated-samples/kernel-builder-linux-6.0 9              9              0              0              0              11m28s         1m16s          1m27s
actuated-samples/actions-usage-job        8              4              2              1              0              46s            6s             11s
actuated-samples/faasd-nix                6              6              0              0              0              24m20s         4m3s           10m49s
</code></pre>
<p>Finally, we have the original value that the tool set out to display:</p>
<pre><code>Total usage: 11h40m20s (700 mins)
</code></pre>
<p>We display the value in a Go duration for readability and in minutes because that's the number that GitHub uses to talk about usage.</p>
<p>One customer told us that they were running into rate limits when querying for 28 days of data, so they dropped down to 14 days and then multiplied the result by two to get a rough estimate.</p>
<pre><code>-days 14 
</code></pre>
<p>The team at Todoist got in touch with us to see if actuated could reduce their bill on GitHub Actions. When he tried to run the tool the rate-limit was exhausted even when he changed the flag to <code>-days 1</code>. Why? They were using 550,000 minutes!</p>
<p>So we can see one of the limitations already of this approach. Fortunately, actuated customers have their job stats recorded in a database and can generate reports <a href="https://docs.actuated.dev/dashboard/">from the dashboard very quickly</a>.</p>
<h2>Finding out about your personal account</h2>
<p>Actuated isn't built for personal users, but for teams, so we didn't add this feature initially. Then we saw a few people reach out via Twitter and GitHub and decided to add it for them.</p>
<p>For your personal account, you only have to change one of the input parameters:</p>
<pre><code class="hljs language-bash">actions-usage \
    -user alexellis \
    -days 28 \
    -by-repo \
    -punch-card \
    -token-file ~/ae-pat.txt 2> /dev/null 
</code></pre>
<p>Now I actually have > 250 repositories and most of them don't even have Actions enabled, so this makes the tool less useful for me personally. So it was great when a community member suggested offering a way to filter repos when you have so many that the tool takes a long time to run or can't complete due to rate-limits.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Being that today I used it to get the same insights from a Github Org where I currently work, which contains 1.4k of repositories.<br><br>And this was running for a considerable time. I am only related to only a few repositories within this organization.</p>&mdash; lbrealdeveloper (@lbrealdeveloper) <a href="https://twitter.com/lbrealdeveloper/status/1669401439431544832?ref_src=twsrc%5Etfw">June 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I've already created an issue and have found someone who'd like to contribute the change: <a href="https://github.com/self-actuated/actions-usage/issues/8">Offer a way to filter repos for large organisations / users #8</a></p>
<p>This is the beauty of open source and community. We all get to benefit from each other's ideas and contributions.</p>
<h2>Running actions-usage with a GitHub Action</h2>
<p>Now this is my favourite way to run the tool. I can run it on a schedule and get a report sent to me via email or Slack.</p>
<p><img src="/images/2023-06-actions-usage/summary.png" alt="Example output from running the tool as a GitHub Action"></p>
<blockquote>
<p>Example output from running the tool as a GitHub Action</p>
</blockquote>
<p>Create <code>actions-usage.yaml</code> in your <code>.github/workflows</code> folder:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">actions-usage</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">main</span>
  <span class="hljs-attr">workflow_dispatch:</span>

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">actions-usage:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">daily-stats</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-any-1cpu-2gb</span>
    <span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">actions-usage:</span> <span class="hljs-string">latest</span>
        <span class="hljs-attr">print-summary:</span> <span class="hljs-literal">false</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Generate</span> <span class="hljs-string">actions-usage</span> <span class="hljs-string">report</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
       echo "### Actions Usage report by [actuated.dev](https://actuated.dev)" >> SUMMARY
       echo "\`\`\`" >> SUMMARY
       actions-usage \
        -org $GITHUB_REPOSITORY_OWNER \
        -days 1 \
        -by-repo \
        -punch-card \
        -token ${{ secrets.GITHUB_TOKEN }}  2> /dev/null  >> SUMMARY
       echo "\`\`\`" >> SUMMARY
       cat SUMMARY >> $GITHUB_STEP_SUMMARY
</span></code></pre>
<p>Ths is designed to run within an organisation, but you can change the <code>-org</code> flag to <code>-user</code> and then use your own username.</p>
<p>The days are for the past day of activity, but you can change this to any number like 7, 14 or 28 days.</p>
<p>You can learn about the other flags by running <code>actions-usage --help</code> on your own machine.</p>
<h2>Wrapping up</h2>
<p>actions-usage is a practical tool that we use with customers to get an idea of their usage and how we can help with actuated. That said, it's also a completely free and open source tool for which the community is finding their own set of use-cases.</p>
<p>And there are no worries about privacy, we've gone very low tech here. The output is only printed to the console, and we never receive any of your data unless you specifically copy and paste the output into an email.</p>
<p>Feel free to create an issue if you have a feature request or a question.</p>
<p>Check out <a href="https://github.com/self-actuated/actions-usage">self-actuated/actions-usage</a> on GitHub</p>
<p>I wrote an eBook writing CLIs like this in Go and keep it up to date on a regular basis - adding new examples and features of Go.</p>
<p>Why not check out what people are saying about it on <a href="https://store.openfaas.com/l/everyday-golang">Gumroad</a>?</p>
<p><a href="https://store.openfaas.com/l/everyday-golang">Everyday Go - The Fast Track</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Secure CI for GitLab with Firecracker microVMs]]></title>
            <link>https://actuated.dev/blog/secure-microvm-ci-gitlab</link>
            <guid>https://actuated.dev/blog/secure-microvm-ci-gitlab</guid>
            <pubDate>Fri, 16 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how actuated for GitLab CI can help you secure your CI/CD pipelines with Firecracker.]]></description>
            <content:encoded><![CDATA[<p>We started building actuated for GitHub Actions because we at OpenFaaS Ltd had a need for: unmetered CI minutes, faster &#x26; more powerful x86 machines, native Arm builds and low maintenance CI builds.</p>
<p>And most importantly, we needed it to be low-maintenance, and securely isolated.</p>
<p>None of the solutions at the time could satisfy all of those requirements, and even today with GitHub adopting the community-based Kubernetes controller to run CI in Pods, there is still a lot lacking.</p>
<p>As we've gained more experience with customers who largely had the same needs as we did for GitHub Actions, we started to hear more and more from GitLab CI users. From large enterprise companies who are concerned about the security risks of running CI with privileged Docker containers, Docker socket binding (from the host!) or the flakey nature and slow speed of VFS with Docker In Docker (DIND).</p>
<blockquote>
<p>The <a href="https://docs.gitlab.com/runner/security/">GitLab docs have a stark warning</a> about using both of these approaches. It was no surprise that when a consultant at Thoughtworks reached out to me, he listed off the pain points and concerns that we'd set out to solve for GitHub Actions.</p>
<p>At KubeCon, I also spoke to several developers who worked at Deutsche Telekom who had numerous frustrations with the user-experience and management overhead of the Kubernetes executor.</p>
</blockquote>
<p>So with growing interest from customers, we built a solution for GitLab CI - just like we did for GitHub Actions. We're excited to share it with you today in tech preview.</p>
<p><img src="/images/2023-06-gitlab-preview/conceptual.png" alt="actuated for GitLab CI"></p>
<p>For every build that requires a runner, we will schedule and boot a complete system with Firecracker using Linux KVM for secure isolation. After the job is completed, the VM will be destroyed and removed from the GitLab instance.</p>
<p>actuated for GitLab is for self-hosted GitLab instances, whether hosted on-premises or on the public cloud.</p>
<p>If you'd like to use it or find out more, please apply here: <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Sign-up for the Actuated pilot</a></p>
<h2>Secure CI with Firecracker microVMs</h2>
<p><a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> is the open-source technology that provides isolation between tenants on certain AWS products like Lambda and Fargate. There's a growing number of cloud native solutions evolving around Firecracker, and we believe that it's the only way to run CI/CD securely.</p>
<p>Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel-based Virtual Machine (KVM) to create and manage microVMs. It's lightweight, fast, and most importantly, provides proper isolation, which anything based upon Docker cannot.</p>
<p>There are no horrible Kernel tricks or workarounds to be able to use user namespaces, no need to change your tooling from what developers love - Docker, to Kaninko or Buildah or similar.</p>
<p>You'll get <code>sudo</code>, plus a fresh Docker engine in every VM, booted up with systemd, so things like Kubernetes work out of the box, if you need them for end to end testing (as so many of us do these days).</p>
<p>You can learn the differences between VMs, containers and microVMs like Firecracker in my video from Cloud Native Rejekts at KubeCon Amsterdam:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pTQ_jVYhAoc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Many people have also told me that they learned how to use Firecracker from my webinar last year with Richard Case: <a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">A cracking time: Exploring Firecracker &#x26; MicroVMs</a>.</p>
<h2>Let's see it then</h2>
<p>Here's a video demo of the tech preview we have available for customers today.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PybSPduDT6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>You'll see that when I create a commit in our self-hosted copy of GitLab Enterprise, within 1 second a microVM is booting up and running the CI job.</p>
<p>Shortly after that the VM is destroyed which means there are absolutely no side-effects or any chance of leakage between jobs.</p>
<p>Here's a later demo of three jobs within a single pipeline, all set to run in parallel.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Here&#39;s 3x <a href="https://twitter.com/gitlab?ref_src=twsrc%5Etfw">@GitLab</a> CI jobs running in parallel within the same Pipeline demoed by <a href="https://twitter.com/alexellisuk?ref_src=twsrc%5Etfw">@alexellisuk</a> <br><br>All in their own ephemeral VM powered by Firecracker 🔥<a href="https://twitter.com/hashtag/cicd?src=hash&amp;ref_src=twsrc%5Etfw">#cicd</a> <a href="https://twitter.com/hashtag/secure?src=hash&amp;ref_src=twsrc%5Etfw">#secure</a> <a href="https://twitter.com/hashtag/isolation?src=hash&amp;ref_src=twsrc%5Etfw">#isolation</a> <a href="https://twitter.com/hashtag/microvm?src=hash&amp;ref_src=twsrc%5Etfw">#microvm</a> <a href="https://twitter.com/hashtag/baremetal?src=hash&amp;ref_src=twsrc%5Etfw">#baremetal</a> <a href="https://t.co/fe5HaxMsGB">pic.twitter.com/fe5HaxMsGB</a></p>&mdash; actuated (@selfactuated) <a href="https://twitter.com/selfactuated/status/1668575246952136704?ref_src=twsrc%5Etfw">June 13, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Everything's completed before I have a chance to even open the logs in the UI of GitLab.</p>
<h2>Wrapping up</h2>
<p>actuated for GitLab is for self-hosted GitLab instances, whether hosted on-premises or on the public cloud.</p>
<p>Here's what we bring to the table:</p>
<ul>
<li>🚀 Faster x86 builds</li>
<li>🚀 Secure isolation with Firecracker microVMs</li>
<li>🚀 Native Arm builds that can actually finish</li>
<li>🚀 Fixed-costs &#x26; less management</li>
<li>🚀 Insights into CI usage across your organisation</li>
</ul>
<p>Runners are registered and running a job in a dedicated VM within less than one second. Our scheduler can pack in jobs across a fleet of servers, <a href="https://docs.actuated.dev/provision-server/">they just need to have KVM available</a>.</p>
<p>If you think your automation for runners could be improved, or work with customers who need faster builds, better isolation or Arm support, get in touch with us.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Sign-up for the Actuated pilot</a></li>
<li><a href="https://docs.actuated.dev/">Browse the docs and FAQ for actuated for GitHub Actions</a></li>
<li><a href="https://actuated.dev/">Read customer testimonials</a></li>
</ul>
<p>You can follow <a href="https://twitter.com/selfactuated">@selfactuated</a> on Twitter, or find <a href="https://twitter.com/alexellisuk">me there too</a> to keep an eye on what we're building.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Faster Nix builds with GitHub Actions and actuated]]></title>
            <link>https://actuated.dev/blog/faster-nix-builds</link>
            <guid>https://actuated.dev/blog/faster-nix-builds</guid>
            <pubDate>Mon, 12 Jun 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Speed up your Nix project builds on GitHub Actions with runners powered by Firecracker.]]></description>
            <content:encoded><![CDATA[<p><a href="https://github.com/openfaas/faasd">faasd</a> is a lightweight and portable version of <a href="https://www.openfaas.com/">OpenFaaS</a> that was created to run on a single host. In my spare time I maintain <a href="https://github.com/welteki/faasd-nix">faasd-nix</a>, a project that packages faasd and exposes a NixOS module so it can be run with NixOS.</p>
<p>The module itself depends on faasd, containerd and the CNI plugins and all of these binaries are built in CI with Nix and then cached using <a href="https://www.cachix.org/">Cachix</a> to save time on subsequent builds.</p>
<p>I often deploy faasd with NixOS on a Raspberry Pi and to the cloud, so I build binaries for both <code>x86_64</code> and <code>aarch64</code>. The build usually runs on the default GitHub hosted action runners. Now because GitHub currently doesn't have Arm support, I use QEMU instead which can emulate them. The drawback of this approach is that builds can sometimes be several times slower.</p>
<blockquote>
<p>For some of our customers, their builds couldn't even complete in 6 hours using QEMU, and only took between 5-20 minutes using native Arm hardware. Alex Ellis, Founder of Actuated.</p>
</blockquote>
<p>While upgrading to the latest nixpkgs release recently I decided to try and build the project on runners managed with Actuated to see the improvements that can be made by switching to both bigger <code>x86_64</code> iron and native Arm hardware.</p>
<h2>Nix and GitHub actions</h2>
<p>One of the features Nix offers are reproducible builds. Once a package is declared it can be built on any system. There is no need to prepare your machine with all the build dependencies. The only requirement is that Nix is installed.</p>
<blockquote>
<p>If you are new to Nix, then I'd recommend you read the <a href="https://zero-to-nix.com/start/install">Zero to Nix</a> guide. It's what got me excited about the project.</p>
</blockquote>
<p>Because Nix is declarative and offers reproducible builds, it is easy to setup a concise build pipeline for GitHub actions. A lot of steps usually required to setup the build environment can be left out. For instance, faasd requires Go, but there's no need to install it onto the build machine, and you'd normally have to install btrfs-progs to build containerd, but that's not something you have to think about, because Nix will take care of it for you.</p>
<p>Another advantage of the reproducible builds is that if it works on your local machine it most likely also works in CI. No need to debug and find any discrepancies between your local and CI environment.</p>
<blockquote>
<p>Of course, if you ever do get frustrated and want to debug a build, you can use the built-in <a href="https://docs.actuated.dev/tasks/debug-ssh/">SSH feature in Actuated</a>. Alex Ellis, Founder of Actuated.</p>
</blockquote>
<p>This is what the workflow looks like for building faasd and its related packages:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v21</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">faasd</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">containerd</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd-containerd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">cni-plugin</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#faasd-cni-plugins
</span></code></pre>
<p>All this pipeline does is install Nix, using the <a href="https://github.com/marketplace/actions/install-nix">cachix/install-nix-action</a> and run the <code>nix build</code> command for the packages that need to be built.</p>
<h2>Notes on the nix build for aarch64</h2>
<p>To build the packages for multiple architectures there are a couple of options:</p>
<ul>
<li>cross-compiling, Nix has great support for cross compilation.</li>
<li>compiling through binfmt QEMU.</li>
<li>compiling natively on an aarch64 machine.</li>
</ul>
<p>The preferred option would be to compile everything natively on an aarch64 machine as that would result in the best performance. However, at the time of writing GitHub does not provide Arm runners. That is why QEMU is used by many people to compile binaries in CI.</p>
<p>Enabling the binfmt wrapper on NixOS can be done easily through the NixOS configuration. On non-NixOS machines, like on the GitHub runner VM, the QEMU static binaries need to be installed and the Nix daemon configuration updated.</p>
<p>Instructions to configure Nix for compilation with QEMU can be found on the <a href="https://nixos.wiki/wiki/NixOS_on_ARM#Compiling_through_binfmt_QEMU">NixOS wiki</a></p>
<p>The workflow for building aarch64 packages with QEMU on GitHub Actions looks like this:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-qemu-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v21</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">extra_nix_config:</span> <span class="hljs-string">|
            extra-platforms = aarch64-linux
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">faasd</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">containerd</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd-containerd
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span> <span class="hljs-string">cni-plugin</span> <span class="hljs-string">🔧</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          nix build -L .#packages.aarch64-linux.faasd-cni-plugins
</span>
</code></pre>
<p>Install the QEMU static binaries using <a href="https://github.com/docker/setup-qemu-action">docker/setup-qemu-action</a>. Let the nix daemon know that it can build for aarch64 by adding <code>extra-platforms = aarch64-linux</code> via the <code>extra_nix_config</code> input on the install nix action. Update the nix build commands to specify platform e.g. <code>nix build .#packages.aarch64-linux.faasd</code>.</p>
<h2>Speeding up the build with a Raspberry Pi</h2>
<p>Nix has great support for caching and build speeds can be improved greatly by never building things twice. This project normally uses <a href="https://www.cachix.org/">Cachix</a> for caching and charing binaries across systems. For this comparison caching was disabled. All packages and their dependencies are built from scratch again each time.</p>
<p>Building the project takes around 4 minutes and 20 seconds on the standard GitHub hosted runner. After switching to a more powerful Actuated runner with 4CPUs and 8GB of RAM the build time dropped to 2 minutes and 15 seconds.</p>
<p><img src="/images/2023-06-faster-nix-builds/amd64-build-comparison.png" alt="Comparison of more powerful Actuated runner with GitHub hosted runner"></p>
<blockquote>
<p>Comparison of more powerful Actuated runner with GitHub hosted runner.</p>
</blockquote>
<p>While build times are still acceptable for x86_64 this is not the case for the aarch64 build. It takes around 55 minutes to complete the Arm build with QEMU on a GitHub runner.</p>
<p>Running the same build with QEMU on the Actuated runner already brings down the build time to 19 minutes and 40 seconds. Running the build natively on a Raspberry Pi 4 (8GB) completed in 11 minutes and 47 seconds. Building on a more powerful Arm machine would potentially reduce this time to a couple of minutes.</p>
<p><img src="/images/2023-06-faster-nix-builds/arm64-build-comparison.png" alt="Results of the matrix build comparing the GitHub hosted runner and the 2 Actuated runners"></p>
<blockquote>
<p>Results of the matrix build comparing the GitHub hosted runner and the 2 Actuated runners.</p>
</blockquote>
<p>Running the build natively on the Pi did even beat the fast bare-metal machine that is using QEMU.</p>
<p>My colleague Alex ran the same build on his Raspberry Pi using Actuated and an NVMe mounted over USB-C, he got the build time down even further. Why? Because it increased the I/O performance. In fact, if you build this on server-grade Arm like the Ampere Altra, it would be about 4x faster than the Pi 4.</p>
<p>Building for Arm:</p>
<ul>
<li>Alex's Raspberry Pi with NVMe: 10m49s</li>
<li>An Ampere Altra on Equinix Metal: 3m29s</li>
</ul>
<p>Building for x86_64</p>
<ul>
<li>AMD Epyc on Equinix Metal: 1m57s</li>
</ul>
<p><img src="/images/2023-06-faster-nix-builds/alex-results.png" alt="Alex&#x27;s results"></p>
<p>These results show that whatever the Arm hardware you pick, it'll likely be faster than QEMU, even when QEMU is run on the fastest bare-metal available, the slowest Arm hardware will beat it by minutes.</p>
<h2>Wrapping up</h2>
<p>Building your projects with Nix allows your GitHub actions pipelines to be concise and easy to maintain.</p>
<p>Even when you are not using Nix to build your project it can still help you to create concise and easy to maintain GitHub Action workflows. With Nix shell environments you can use Nix to declare which dependencies you want to make available inside an isolated shell environment for your project: <a href="https://determinate.systems/posts/nix-github-actions">Streamline your GitHub Actions dependencies using Nix</a></p>
<p>Building Nix packages or entire NixOS systems on GitHub Actions can be slow especially if you need to build for Arm. Bringing your own metal to GitHub actions can speed up your builds. If you need Arm runners, Actuated is one of the only options for securely isolated CI that is safe for Open Source and public repositories. Alex explains why in: <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></p>
<p>Another powerful feature of the Nix ecosystem is the ability to run integration tests using virtual machines (NixOS test). This feature requires hardware acceleration to be available in the CI runner. Actuated makes it possible to run these tests in GitHub Actions CI pipelines: <a href="https://actuated.dev/blog/kvm-in-github-actions">how to run KVM guests in your GitHub Actions</a>.</p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions">Bring Your Own Metal - Case Study with GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fixing the cache latency for self-hosted GitHub Actions]]></title>
            <link>https://actuated.dev/blog/faster-self-hosted-cache</link>
            <guid>https://actuated.dev/blog/faster-self-hosted-cache</guid>
            <pubDate>Wed, 24 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[The cache for GitHub Actions can speed up CI/CD pipelines. But what about when it slows you down?]]></description>
            <content:encoded><![CDATA[<p>In some of our builds for actuated we cache things like the Linux Kernel, so we don't needlessly rebuild it when we update packages in our base images. It can shave minutes off every build meaning our servers can be used more efficiently. Most customers we've seen so far only make light to modest use of GitHub's hosted cache, so haven't noticed much of a latency problem.</p>
<p>But you don't have to spend too long on the <a href="https://github.com/actions/cache/issues?q=is%3Aissue+cache+slow+">issuer tracker for GitHub Actions</a> to find people complaining about the cache being slow or locking up completely for self-hosted runners.</p>
<p>Go, Rust, Python and other languages don't tend to make heavy use of caches, and Docker has some of its own mechanisms like building cached steps into published images aka <em><a href="https://docs.docker.com/build/cache/backends/inline/">inline caching</a></em>. But for the Node.js ecosystem, the <code>node_modules</code> folder and yarn cache can become huge and take a long time to download. That's one place where you may start to see tension between the speed of self-hosted runners and the latency of the cache. If your repository is a monorepo or has lots of large artifacts, you may get a speed boost by caching that too.</p>
<p>So why is GitHub's cache so fast for hosted runners, and (sometimes) so slow self-hosted runners?</p>
<p>Simply put - GitHub runs VMs and the accompanying cache on the same network, so they can talk over a high speed backbone connection. But when you run a self-hosted runner, then any download or upload operations are taking place over the public Internet.</p>
<p>Something else that can slow builds down is having to download large base images from the Docker Hub. We've already <a href="https://docs.actuated.dev/tasks/registry-mirror/">covered how to solve that for actuated in the docs</a>.</p>
<h2>Speeding up in the real world</h2>
<p>We recently worked with Roderik, the CTO of <a href="https://settlemint.com">SettleMint</a> to migrate their CI from a self-hosted Kubernetes solution Actions Runtime Controller (ARC) to actuated. He told me that they originally moved from GitHub's hosted runners to ARC to save money, increase speed and to lower the latency of their builds. Unfortunately, running container builds within Kubernetes provided very poor isolation, and side effects were being left over between builds, even with a pool of ephemeral containers. They also wanted to reduce the amount of effort required to maintain a Kubernetes cluster and control-plane for CI.</p>
<p>Roderik explained that he'd been able to get times down by using <a href="https://pnpm.io">pnpm</a> instead of yarn, and said every Node project should try it out to see the speed increases. He believes the main improvement is due to efficient downloading and caching. pnpm is a drop-in replacement for npm and yarn, and is compatible with both.</p>
<blockquote>
<p>In some cases, we found that downloading dependencies from the Internet was faster than using GitHub's remote cache. The speed for a hosted runner was often over 100MBs/sec, but for a self-hosted runner it was closer to 20MBs/sec.</p>
</blockquote>
<p>That's when we started to look into how we could run a cache directly on the same network as our self-hosted runners, or even on the machine that was scheduling the Firecracker VMs.</p>
<blockquote>
<p>"With the local cache that Alex helped us set up, the cache is almost instantaneous. It doesn't even have time to show a progress bar."</p>
</blockquote>
<p>Long story short, SettleMint have successfully migrated their CI for x86 and Arm to actuated for the whole developer team:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Super happy with my new self hosted GHA runners powered by <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a>, native speeds on both AMD and ARM bare metal monster machines. Our CI now goes brrrr… <a href="https://t.co/quZ4qfcLmu">pic.twitter.com/quZ4qfcLmu</a></p>&mdash; roderik.eth (@r0derik) <a href="https://twitter.com/r0derik/status/1661109934346346510?ref_src=twsrc%5Etfw">May 23, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This post is about speed improvements for caching, but if you're finding that QEMU is too slow to build your Arm containers on hosted runners, you may benefit from switching to actuated with bare-metal Arm servers.</p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/how-to-run-multi-arch-builds-natively">How to split up multi-arch Docker builds to run natively</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>
<h2>Set up a self-hosted cache for GitHub Actions</h2>
<p>In order to set up a self-hosted cache for GitHub Actions, we switched out the official <a href="https://github.com/actions/cache">actions/cache@v3</a> action for <a href="https://github.com/tespkg/actions-cache">tespkg/actions-cache@v1</a> created by Target Energy Solutions, a UK-based company, which can target S3 instead of the proprietary GitHub cache.</p>
<p>We then had to chose between Seaweedfs and Minio for the self-hosted S3 server. Of course, there's also nothing stopping you from actually using AWS S3, or Google Cloud Storage, or another hosted service.</p>
<p>Then, the question was - should we run the S3 service directly on the server that was running Firecracker VMs, for ultimate near-loopback speed, or on a machine provisioned in the same region, just like GitHub does with Azure?</p>
<p>Either would be a fine option. If you decide to host a public S3 cache, make sure that authentication and TLS are both enabled. You may also want to set up an IP whitelist just to deter any bots that may scan for public endpoints.</p>
<h3>Set up Seaweedfs</h3>
<p>The <a href="https://github.com/seaweedfs/seaweedfs">Seaweedfs</a> README describes the project as:</p>
<blockquote>
<p>"a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, cross-DC active-active replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding."</p>
</blockquote>
<p>We liked it so much that we'd already added it to the arkade marketplace, arkade is a faster, developer-focused alternative to brew.</p>
<pre><code class="hljs language-bash">arkade get seaweedfs
sudo <span class="hljs-built_in">mv</span> ~/.arkade/bin/seaweedfs /usr/local/bin
</code></pre>
<p>Define a secret key and access key to be used from the CI jobs in the <code>/etc/seaweedfs/s3.conf</code> file:</p>
<pre><code>{
  "identities": [
    {
      "name": "actuated",
      "credentials": [
        {
          "accessKey": "s3cr3t",
          "secretKey": "s3cr3t"
        }
      ],
      "actions": [
        "Admin",
        "Read",
        "List",
        "Tagging",
        "Write"
      ]
    }
  ]
}
</code></pre>
<p>Create <code>seaweedfs.service</code>:</p>
<pre><code>[Unit]
Description=SeaweedFS
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/seaweedfs server -ip=192.168.128.1 -volume.max=0 -volume.fileSizeLimitMB=2048 -dir=/home/runner-cache -s3 -s3.config=/etc/seaweedfs/s3.conf
Restart=on-failure

[Install]
WantedBy=multi-user.target
</code></pre>
<p>We have set <code>-volume.max=0 -volume.fileSizeLimitMB=2048</code> to minimize the amount of space used and to allow large zip files of up to 2GB, but you can change this to suit your needs. See <code>seaweedfs server --help</code> for more options.</p>
<p>Install it and check that it started:</p>
<pre><code>sudo cp ./seaweedfs.service /etc/systemd/system/seaweedfs.service
sudo systemctl enable seaweedfs

sudo journalctl -u seaweedfs -f
</code></pre>
<h2>Try it out</h2>
<p>You'll need to decide what you want to cache and whether you want to use a hosted, or self-hosted S3 service - either directly on the actuated server or on a separate machine in the same region.</p>
<p>Roderik explained that the pnpm cache was important for node_modules, but that actually caching the git checkout saved a lot of time too. So he added both into his builds.</p>
<p>Here's an example:</p>
<pre><code class="hljs language-yaml">    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"Set current date as env variable"</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        echo "CHECKOUT_DATE=$(date +'%V-%Y')" >> $GITHUB_ENV
</span>      <span class="hljs-attr">id:</span> <span class="hljs-string">date</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">tespkg/actions-cache@v1</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">endpoint:</span> <span class="hljs-string">"192.168.128.1"</span>
        <span class="hljs-attr">port:</span> <span class="hljs-number">8333</span>
        <span class="hljs-attr">insecure:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">accessKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">secretKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">bucket:</span> <span class="hljs-string">actuated-runners</span>
        <span class="hljs-attr">region:</span> <span class="hljs-string">local</span>
        <span class="hljs-attr">use-fallback:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">path:</span> <span class="hljs-string">./.git</span>
        <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-checkout-${{</span> <span class="hljs-string">env.CHECKOUT_DATE</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
          ${{ runner.os }}-checkout-
</span></code></pre>
<ul>
<li><code>use-fallback</code> - option means that if seaweedfs is not installed on the host, or is inaccessible, the action will fall back to using the GitHub cache.</li>
<li><code>key</code> - as per GitHub's action - created when saving a cache and the key used to search for a cache</li>
<li><code>restore-keys</code> - as per GitHub's action - if no cache hit occurs for key, these restore keys are used sequentially in the order provided to find and restore a cache.</li>
<li><code>bucket</code> - the name of the bucket to use in seaweedfs</li>
<li><code>accessKey</code> and <code>secretKey</code> - the credentials to use to access the bucket - we'd recommend using an organisation-level secret for this</li>
<li><code>endpoint</code> - the IP address <code>192.168.128.1</code> refers to the host machine where the Firecracker VM is running</li>
</ul>
<p>See also: <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows">Official GitHub Actions Cache action</a></p>
<p>You may also want to create a self-signed certificate for the S3 service and then set <code>insecure: false</code> to ensure that the connection is encrypted. If you're running these builds within private repositories, tampering is unlikely.</p>
<p>Roderik explained that the cache key uses a week-year format, rather than a SHA. Why? Because a SHA would change on every build, meaning that a save and load would be performed on every build, using up more space and slowing things down. In this example, There's only ever 52 cache entries per year.</p>
<blockquote>
<p>You define a key which is unique if the cache needs to be updated. Then you define a restore key that matches part or all of the key.
Part means it takes the last one that matches, then updates at the end of the run, in the post part, it then uses the key to upload the zip file if the key is different from the one stored.</p>
</blockquote>
<p>In one instance, a cached checkout went from 2m40s to 11s. That kind of time saving adds up quickly if you have a lot of builds.</p>
<p>Roderik's pipeline has multiple steps, and may need to run multiple times, so we're looking at 55s instead of 13 minutes for 5 jobs or runs.</p>
<p><img src="/images/2023-05-faster-cache/SettleMint.png" alt="Example pipeline"></p>
<blockquote>
<p>One of the team's pipelines</p>
</blockquote>
<p>Here's how to enable a cache for <code>pnpm</code>:</p>
<pre><code class="hljs language-yaml">    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">PNPM</span>
      <span class="hljs-attr">uses:</span> <span class="hljs-string">pnpm/action-setup@v2</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">run_install:</span> <span class="hljs-string">|
          - args: [--global, node-gyp]
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">pnpm</span> <span class="hljs-string">store</span> <span class="hljs-string">directory</span>
      <span class="hljs-attr">id:</span> <span class="hljs-string">pnpm-cache</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">tespkg/actions-cache@v1</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">endpoint:</span> <span class="hljs-string">"192.168.128.1"</span>
        <span class="hljs-attr">port:</span> <span class="hljs-number">8333</span>
        <span class="hljs-attr">insecure:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">accessKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">secretKey:</span> <span class="hljs-string">"s3cr3t"</span>
        <span class="hljs-attr">bucket:</span> <span class="hljs-string">actuated-runners</span>
        <span class="hljs-attr">region:</span> <span class="hljs-string">local</span>
        <span class="hljs-attr">use-fallback:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">path:</span>
          <span class="hljs-string">${{</span> <span class="hljs-string">steps.pnpm-cache.outputs.STORE_PATH</span> <span class="hljs-string">}}</span>
          <span class="hljs-string">~/.cache</span>
          <span class="hljs-string">.cache</span>
        <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-pnpm-store-${{</span> <span class="hljs-string">hashFiles('**/pnpm-lock.yaml')</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
          ${{ runner.os }}-pnpm-store-
</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">dependencies</span>
      <span class="hljs-attr">shell:</span> <span class="hljs-string">bash</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        pnpm install --frozen-lockfile --prefer-offline
</span>      <span class="hljs-attr">env:</span>
        <span class="hljs-attr">HUSKY:</span> <span class="hljs-string">'0'</span>
        <span class="hljs-attr">NODE_ENV:</span> <span class="hljs-string">development</span>
</code></pre>
<p>Picking a good key and restore key can help optimize when the cache is read from and written to:</p>
<blockquote>
<p>"You need to determine a good key and restore key. For pnpm, we use the hash of the lock file in the key, but leave it out of the restore key. So if I update the lock file, it starts from the last cache, updates it, and stores the new cache with the new hash"</p>
</blockquote>
<p>If you'd like a good starting-point for GitHub Actions Caching, Han Verstraete from our team wrote up a good primer for the actuated docs:</p>
<p><a href="https://docs.actuated.dev/examples/github-actions-cache/">Example: GitHub Actions cache</a></p>
<h2>Conclusion</h2>
<p>We were able to dramatically speed up caching for GitHub Actions by using a self-hosted S3 service. We used Seaweedfs directly on the server running Firecracker with a fallback to GitHub's cache if the S3 service was unavailable.</p>
<p><a href="https://twitter.com/alexellisuk/status/1661282581617229827/"><img src="https://pbs.twimg.com/media/Fw4PQEfWwAIl-6u?format=jpg&#x26;name=medium" alt="Brr"></a></p>
<blockquote>
<p>An <a href="https://amperecomputing.com/en/">Ampere</a> Altra Arm server running parallel VMs using Firecracker. The CPU is going brr. <a href="https://docs.actuated.dev/provision-server/">Find a server with our guide</a></p>
</blockquote>
<p>We also tend to recommend that all customers enable a mirror of the Docker Hub to counter restrictive rate-limits. The other reason is to avoid any penalties that you'd see from downloading large base images - or from downloading small to medium sized images when running in high concurrency.</p>
<p>You can find out how to configure a container mirror for the Docker Hub using actuated here: <a href="https://docs.actuated.dev/tasks/registry-mirror/">Set up a registry mirror</a>. When testing builds for the <a href="https://github.com/discourse/discourse">Discourse</a> team, there was a 2.5GB container image used for UI testing with various browsers preinstalled within it. We found that we could shave off a few minutes off the build time by using the local mirror. Imagine 10x of those builds running at once, needlessly downloading 250GB of data.</p>
<p>What if you're not an actuated customer? Can you still benefit from a faster cache? You could try out a hosted service like AWS S3 or Google Cloud Storage, provisioned in a region closer to your runners. The speed probably won't quite be as good, but it should still be a lot faster than reaching over the Internet to GitHub's cache.</p>
<p>If you'd like to try out actuated for your team, <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">reach out to us to find out more</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Book 20 mins with me if you think your team could benefit from the below for GitHub Actions:<br><br>🚀 Insights into CI usage across your organisation<br>🚀 Faster x86 builds<br>🚀 Native Arm builds that can actually finish<br>🚀 Fixed-costs &amp; less management<a href="https://t.co/iTiZsH9pgv">https://t.co/iTiZsH9pgv</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1656300308325179393?ref_src=twsrc%5Etfw">May 10, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Keyless deployment to OpenFaaS with OIDC and GitHub Actions]]></title>
            <link>https://actuated.dev/blog/oidc-proxy-for-openfaas</link>
            <guid>https://actuated.dev/blog/oidc-proxy-for-openfaas</guid>
            <pubDate>Fri, 05 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[We're announcing a new OIDC proxy for OpenFaaS for keyless deployments from GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>In 2021, GitHub released <a href="https://openid.net/connect/">OpenID Connect (OIDC)</a> support for CI jobs running under GitHub Actions. This was a huge step forward for security meaning that any GitHub Action could mint an OIDC token and use it to securely federate into another system without having to store long-lived credentials in the repository.</p>
<p>I wrote a prototype for OpenFaaS shortly after the announcement and a deep dive explaining how it works. I used <a href="https://inlets.dev/blog/2022/11/16/automate-a-self-hosted-https-tunnel.html">inlets</a> to set up a HTTPS tunnel, and send the token to my machine for inspection. Various individuals and technical teams have used my content as <a href="https://twitter.com/mlbiam/status/1653391969110900741?s=20">a reference guide</a> when working with GitHub Actions and OIDC.</p>
<p>See the article: <a href="https://blog.alexellis.io/deploy-without-credentials-using-oidc-and-github-actions/">Deploy without credentials with GitHub Actions and OIDC</a></p>
<p>Since then, custom actions for GCP, AWS and Azure have been created which allow an OIDC token from a GitHub Action to be exchanged for a short-lived access token for their API - meaning you can manage cloud resources securely. For example, see: <a href="https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services">Configuring OpenID Connect in Amazon Web Services</a> - we have actuated customers who use this approach to deploy to ECR from their self-hosted runners without having to store long-lived credentials in their repositories.</p>
<h2>Why OIDC is important for OpenFaaS customers</h2>
<p>Before we talk about the new OIDC proxy for OpenFaaS, I should say that <a href="https://openfaas.com/pricing">OpenFaaS Enterprise</a> <em>also has</em> an IAM feature which includes OIDC support for the CLI, dashboard and API. It supports any trusted OIDC provider, not just GitHub Actions. Rather than acting as a proxy, it actually implements a full fine-grained authorization and permissions policy language that resembles the one you'll be used to from AWS.</p>
<p>However, not everyone needs this level of granularity.</p>
<p><a href="https://il.linkedin.com/in/shaked-askayo-18403714a">Shaked, the CTO of Kubiya.ai</a> is an <a href="https://openfaas.com/">OpenFaaS</a> &#x26; <a href="https://inlets.dev">inlets</a> customer. His team at <a href="https://kubiya.ai">Kubiya</a> is building a conversational AI for DevOps - if you're ever tried ChatGPT, imagine that it was hooked up to your infrastructure and had superpowers. On a recent call, he told me that their team now has 30 different repositories which deploy OpenFaaS functions to their various AWS EKS clusters. That means that a secret has to be maintained at the organisation level and then consumed via <code>faas-cli login</code> in each job.</p>
<p>It gets a little worse for them - because different branches deploy to different OpenFaaS gateways and to different EKS clusters.</p>
<p>In addition to managing various credentials for each cluster they add - they were uncomfortable with exposing all of their functions on the Internet.</p>
<p>So today the team working on actuated is releasing a new OIDC proxy which can be deployed to any OpenFaaS cluster to avoid the need to manage and share long-lived credentials with GitHub.</p>
<p><img src="/images/2023-05-openfaas-oidc-proxy/conceptual.png" alt="Conceptual design of the OIDC proxy for OpenFaaS"></p>
<blockquote>
<p>Conceptual design of the OIDC proxy for OpenFaaS</p>
</blockquote>
<p><strong>About the OIDC proxy for OpenFaaS</strong></p>
<ul>
<li>It can be deployed via Helm</li>
<li>Only allows access to a given set of GitHub organisations</li>
<li>Uses a standard Ingress record</li>
<li>It only accepts OIDC tokens signed from GitHub Actions</li>
<li>It only allows requests to the <code>/system</code> endpoints of the OpenFaaS REST API - keeping your functions safe</li>
</ul>
<p>Best of all, unlike OpenFaaS Enterprise, it's free for all actuated customers - whether they're using OpenFaaS CE, Standard or Enterprise.</p>
<p>Here's what Shaked had to say about the new proxy:</p>
<blockquote>
<p>That's great - thank you! Looking forward to it as it will simplify our usage of the openfaas templates and will speed up our development process
Shaked, CTO, Kubiya.ai</p>
</blockquote>
<h2>How to deploy the proxy for OpenFaaS</h2>
<p>Here's what you need to do:</p>
<ul>
<li>Install OpenFaaS on a Kubernetes cluster</li>
<li>Create a new DNS A or CNAME record for the Ingress to the proxy i.e. <code>oidc-proxy.example.com</code></li>
<li>Install the OIDC proxy using Helm</li>
<li>Update your GitHub Actions workflow to use the OIDC token</li>
</ul>
<blockquote>
<p>My cluster is not publicly exposed on the Internet, so I'm using an <a href="https://inlets.dev/blog/2022/11/16/automate-a-self-hosted-https-tunnel.html">inlets tunnel</a> to expose the OIDC Proxy from my local KinD cluster. I'll be using the domain <code>minty.exit.o6s.io</code> but you'd create something more like <code>oidc-proxy.example.com</code> for your own cluster.</p>
</blockquote>
<p>First Set up your values.yaml for Helm:</p>
<pre><code class="hljs language-yaml"><span class="hljs-comment"># The public URL to access the proxy</span>
<span class="hljs-attr">publicURL:</span> <span class="hljs-string">https://oidc-proxy.example.com</span>

<span class="hljs-comment"># Comma separated list of repository owners for which short-lived OIDC tokens are authorized.</span>
<span class="hljs-comment"># For example: alexellis,self-actuated</span>
<span class="hljs-attr">repositoryOwners:</span> <span class="hljs-string">'alexellis,self-actuated'</span>
<span class="hljs-attr">ingress:</span>
    <span class="hljs-attr">host:</span> <span class="hljs-string">oidc-proxy.example.com</span>
    <span class="hljs-attr">issuer:</span> <span class="hljs-string">letsencrypt-prod</span>
</code></pre>
<p>The chart will create an Ingress record for you using an existing issuer. If you want to use something else like Inlets or Istio to expose the OIDC proxy, then simply set <code>enabled: false</code> under the <code>ingress:</code> section.</p>
<p>Create a secret for the actuated subscription key:</p>
<pre><code class="hljs language-bash">kubectl create secret generic actuated-license \
  -n openfaas \
  --from-file=actuated-license=<span class="hljs-variable">$HOME</span>/.actuated/LICENSE
</code></pre>
<p>Then run:</p>
<pre><code class="hljs language-bash">helm repo add actuated https://self-actuated.github.io/charts/
helm repo update

helm upgrade --install actuated/openfaas-oidc-proxy \
    -f ./values.yaml
</code></pre>
<p>For the full setup - <a href="https://github.com/self-actuated/charts/tree/master/chart/openfaas-oidc-proxy">see the README for the Helm chart</a></p>
<p>You can now go to one of your repositories and update the workflow to authenticate to the REST API via an OIDC token.</p>
<p>In order to get an OIDC token within a build, add the <code>id_token: write</code> permission to the permissions list.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">keyless_deploy</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">workflow_dispatch:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">'*'</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">keyless_deploy:</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">contents:</span> <span class="hljs-string">'read'</span>
      <span class="hljs-attr">id-token:</span> <span class="hljs-string">'write'</span>
</code></pre>
<p>Then set <code>runs-on</code> to <code>actuated</code> to use your faster actuated servers:</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">-   runs-on: ubuntu-latest</span>
<span class="hljs-addition">+   runs-on: actuated</span>
</code></pre>
<p>Then in the workflow, install the OpenFaaS CLI:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
    <span class="hljs-attr">with:</span>
        <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">faas-cli</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">curl</span> <span class="hljs-string">-sLS</span> <span class="hljs-string">https://cli.openfaas.com</span> <span class="hljs-string">|</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sh</span>
</code></pre>
<p>Then get a token:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">|
        OPENFAAS_URL=https://minty.exit.o6s.io
        OIDC_TOKEN=$(curl -sLS "${ACTIONS_ID_TOKEN_REQUEST_URL}&#x26;audience=$OPENFAAS_URL" -H "User-Agent: actions/oidc-client" -H "Authorization: Bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN")
        JWT=$(echo $OIDC_TOKEN | jq -j '.value')
</span></code></pre>
<p>Finally, use the token whenever you need it by passing in the <code>--token</code> flag to any of the <code>faas-cli</code> commands:</p>
<pre><code class="hljs language-yaml"><span class="hljs-string">faas-cli</span> <span class="hljs-string">list</span> <span class="hljs-string">-n</span> <span class="hljs-string">openfaas-fn</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
<span class="hljs-string">faas-cli</span> <span class="hljs-string">ns</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
<span class="hljs-string">faas-cli</span> <span class="hljs-string">store</span> <span class="hljs-string">deploy</span> <span class="hljs-string">printer</span> <span class="hljs-string">--name</span> <span class="hljs-string">p1</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>

<span class="hljs-string">faas-cli</span> <span class="hljs-string">describe</span> <span class="hljs-string">p1</span> <span class="hljs-string">--token</span> <span class="hljs-string">"$JWT"</span>
</code></pre>
<p>Since we have a lot of experience with GitHub Actions, we decided to make the above simpler by creating a custom <a href="https://docs.github.com/en/actions/creating-actions/creating-a-composite-action">Composite Action</a>. If you check out the code for <a href="https://github.com/self-actuated/openfaas-oidc">self-actuated/openfaas-oidc</a> you'll see that it obtains a token, then writes it into an openfaaas config file, so that the <code>--token</code> flag isn't required.</p>
<p>Here's how it changes:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/openfaas-oidc@v1</span>
  <span class="hljs-attr">with:</span> 
    <span class="hljs-attr">gateway:</span> <span class="hljs-string">https://minty.exit.o6s.io</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Check</span> <span class="hljs-string">OpenFaaS</span> <span class="hljs-string">version</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">|
    OPENFAAS_CONFIG=$HOME/.openfaas/
    faas-cli version
</span></code></pre>
<p>Here's the complete example:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">federate</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">workflow_dispatch:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">'*'</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">auth:</span>
    <span class="hljs-comment"># Add "id-token" with the intended permissions.</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">contents:</span> <span class="hljs-string">'read'</span>
      <span class="hljs-attr">id-token:</span> <span class="hljs-string">'write'</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">faas-cli</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">curl</span> <span class="hljs-string">-sLS</span> <span class="hljs-string">https://cli.openfaas.com</span> <span class="hljs-string">|</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sh</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/openfaas-oidc@v1</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">gateway:</span> <span class="hljs-string">https://minty.exit.o6s.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          export OPENFAAS_URL=https://minty.exit.o6s.io
          faas-cli store deploy env --name http-header-printer
          faas-cli list
</span></code></pre>
<p>How can we be sure that our functions cannot be invoked over the proxy?</p>
<p>Just add an extra line to test it out:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">token</span> <span class="hljs-string">and</span> <span class="hljs-string">use</span> <span class="hljs-string">the</span> <span class="hljs-string">CLI</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          export OPENFAAS_URL=https://minty.exit.o6s.io
          faas-cli store deploy env --name http-header-printer
          sleep 5
</span>
          <span class="hljs-string">echo</span> <span class="hljs-string">|</span> <span class="hljs-string">faas-cli</span> <span class="hljs-string">invoke</span> <span class="hljs-string">http-header-printer</span>
</code></pre>
<p><img src="/images/2023-05-openfaas-oidc-proxy/failed-invoke.png" alt="A failed invocation over the proxy"></p>
<blockquote>
<p>A failed invocation over the proxy</p>
</blockquote>
<p>Best of all, now that you're using OIDC, you can now go and delete any of those long lived basic auth credentials from your secrets!</p>
<h2>Wrapping up</h2>
<p>The new OIDC proxy for OpenFaaS is available for all actuated customers and works with OpenFaaS CE, Standard and Enterprise. You can use it on as many clusters as you like, whilst you have an active subscription for actuated at no extra cost.</p>
<p>In a short period of time, you can set up the Helm chart for the OIDC proxy and no longer have to worry about storing various secrets in GitHub Actions for all your clusters, simply obtain a token and use it to deploy to any cluster - securely. There's no risk that your functions will be exposed on the Internet, because the OIDC proxy only works for the <code>/system</code> endpoints of the OpenFaaS REST API.</p>
<p><strong>An alternative for those who need it</strong></p>
<p><a href="https://openfaas.com/pricing">OpenFaaS Enterprise</a> has its own OIDC integration with much more fine-grained permissions implemented. It means that team members using the CLI, Dashboard or API do not need to memorise or share basic authentication credentials with each other, or worry about getting the right password for the right cluster.</p>
<p>An OpenFaaS Enterprise policy can restrict all the way down to read/write permissions on a number of namespaces, and also integrates with OIDC.</p>
<p>See an example:</p>
<ul>
<li><a href="https://docs.openfaas.com/openfaas-pro/iam/github-actions-federation/">OpenFaaS Enterprise - IAM with GitHub Actions</a></li>
<li><a href="https://docs.openfaas.com/openfaas-pro/iam/gitlab-federation/">OpenFaaS Enterprise - IAM with GitLab</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lessons learned managing GitHub Actions and Firecracker]]></title>
            <link>https://actuated.dev/blog/managing-github-actions</link>
            <guid>https://actuated.dev/blog/managing-github-actions</guid>
            <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Alex shares lessons from building a managed service for GitHub Actions with Firecracker.]]></description>
            <content:encoded><![CDATA[<p>Over the past few months, we've launched over 20,000 VMs for customers and have handled over 60,000 webhook messages from the GitHub API. We've learned a lot from every customer PoC and from our own usage in OpenFaaS.</p>
<p>First of all - what is it we're offering? And how is it different to managed runners and the self-hosted runner that GitHub offers?</p>
<p>Actuated replicates the hosted experience you get from paying for hosted runners, and brings it to hardware under your own control. That could be a bare-metal Arm server, or a regular Intel/AMD cloud VM that has nested virtualisation enabled.</p>
<p>Just like managed runners - every time actuated starts up a runner, it's within a single-tenant virtual machine (VM), with an immutable filesystem.</p>
<p><a href="https://twitter.com/alexellisuk/status/1641471236738875399/photo/1"><img src="https://pbs.twimg.com/media/FsesfBIWAAEYjGY?format=jpg&#x26;name=large" alt="The Asahi Linux lab"></a></p>
<blockquote>
<p>Asahi Linux running on my lab of two M1 Mac Minis - used for building the Arm64 base images and Kernels.</p>
</blockquote>
<p>Can't you just use a self-hosted runner on a VM? Yes, of course you can. But it's actually more nuanced than that. The self-hosted runner <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">isn't safe</a> for OSS or public repos. And whether you run it directly on the host, or in Kubernetes - it's subject to side-effects, poor isolation, malware and in some cases uses very high privileges that could result in taking over a host completely.</p>
<p>You can learn more in the <a href="https://actuated.dev/blog/blazing-fast-ci-with-microvms">actuated announcement</a> and <a href="https://docs.actuated.dev/faq">FAQ</a>.</p>
<h2>how does it work?</h2>
<p>We run a SaaS - a managed control-plane which is installed onto your organisation as a GitHub App. At that point, we'll receive webhooks about jobs in a queued state.</p>
<p><img src="/images/2023-03-lessons-learned/conceptual.jpg" alt="Conceptual architecture"></p>
<p>As you can see in the diagram above, when a webhook is received, and we determine it's for your organisation, we'll schedule a <a href="https://firecracker-microvm.github.io/">Firecracker</a> MicroVM on one of your servers.</p>
<p>We have no access to your code or build secrets. We just obtain a registration token and send the runner a bit of metadata. Then we get out the way and let the self-hosted runner do its thing - in an isolated Kernel, with an immutable filesystem and its own Docker daemon.</p>
<p>Onboarding doesn't take very long - you can use your own servers or get them from a cloud provider. We've got a <a href="https://docs.actuated.dev/provision-server/">detailed guide</a>, but can also recommend an option on a discovery call.</p>
<p>Want to learn more about how Firecracker compares to VMs and containers? <a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch my webinar on YouTube</a></p>
<h2>Lesson 1 - GitHub's images are big and beautiful</h2>
<p>The first thing we noticed when building our actuated VM images was that the GitHub ones are huge.</p>
<p>And if you've ever tried to find out how they're built, or hoped to find a nice little Dockerfile, you can may be disappointed. The images for Linux, Windows and MacOS are built <a href="https://github.com/actions/runner-images">through a set of bespoke scripts</a>, and are hard to adapt for your own use.</p>
<p>Don't get me wrong. The scripts are very clever and they work well. GitHub have been tuning these runner images for years, and they cover a variety of different use-cases.</p>
<p>The first challenge for actuated before launching a pilot was getting enough of the most common packages installed through a Dockerfile. Most of our own internal software is built with Docker, so we can get by with quite a spartan environment.</p>
<p>We also had to adapt the sample Kernel configuration provided by the Firecracker team so that it could launch Docker and so it had everything it required to launch <a href="https://twitter.com/alexellisuk/status/1641476545695952905?s=20">Kubernetes</a>.</p>
<p><a href="https://twitter.com/alexellisuk/status/1641476545695952905/photo/1"><img src="https://pbs.twimg.com/media/FseyAyWXsAQJBCe?format=png&#x26;name=large" alt="M1s running Firecracker"></a></p>
<blockquote>
<p>Two M1 Mac Minis running Asahi Linux and four separate versions of K3s</p>
</blockquote>
<p>So by following the 80/20 principle, and focusing on the most common use-cases, we were able to launch quite quickly and cover 80% of the use-cases.</p>
<p>I don't know if you realised, things like Node.js are pre-installed in the environment, but many Node developers also add the "setup-node" action which guess what? Downloads and installs Node.js again. The same is true for many other languages and tools. We do ship Node.js and Python in the image, but the chances are that we could probably remove them at some point.</p>
<p>With one of our earliest pilots, a customer wanted to use a terraform action. It failed and I felt a bit embarrassed by the reason. We were missing unzip in our images.</p>
<p>The cure? Go and add unzip to the Dockerfile, and hit publish on our builder repository. In 3 minutes the problem was solved.</p>
<p>But GitHub Actions is also incredibly versatile and it means even if something is missing, we don't necessary have to publish a new image for you to continue your work. Just add a step to your workflow to install the missing package.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Add</span> <span class="hljs-string">unzip</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">install</span> <span class="hljs-string">-qy</span> <span class="hljs-string">unzip</span>
</code></pre>
<p>With every customer pilot we've done, there's tended to be one or two packages like this that they expected to see. For another customer it was "libpq". As a rule, if something is available in the hosted runner, we'll strongly consider adding it to ours.</p>
<h2>Lesson 2 - It’s not GitHub, it can’t be GitHub. It was GitHub</h2>
<p>Since actuated is a control-plane, a SaaS, a full-service - supported product, we are always asking first - is it us? Is it our code? Is it our infrastructure? Is it our network? Is it our hardware?</p>
<p>If you open up the <a href="https://githubstatus.com">GitHub status page</a>, you'll notice an outage almost every week - at times on consecutive days, or every few days on GitHub Actions or a service that affects them indirectly - like the package registry, Pages or Pull Requests.</p>
<p><img src="/images/2023-03-lessons-learned/outage.png" alt="Outage this week"></p>
<blockquote>
<p>The second outage this week that unfortunately affected actuated customers.</p>
</blockquote>
<p>I'm not bashing on GitHub here, we're paying a high R&#x26;D cost to build on their platform. We want them to do well.</p>
<p>But this is getting embarrassing. On a recent call, a customer told us: "it's not your solution, it looks great for us, it's the reliability of GitHub, we're worried about adopting it"</p>
<p>What can you say to that? I can't tell them that their concerns are misplaced, because they're not.</p>
<p>I reached out to Martin Woodward - Director of DevRel at GitHub. He told me that "leadership are taking this very seriously. We're doing better than we were 12 months ago."</p>
<p>GitHub is too big to fail. Let's hope they smooth out these bumps.</p>
<h2>Lesson 3 - Webhooks can be unreliable</h2>
<p>There's no good API to collect this historical data at the moment but we do have an open-source tool (<a href="https://github.com/self-actuated/actions-usage">self-actuated/actions-usage</a>) we give to customers to get a summary of their builds before they start out with us.</p>
<p>So we mirror a summary of job events from GitHub into our database, so that we can show customers trends in behaviour, and identify hot-spots on specific repos - long build times, or spikes in failure rates.</p>
<p><img src="https://pbs.twimg.com/media/FqnJ8rLXgAEnJDZ?format=png&#x26;name=large" alt="Insights chart"></p>
<blockquote>
<p>Insights chart from the actuated dashboard</p>
</blockquote>
<p>We noticed that from time to time, jobs would show in our database as "queued" or "in_progress" and we couldn't work out why. A VM had been scheduled, the job had run, and completed.</p>
<p>In some circumstances, GitHub forgot to send us an In Progress event, or they never sent us a queued event.</p>
<p>Or they sent us queued, in progress, then completed, but in the reverse order.</p>
<p>It took us longer than I'm comfortable with to track down this issue, but we've now adapted our API to handle these edge-cases.</p>
<p>Some deeper digging showed that people have also had issues with Stripe webhooks coming out of order. We saw this issue only very recently, after handling 60k webhooks - so perhaps it was a change in the system being used at GitHub?</p>
<h2>Lesson 4 - Autoscaling is hard and the API is sometimes wrong</h2>
<p>We launch a VM on your servers for every time we receive a queued event. But we have no good way of saying that a particular VM can only run for a certain job.</p>
<p>If there were five jobs queued up, then GitHub would send us five queued events, and we'd launch five VMs. But if the first job was cancelled, we'd still have all of those VMs running.</p>
<p>Why? Why can't we delete the 5th?</p>
<p>Because there is no determinism. It'd be a great improvement for user experience if we could tell GitHub's API - "great, we see you queued build X, it must run on a runner with label Y". But we can't do that today.</p>
<p>So we developed a "reaper" - a background task that tracks launched VMs and can delete them after a period of inactivity. We did have an initial issue where GitHub was taking over a minute to send a job to a ready runner, which we fixed by increasing the idle timeout value. Right now it's working really well.</p>
<p>There is still one remaining quirk where GitHub's API reports that an active runner where a job is running as idle. This happens surprisingly often - but it's not a big deal, the VM deletion call gets rejected by the GitHub API.</p>
<h2>Lesson 5 - We can launch in under one second, but what about GitHub?</h2>
<p>The way we have things tuned today, the delay from you hitting commit in GitHub, to the job executing is similar to that of hosted runners. But sometimes, GitHub lags a little - especially during an outage or when they're under heavy load.</p>
<p><img src="/images/2023-03-lessons-learned/vms_running.png" alt="VM launches"></p>
<blockquote>
<p>Grafana Cloud showing a gauge of microVMs per managed host</p>
</blockquote>
<p>There could be a delay between when you commit, and when GitHub delivers the "queued" webhook.</p>
<p>Scoring and placing a VM on your servers is very quick, then the boot time of the microVM is generally less than 1 second including starting up a dedicated Docker daemon inside the VM.</p>
<p>Then the runner has to run a configuration script to register itself on the API</p>
<p>Finally, the runner connects to a queue, and GitHub has to send it a payload to start the job.</p>
<p>On those last two steps - we see a high success rate, but occasionally, GitHub's API will fail on either of those two operations. We receive an alert via Grafana Cloud and Discord - then investigate. In the worst case, we re-queue via our API the job and the new VM will pick up the pending job.</p>
<p>Want to <a href="https://www.youtube.com/watch?v=2o28iUC-J1w">watch a demo</a>?</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2>Lesson 6 - Sometimes we need to debug a runner</h2>
<p>When I announced actuated, I heard a lot of people asking for CircleCI's debug experience, <a href="https://docs.actuated.dev/tasks/debug-ssh/">so I built something similar</a> and it's proved to be really useful for us in building actuated.</p>
<p>Only yesterday, Ivan Subotic from Dasch Swiss messaged me and said:</p>
<blockquote>
<p>"How cool!!! you don’t know how many hours I have lost on GitHub Actions without this."</p>
</blockquote>
<p>Recently there were two cases where we needed to debug a runner with an SSH shell.</p>
<p>The first was for a machine on Hetzner, where the Docker Daemon was unable to pull images due to a DNS failure. I added steps to print out <code>/etc/resolv.conf</code> and that would be my first port of call. Debugging is great, but it's slow, if an extra step in the workflow can help us diagnose the problem, it's worth it.</p>
<p>In the end, it took me about a day and a half to work out that Hetzner was blocking outgoing traffic on port 53 to Google and Cloudflare. What was worse - was that it was an intermittent problem.</p>
<p>When we did other customer PoCs on Hetzner, we did not run into this issue. I even launched a "cloud" VM in the same region and performed a couple of <code>nslookup</code>s - they worked as expected for me.</p>
<p>So I developed a <a href="https://github.com/self-actuated/hetzner-dns-action">custom GitHub Action</a> to unblock the customer:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hetzner-dns-action@v1</span>
</code></pre>
<p>Was this environmental issue with Hetzner our responsibility? Arguably not, but our customers pay us to provide a "like managed" solution, and we are currently able to help them be successful.</p>
<p>In the second case, Ivan needed to launch headless Chrome, and was using one of the many <code>setup-X</code> actions from the marketplace.</p>
<p>I opened a debug session on one of our own runners, then worked backwards:</p>
<pre><code class="hljs language-bash">curl -sLS -O https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb

dpkg -i ./google-chrome-stable_current_amd64.deb
</code></pre>
<p>This reported that some packages were missing, I got which packages by running <code>apt-get --fix-missing --no-recommends</code> and provided an example of how to add them.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">chrome:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">chrome</span>
        <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
        <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Add</span> <span class="hljs-string">extra</span> <span class="hljs-string">packages</span> <span class="hljs-string">for</span> <span class="hljs-string">Chrome</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            sudo apt install -qyyy --no-install-recommends adwaita-icon-theme fontconfig fontconfig-config fonts-liberation gtk-update-icon-cache hicolor-icon-theme humanity-icon-theme libatk-bridge2.0-0 libatk1.0-0 libatk1.0-data libatspi2.0-0 ...
</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">browser-actions/setup-chrome@v1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">run:</span> <span class="hljs-string">chrome</span> <span class="hljs-string">--version</span>
</code></pre>
<p>We could also add these to the base image by editing the Dockerfile that we maintain.</p>
<h2>Lesson 7 - Docker Hub Rate Limits are a pain</h2>
<p>Docker Hub rate limits are more of a pain on self-hosted runners than they are on GitHub's own runners.</p>
<p>I ran into this problem whilst trying to rebuild around 20 OpenFaaS Pro repositories to upgrade a base image. So after a very short period of time, all code ground to a halt and every build failed.</p>
<p>GitHub has a deal to pay Docker Inc so that you don't run into rate limits. At time of writing, you'll find a valid Docker Hub credential in the <code>$HOME/.docker/config.json</code> file on any hosted runner.</p>
<p>Actuated customers would need to login at the top of every one of their builds that used Docker, and create an organisation-level secret with a pull token from the Docker Hub.</p>
<p>We found a way to automate this, and speed up subsequent jobs by <a href="https://docs.actuated.dev/tasks/registry-mirror/">caching images directly on the customer's server</a>.</p>
<p>All they need to add to their builds is:</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>
</code></pre>
<h2>Wrapping up</h2>
<p>I hope that you've enjoyed hearing a bit about our journey so far. With every new pilot customer we learn something new, and improve the offering.</p>
<p>Whilst there was a significant amount of very technical work at the beginning of actuated, most of our time now is spent on customer support, education, and improving the onboarding experience.</p>
<p>If you'd like to know how actuated compares to hosted runners or managing the self-hosted runner on your own, we'd encourage <a href="https://actuated.dev/blog">checking out the blog</a> and <a href="https://actuated.dev/faq">FAQ</a>.</p>
<p>Are your builds slowing the team down? Do you need better organisation-level insights and reporting? Or do you need Arm support? Are you frustrated with managing self-hosted runners?</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Reach out to us take part in the pilot</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to split up multi-arch Docker builds to run natively]]></title>
            <link>https://actuated.dev/blog/how-to-run-multi-arch-builds-natively</link>
            <guid>https://actuated.dev/blog/how-to-run-multi-arch-builds-natively</guid>
            <pubDate>Fri, 24 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[QEMU is a convenient way to publish containers for multiple architectures, but it can be incredibly slow. Native is much faster.]]></description>
            <content:encoded><![CDATA[<p>In two previous articles, we covered huge improvements in performance for the Parca project and VPP (Network Service Mesh) simply by switching to actuated with Arm64 runners instead of using QEMU and hosted runners.</p>
<p>In the <a href="http://actuated.dev/blog/native-arm64-for-github-actions">first case</a>, using QEMU took over 33 minutes, and bare-metal Arm showed a 22x improvement at only 1 minute 26 seconds. For Network Service Mesh, <a href="http://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions">VPP</a> couldn't even complete a build in 6 hours using QEMU - and I got it down to 9 minutes flat using a bare-metal <a href="https://amperecomputing.com/processors/ampere-altra">Ampere Altra server</a>.</p>
<h2>What are we going to see and why is it better?</h2>
<p>In this article, I'll show you how to run multi-arch builds natively on bare-metal hardware using GitHub Actions and actuated.</p>
<p><a href="https://actuated.dev/">Actuated</a> is a SaaS service that we built so that you can Bring Your Own compute to GitHub Actions, and have every build run in an immutable, single-use VM.</p>
<p><a href="https://twitter.com/alexellisuk/status/1639232258887372801?s=20"><img src="https://pbs.twimg.com/media/Fr_CcVJWIAEY1gL?format=png&#x26;name=medium" alt="Comparison of the two builds"></a></p>
<blockquote>
<p>Comparison of splitting out to run in parallel on native hardware and QEMU.</p>
</blockquote>
<p>Not every build will see such a dramatic increase as the ones I mentioned in the introduction. Here, with the inlets-operator, we gained 4 minutes on each commit. But I often speak to users who are running past 30 minutes to over an hour because of QEMU.</p>
<p>Three things got us a speed bump here:</p>
<ul>
<li>We ran on bare-metal, native hardware - not the standard hosted runner from GitHub</li>
<li>We split up the work and ran in parallel - rather than waiting for two builds to run in serial</li>
<li>Finally, we did away with QEMU and ran the Arm build directly on an Arm server</li>
</ul>
<p>Only last week an engineer at Calyptia (the team behind <a href="https://fluentbit.io/">fluent-bit</a>) reached out for help after telling me they had to disable and stop publishing open source images for Arm, it was simply timing out at the 6 hour mark.</p>
<p>So how does this thing work, and is QEMU actually "OK"?</p>
<h2>QEMU can be slow, but it's actually "OK"</h2>
<p>So if the timings are so bad, why does anyone use QEMU?</p>
<p>Well it's free - as in beer, there's no cost at all to use it. And many builds can complete in a reasonable amount of time using QEMU, even if it's not as fast as native.</p>
<p>That's why we wrote up how we build 80+ multi-arch images for various products like OpenFaaS and Inlets:</p>
<p><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></p>
<p>Here's what the build looks like with QEMU:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">split-operator</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">qemu</span> ]

<span class="hljs-attr">jobs:</span>

  <span class="hljs-attr">publish_qemu:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-qemu</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">QEMU</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-qemu-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/amd64,linux/arm64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-qemu
</span></code></pre>
<p>This is the kind of build that is failing or causing serious delays for projects like Parca, VPP and Fluent Bit.</p>
<p>Let's look at the alternative.</p>
<h2>Building on native hardware</h2>
<p>Whilst QEMU emulates the architecture you need within a build, it's not the same as running on the real hardware. This is why we see such a big difference in performance.</p>
<p>The downside is that we have to write a bit more CI configuration and run two builds instead of one, but there is some good news - we can now run them in parallel.</p>
<p>In parallel we:</p>
<ol>
<li>We publish the x86_64 image - <code>ghcr.io/owner/repo:sha-amd64</code></li>
<li>We publish the ARM image - <code>ghcr.io/owner/repo:sha-arm64</code></li>
</ol>
<p>Then:</p>
<ol start="3">
<li>We create a manifest with its own name - <code>ghcr.io/owner/repo:sha</code></li>
<li>We annotate the manifest with the images we built earlier</li>
<li>We push the manifest</li>
</ol>
<p>In this way, anyone can pull the image with the name <code>ghcr.io/owner/repo:sha</code> and it will map to either of the two images for Arm64 or Amd64.</p>
<p><img src="/images/2023-split-native/parallel.jpg" alt="Parallel execution"></p>
<blockquote>
<p>The two builds on the left ran on two separate bare-metal hosts, and the manifest was published using one of GitHub's hosted runners.</p>
</blockquote>
<p>Here's a sample for the inlets-operator, a Go binary which connects to the Kubernetes API.</p>
<p>First up, we have the x86 build:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">split-operator</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span> ]

<span class="hljs-attr">jobs:</span>

  <span class="hljs-attr">publish_x86:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-x86</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">mirror</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/amd64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64
</span></code></pre>
<p>Then we have the arm64 build which is almost identical, but we specify a different value for <code>platforms</code> and the <code>runs-on</code> field.</p>
<pre><code class="hljs language-yaml">
  <span class="hljs-attr">publish_aarch64:</span>
    <span class="hljs-attr">concurrency:</span> 
      <span class="hljs-attr">group:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.ref</span> <span class="hljs-string">}}-aarch64</span>
      <span class="hljs-attr">cancel-in-progress:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">permissions:</span>
      <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-aarch64</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">inlets/inlets-operator</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">"./"</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">mirror</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/hub-mirror@master</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Release</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">release_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">platforms:</span> <span class="hljs-string">linux/arm64</span>
          <span class="hljs-attr">file:</span> <span class="hljs-string">./Dockerfile</span>
          <span class="hljs-attr">context:</span> <span class="hljs-string">.</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            Version=dev
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
</span></code></pre>
<p>Finally, we need to create the manifest. GitHub Actions has a <code>needs</code> variable that we can set to control the execution order:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-attr">publish_manifest:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">needs:</span> [<span class="hljs-string">publish_x86</span>, <span class="hljs-string">publish_aarch64</span>]
    <span class="hljs-attr">steps:</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Get</span> <span class="hljs-string">Repo</span> <span class="hljs-string">Owner</span>
      <span class="hljs-attr">id:</span> <span class="hljs-string">get_repo_owner</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">echo</span> <span class="hljs-string">"REPO_OWNER=$(echo $<span class="hljs-template-variable">{{ github.repository_owner }}</span> | tr '[:upper:]' '[:lower:]')"</span> <span class="hljs-string">></span> <span class="hljs-string">$GITHUB_ENV</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">container</span> <span class="hljs-string">Registry</span>
      <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
        <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">manifest</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        docker manifest create ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} \
          --amend ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64 \
          --amend ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
        docker manifest annotate --arch amd64 --os linux ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-amd64
        docker manifest annotate --arch arm64 --os linux ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }} ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}-aarch64
        docker manifest inspect ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}
</span>
        <span class="hljs-string">docker</span> <span class="hljs-string">manifest</span> <span class="hljs-string">push</span> <span class="hljs-string">ghcr.io/${{</span> <span class="hljs-string">env.REPO_OWNER</span> <span class="hljs-string">}}/inlets-operator:${{</span> <span class="hljs-string">github.sha</span> <span class="hljs-string">}}</span>
</code></pre>
<p>One thing I really dislike about this final stage is how much repetition we get. Fortunately, it's relatively simple to hide this complexity behind a custom GitHub Action.</p>
<p>Note that this is just an example at the moment, but I could make a custom composite action in Bash in about 30 minutes, including testing. So it's not a lot of work and it would make our whole workflow a lot less repetitive.</p>
<pre><code class="hljs language-yaml">   <span class="hljs-attr">uses:</span> <span class="hljs-string">self-actuated/compile-manifest@master</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">ghcr.io/${{</span> <span class="hljs-string">env.REPO_OWNER</span> <span class="hljs-string">}}/inlets-operator</span>
      <span class="hljs-attr">sha:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.sha</span> <span class="hljs-string">}}</span>
      <span class="hljs-attr">platforms:</span> <span class="hljs-string">amd64,arm64</span>
</code></pre>
<p>As a final note, we recently saw that with upgrading from <code>docker/build-push-action@v3</code> to <code>docker/build-push-action@v4</code>, buildx no longer publishes an image, but a manifest for each architecture. This is because a new "provenance" feature is enabled which under the hood is publishing multiple artifacts instead of a single image. We've turned this off with <code>provenance: false</code> and <a href="https://github.com/docker/build-push-action/issues/755#issuecomment-1607792956">are awaiting a response from Docker</a> on how to enable provenance for multi-arch images built with a split build.</p>
<h2>Wrapping up</h2>
<p>Yesterday we took a new customer on for actuated who wanted to improve the speed of Arm builds, but on the call we both knew they would need to leave QEMU behind. I put this write-up together to show what would be involved, and I hope it's useful to you.</p>
<p>Where can you run these builds?</p>
<p>Couldn't you just add a low-cost Arm VM from AWS, Oracle Cloud, Azure or Google Cloud?</p>
<p>The answer unfortunately is no.</p>
<p>The self-hosted runner is not suitable for open source / public repositories, the GitHub documentation has a stark warning about this.</p>
<p>The Kubernetes controller that's available has the same issues, because it re-uses the Pods by default, and runs in a dangerous Docker In Docker Mode as a privileged container or by mounting the Docker Socket. I'm not sure which is worse, but both mean that code in CI can take over the host, potentially even the whole cluster.</p>
<p>Hosted runners solve this by creating a fresh VM per job, and destroying it immediately. That's the same approach that we took with actuated, but you get to bring your own metal along, so that you keep costs from growing out of control. Actuated also supports Arm, out of the box.</p>
<p>Want to know more about the security of self-hosted runners? <a href="https://docs.actuated.dev/faq/">Read more in our FAQ</a>.</p>
<p>Want to talk to us about your CI/CD needs? We're happy to help.</p>
<ul>
<li><a href="/pricing#sign-up-for-the-pilot">Contact us about a PoC</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bring Your Own Metal Case Study with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions</link>
            <guid>https://actuated.dev/blog/case-study-bring-your-own-bare-metal-to-actions</guid>
            <pubDate>Fri, 10 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[See how BYO bare-metal made a 6 hour GitHub Actions build complete 25x faster.]]></description>
            <content:encoded><![CDATA[<p>I'm going to show you how both a regular x86_64 build and an Arm build were made dramatically faster by using Bring Your Own (BYO) bare-metal servers.</p>
<p>At the early stage of a project, GitHub's standard runners with 2x cores, 8GB RAM, and a little free disk space are perfect because they're free for public repos. For private repos they come in at a modest cost, if you keep your usage low.</p>
<p>What's not to love?</p>
<p>Well, <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke</a>, Distinguished Engineer at Cisco contacted me a few weeks ago and told me about the VPP project, and some of the problems he was running into trying to build it with hosted runners.</p>
<blockquote>
<p>The Fast Data Project (FD.io) is an open-source project aimed at providing the world's fastest and most secure networking data plane through <a href="https://fd.io/">Vector Packet Processing (VPP)</a>.</p>
</blockquote>
<p>Whilst VPP can be used as a stand-alone project, it is also a key component in the <a href="https://www.cncf.io/">Cloud Computing Foundation's (CNCF's)</a> Open Source <a href="https://networkservicemesh.io/">Network Service Mesh</a> project.</p>
<p>There were two issues:</p>
<ol>
<li>
<p>The x86_64 build was taking 1 hour 25 minutes on a standard runner.</p>
<p>Why is that a problem? CI is meant to both validate against regression, but to build binaries for releases. If that process can take 50 minutes before failing, it's incredibly frustrating. For an open source project, it's actively hostile to contributors.</p>
</li>
<li>
<p>The Arm build was hitting the 6 hour limit for GitHub Actions then failing</p>
<p>Why? Well it was using <a href="https://www.qemu.org/">QEMU</a>, and I've spoken about this in the past - QEMU is a brilliant, zero cost way to build Arm binaries on a regular machine, but it's slow. And you'll see just how slow in the examples below, including where my Raspberry Pi beat a GitHub runner.</p>
</li>
</ol>
<p>We explain how to use QEMU in Docker Actions in the following blog post:</p>
<p><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></p>
<h2>Rubbing some bare-metal on it</h2>
<p>So GitHub does actually have a beta going for "larger runners", and if Ed wanted to try that out, he'd have to apply to a beta waitlist, upgrade to a Team or Enterprise Plan, and then pick a new runner size.</p>
<p>But that wouldn't have covered him for the Arm build, GitHub don't have any support there right now. I'm sure it will come one, day, but here we are unable to release binaries for our Arm users.</p>
<p>With actuated, we have no interest in competing with GitHub's business model of selling compute on demand. We want to do something more unique than that - we want to enable you to bring your own (BYO) devices and then use them as runners, with VM-level isolation and one-shot runners.</p>
<blockquote>
<p>What does Bring Your Own (BYO) mean?</p>
<p>"Your Own" does not have to mean physical ownership. You do not need to own a datacenter, or to send off a dozen Mac Minis to a Colo.
You can provision bare-metal servers on AWS or with Equinix Metal as quickly as you can get an EC2 instance.
Actually, bare-metal isn't strictly needed at all, and even DigitalOcean's and Azure's VMs will work with actuated because they support KVM, which we use to launch Firecracker.</p>
</blockquote>
<p>And who is behind actuated? We are a nimble team, but have a pedigree with Cloud Native and self-hosted software going back 6-7 years from <a href="https://openfaas.com/">OpenFaaS</a>. OpenFaaS is a well known serverless platform which is used widely in production by commercial companies including Fortune 500s.</p>
<p>Actuated uses a Bring Your Own (BYO) server model, but there's very little for you to do once you've installed the actuated agent.</p>
<p>Here's how to set up the agent software: <a href="https://docs.actuated.dev/install-agent/">Actuated Docs: Install the Agent</a>.</p>
<p>You then get detailed stats about each runner, the build queue and insights across your whole GitHub organisation, in one place:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Actuated now aggregates usage data at the organisation level, so you can get insights and spot changes in behaviour.<br><br>This peak of 57 jobs was when I was quashing CVEs for <a href="https://twitter.com/openfaas?ref_src=twsrc%5Etfw">@openfaas</a> Pro customers in Alpine Linux and a bunch of Go <a href="https://t.co/a84wLNYYjo">https://t.co/a84wLNYYjo</a>… <a href="https://t.co/URaxgMoQGW">https://t.co/URaxgMoQGW</a> <a href="https://t.co/IuPQUjyiAY">pic.twitter.com/IuPQUjyiAY</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1633059062639108096?ref_src=twsrc%5Etfw">March 7, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2>First up - x86_64</h2>
<p>I forked Ed's repo into the "actuated-samples" repo, and edited the "runs-on:" field from "ubuntu-latest" to "actuated".</p>
<p>The build which previously took 1 hour 25 minutes now took 18 minutes 58 seconds. That's a 4.4x improvement.</p>
<p><img src="/images/2023-03-vpp/x86.png" alt="Improvements over the standard runner"></p>
<p>4.4x doesn't sound like a big number, but look at the actual number.</p>
<p>It used to take well over an hour to get feedback, now you get it in less than 20 minutes.</p>
<p>And for context, this x86_64 build took 17 minutes to build on Ed's laptop, with some existing caches in place.</p>
<p>I used an <a href="https://deploy.equinix.com/product/servers/m3-small/">Equinix Metal m3.small.x86 server</a>, which has 8x Intel Xeon E-2378G cores @ 2.8 GHz. It also comes with a local SSD, local NVMe would have been faster here.</p>
<p>The Firecracker VM that was launched had 12GB of RAM and 8x vCPUs allocated.</p>
<h2>Next up - Arm</h2>
<p>For the Arm build I created a new branch and had to change a few hard-coded references from "_amd64.deb" to "_arm64.deb" and then I was able to run the build. This is common enablement work. I've been doing Arm enablement for Cloud Native and OSS since 2015, so I'm very used to spotting this kind of thing.</p>
<p>So the build took 6 hours, and didn't even complete when running with QEMU.</p>
<p>How long did it take on bare-metal? 14 minutes 28 seconds.</p>
<p><img src="/images/2023-03-vpp/arm64.png" alt="Improvements over QEMU"></p>
<p>That's a 25x improvement.</p>
<p>The Firecracker VM that we launched had 16GB of RAM and 8x vCPUs allocated.</p>
<p>It was running on a Mac Mini M1 configured with 16GB RAM, running with Asahi Linux. I bought it for development and testing, as a one-off cost, and it's a very fast machine.</p>
<p>But, this case-study is <em>not</em> specifically about using consumer hardware, or hardware plugged in under your desk.</p>
<p>Equinix Metal and Hetzner both have the Ampere Altra bare-metal server available on either an hourly or monthly basis, and AWS customers can get access to the a1.metal instance on an hourly basis too.</p>
<blockquote>
<p>To prove the point, that BYO means cloud servers, just as much as physically owned machines, <a href="https://github.com/actuated-samples/govpp/actions/runs/4391240138">I also ran the same build on an Ampere Altra from Equinix Metal</a> with 20 GB of RAM, and 32 vCPUs, it completed in 9 minutes 39 seconds.</p>
</blockquote>
<p>See our hosting recommendations: <a href="https://docs.actuated.dev/provision-server/">Actuated Docs: Provision a Server</a></p>
<p>In October last year, I benchmarked a Raspberry Pi 4 as an actuated server and pitted it directly against QEMU and GitHub's Hosted runners.</p>
<p>It was 24 minutes faster. That's how bad using QEMU can be instead of using bare-metal Arm.</p>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">Then, just for run I scheduled the MicroVM on my <a href="https://twitter.com/Raspberry_Pi?ref_src=twsrc%5Etfw">@Raspberry_Pi</a> instead of an <a href="https://twitter.com/equinixmetal?ref_src=twsrc%5Etfw">@equinixmetal</a> machine.<br><br>Poor little thing has 8GB RAM and 4 Cores with an SSD connected over USB-C.<br><br>Anyway, it still beat QEMU by 24 minutes! <a href="https://t.co/ITyRpbnwEE">pic.twitter.com/ITyRpbnwEE</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583092051398524928?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2>Wrapping up</h2>
<p>So, wrapping up - if you only build x86_64, and have very few build minutes, and are willing to upgrade to a Team or Enterprise Plan on GitHub, "faster runners" may be an option you want to consider.</p>
<p>If you don't want to worry about how many minutes you're going to use, or surprise bills because your team got more productive, or grew in size, or is finally running those 2 hour E2E tests every night, then actuated may be faster and better value overall for you.</p>
<p>But if you need Arm runners, and <a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">want to use them with public repos</a>, then there are not many options for you which are going to be secure and easy to manage.</p>
<h3>A recap on the results</h3>
<p><a href="https://twitter.com/alexellisuk/status/1634128821124313091?s=20"><img src="/images/2023-03-vpp/background.jpg" alt="The improvement on the x86 build"></a></p>
<blockquote>
<p><a href="https://twitter.com/alexellisuk/status/1634128821124313091?s=20">The improvement on the x86 build</a></p>
</blockquote>
<p>You can see the builds here:</p>
<p>x86_64 - 4.4x improvement</p>
<ul>
<li>Before: 1 hour 25 minutes - <a href="https://github.com/edwarnicke/govpp/actions/runs/3622982661">x86_64 build on a hosted runner</a></li>
<li>After: 18 minutes - 58 seconds <a href="https://github.com/actuated-samples/govpp/actions/runs/4383082399">x86_64 build on Equinix Metal</a></li>
</ul>
<p>Arm - 25x improvement</p>
<ul>
<li>Before: 6 hours (and failing) - <a href="https://github.com/edwarnicke/govpp/actions/runs/3643464160">Arm/QEMU build on a hosted runner</a></li>
<li>After: 14 minutes 28 seconds - <a href="https://github.com/actuated-samples/govpp/actions/runs/4383475307">Arm build on Mac Mini M1</a></li>
</ul>
<h3>Want to work with us?</h3>
<p>Want to get in touch with us and try out actuated for your team?</p>
<p>We're looking for pilot customers who want to speed up their builds, or make self-hosted runners simpler to manager, and ultimately, about as secure as they're going to get with MicroVM isolation.</p>
<p>Set up a 30 min call with me to ask any questions you may have and find out next steps.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the actuated pilot</a></li>
</ul>
<p>Learn more about how it compares to other solutions in the FAQ: <a href="https://docs.actuated.dev/faq">Actuated FAQ</a></p>
<p>See also:</p>
<ul>
<li><a href="https://actuated.dev/blog/multi-arch-docker-github-actions">The efficient way to publish multi-arch containers from GitHub Actions</a></li>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></li>
<li><a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to run KVM guests in your GitHub Actions]]></title>
            <link>https://actuated.dev/blog/kvm-in-github-actions</link>
            <guid>https://actuated.dev/blog/kvm-in-github-actions</guid>
            <pubDate>Fri, 17 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[From building cloud images, to running NixOS tests and the android emulator, we look at how and why you'd want to run a VM in GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>GitHub's hosted runners do not support nested virtualization. This means some frequently used tools that require KVM like packer, the Android emulator, etc can not be used in GitHub Actions CI pipelines.</p>
<p>We noticed there are quite a few issues for people requesting KVM support for GitHub Actions:</p>
<ul>
<li><a href="https://github.com/actions/runner-images/issues/183">Enable nested virtualization · Issue #183 · actions/runner-images</a></li>
<li><a href="https://github.com/community/community/discussions/8305">Revisiting KVM support for Hosted GitHub Actions · Discussion #8305 · community/community</a></li>
<li><a href="https://github.com/WikiWatershed/model-my-watershed/pull/3586">[Experimental] Add CI GitHub Actions workflow by rajadain · Pull Request #3586 · WikiWatershed/model-my-watershed</a></li>
</ul>
<p>As mentioned in some of these issues, an alternative would be to run your own self-hosted runner on a bare metal host. This comes with the downside that builds can conflict and cause side effects to system-level packages. On top if this self-hosted runners are considered <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">insecure for public repositories</a>.</p>
<p>Solutions like the "actions-runtime-controller" or ARC that use Kubernetes to orchestrate and run self-hosted runners in Pods are also out of scope if you need to run VMs.</p>
<p>With Actuated we make it possible to launch a Virtual Machine (VM) within a GitHub Action. Jobs are launched in isolated VMs just like GitHub hosted runners but with support for nested virtualization.</p>
<h2>Case Study: Githedgehog</h2>
<p>One of our customers Sergei Lukianov, founding engineer at <a href="https://githedgehog.com">Githedgehog</a> told us he needed somewhere to build Docker images and to test them with Kubernetes, he uses KinD for that.</p>
<p>Prior to adopting Actuated, his team used hosted runners which are considerably slower, and paid on a per minute basis. Actuated made his builds both faster, and more secure than using any of the alternatives for self-hosted runners.</p>
<p>It turned out that he also needed to launch VMs in those jobs, and that's something else that hosted runners cannot cater for right now. Actuated’s KVM guest support means he can run all of his workloads on fast hardware.</p>
<p>Some other common use cases that require KVM support on the CI runner:</p>
<ul>
<li>Running <a href="https://www.packer.io/">Packer</a> for creating Amazon Machine Images (AMI) or VM images for other cloud platforms.</li>
<li>Accelerating the <a href="https://developer.android.com/studio/run/emulator-commandline">Android Emulator</a> via KVM.</li>
<li>Running <a href="https://nixos.org/">NixOS</a> tests or builds that depend on VMs.</li>
<li>Testing software that can only be done with <a href="https://www.linux-kvm.org/page/Main_Page">KVM</a> or in a VM.</li>
</ul>
<h2>Running VMs in GitHub Actions</h2>
<p>In this section we will walk you through a couple of hands-on examples.</p>
<h3>Firecracker microVM</h3>
<p>In this example we are going to follow the Firecracker quickstart guide to boot up a Firecracker VM but instead of running it on our local machine we will run it from within a GitHub Actions workflow.</p>
<p>The workflow instals Firecracker, configures and boots a guest VM and then waits 20 seconds before shutting down the VM and exiting the workflow. The image below shows the run logs of the workflow. We see the login prompt of the running microVM.</p>
<p><img src="/images/2023-02-17-kvm-in-github-actions/nested-firecracker.png" alt="Running a firecracker microVM in a GitHub Actions job"></p>
<blockquote>
<p>Running a firecracker microVM in a GitHub Actions job</p>
</blockquote>
<p>Here is the workflow file used by this job:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">run-vm</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">vm-run:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated-4cpu-8gb</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">firecracker</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          sudo arkade system install firecracker
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">microVM</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">sudo</span> <span class="hljs-string">-E</span> <span class="hljs-string">./run-vm.sh</span>
</code></pre>
<p>The <a href="https://github.com/alexellis/setup-arkade">setup-arkade</a> is to install arkade on the runner. Next firecracker is installed from the arkade system apps.</p>
<p>As a last step we run a firecracker microVM. The <code>run-vm.sh</code> script is based on the <a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/getting-started.md">firecracker quickstart</a> and collects all the steps into a single script that can be run in the CI pipeline.</p>
<p>It script will:</p>
<ul>
<li>Get the kernel and rootfs for the microVM</li>
<li>Start fireckracker and configure the guest kernel and rootfs</li>
<li>Start the guest machine</li>
<li>Wait for 20 seconds and kill the firecracker process so workflow finishes.</li>
</ul>
<p>The <code>run-vm.sh</code> script:</p>
<pre><code class="hljs language-sh"><span class="hljs-meta">#!/bin/bash</span>

<span class="hljs-comment"># Get a kernel and rootfs</span>
<span class="hljs-built_in">arch</span>=`<span class="hljs-built_in">uname</span> -m`
dest_kernel=<span class="hljs-string">"hello-vmlinux.bin"</span>
dest_rootfs=<span class="hljs-string">"hello-rootfs.ext4"</span>
image_bucket_url=<span class="hljs-string">"https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/<span class="hljs-variable">$arch</span>"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    kernel=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/kernels/vmlinux.bin"</span>
    rootfs=<span class="hljs-string">"<span class="hljs-variable">${image_bucket_url}</span>/rootfs/bionic.rootfs.ext4"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$kernel</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_kernel</span> <span class="hljs-variable">$kernel</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading <span class="hljs-variable">$rootfs</span>..."</span>
curl -fsSL -o <span class="hljs-variable">$dest_rootfs</span> <span class="hljs-variable">$rootfs</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">"Saved kernel file to <span class="hljs-variable">$dest_kernel</span> and root block device to <span class="hljs-variable">$dest_rootfs</span>."</span>

<span class="hljs-comment"># Start firecracker</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Starting firecracker"</span>
firecracker --api-sock /tmp/firecracker.socket &#x26;
firecracker_pid=$!

<span class="hljs-comment"># Set the guest kernel and rootfs</span>
rch=`<span class="hljs-built_in">uname</span> -m`
kernel_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-vmlinux.bin"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"x86_64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">elif</span> [ <span class="hljs-variable">${arch}</span> = <span class="hljs-string">"aarch64"</span> ]; <span class="hljs-keyword">then</span>
    curl --unix-socket /tmp/firecracker.socket -i \
      -X PUT <span class="hljs-string">'http://localhost/boot-source'</span>   \
      -H <span class="hljs-string">'Accept: application/json'</span>           \
      -H <span class="hljs-string">'Content-Type: application/json'</span>     \
      -d <span class="hljs-string">"{
            \"kernel_image_path\": \"<span class="hljs-variable">${kernel_path}</span>\",
            \"boot_args\": \"keep_bootcon console=ttyS0 reboot=k panic=1 pci=off\"
       }"</span>
<span class="hljs-keyword">else</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Cannot run firecracker on <span class="hljs-variable">$arch</span> architecture!"</span>
    <span class="hljs-built_in">exit</span> 1
<span class="hljs-keyword">fi</span>

rootfs_path=$(<span class="hljs-built_in">pwd</span>)<span class="hljs-string">"/hello-rootfs.ext4"</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/drives/rootfs'</span> \
  -H <span class="hljs-string">'Accept: application/json'</span>           \
  -H <span class="hljs-string">'Content-Type: application/json'</span>     \
  -d <span class="hljs-string">"{
        \"drive_id\": \"rootfs\",
        \"path_on_host\": \"<span class="hljs-variable">${rootfs_path}</span>\",
        \"is_root_device\": true,
        \"is_read_only\": false
   }"</span>

<span class="hljs-comment"># Start the guest machine</span>
curl --unix-socket /tmp/firecracker.socket -i \
  -X PUT <span class="hljs-string">'http://localhost/actions'</span>       \
  -H  <span class="hljs-string">'Accept: application/json'</span>          \
  -H  <span class="hljs-string">'Content-Type: application/json'</span>    \
  -d <span class="hljs-string">'{
      "action_type": "InstanceStart"
   }'</span>

<span class="hljs-comment"># Kill the firecracker process to exit the workflow</span>
<span class="hljs-built_in">sleep</span> 20
<span class="hljs-built_in">kill</span> -9 <span class="hljs-variable">$firecracker_pid</span>

</code></pre>
<p>The full example can be found on <a href="https://github.com/skatolo/nested-firecracker">GitHub</a></p>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker you can watch Alex's webinar on the topic.</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h3>NixOS integration tests</h3>
<p>With nix there is the ability to provide a set of declarative configuration to define integration tests that spin up virtual machines using <a href="https://www.qemu.org/">QEMU</a> as the backend. While running these tests in CI without hardware acceleration is supported this is considerably slower.</p>
<p>For a more detailed overview of the test setup and configuration see the original tutorial on nix.dev:</p>
<ul>
<li><a href="https://nix.dev/tutorials/nixos/build-and-deploy/integration-testing-using-virtual-machines">Integration testing using virtual machines (VMs)</a></li>
</ul>
<p>The workflow file for running NixOS tests on  GitHub Actions:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">nixos-tests</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">nixos-test:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">actuated</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-python@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">python-version:</span> <span class="hljs-string">'3.x'</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">cachix/install-nix-action@v16</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">extra_nix_config:</span> <span class="hljs-string">"system-features = nixos-test benchmark big-parallel kvm"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NixOS</span> <span class="hljs-string">test</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">nix</span> <span class="hljs-string">build</span> <span class="hljs-string">-L</span> <span class="hljs-string">.#checks.x86_64-linux.postgres</span>
</code></pre>
<p>We just install Nix using the <a href="https://github.com/cachix/install-nix-action">install-nix-action</a> and run the tests in the next step.</p>
<p>The full example is available on <a href="https://github.com/skatolo/gh-actions-nixos-tests">GitHub</a></p>
<h3>Other examples of using a VM</h3>
<p>In the previous section we showed you some brief examples for the kind of workflows you can run. Here are some other resources and tutorials that should be easy to adapt and run in CI.</p>
<ul>
<li>Create KVM virtual machine images with the <a href="https://developer.hashicorp.com/packer/plugins/builders/qemu">Packer QEMU Builder</a></li>
<li>Launching an Ubuntu cloud image with cloud-init: <a href="https://fabianlee.org/2020/02/23/kvm-testing-cloud-init-locally-using-kvm-for-an-ubuntu-cloud-image/">KVM: Testing cloud-init locally using KVM for an Ubuntu cloud image</a></li>
</ul>
<h2>Conclusion</h2>
<p>Hosted runners do not support nested virtualization. That makes them unsuitable for running CI jobs that require KVM support.</p>
<p>For Actuated runners we provide a custom Kernel that enables KVM support. This will allow you to run Virtual Machines within your CI jobs.</p>
<p>At time of writing there is no support for aarch64 runners. Only Intel and AMD CPUs support nested virtualisation.</p>
<p>While it is possible to deploy your own self-hosted runners to run jobs that need KVM support, this is not recommended:</p>
<ul>
<li><a href="https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions">Is the GitHub Actions self-hosted runner safe for Open Source?</a></li>
</ul>
<p>Want to see a demo or talk to our team? <a href="https://forms.gle/8XmpTTWXbZwWkfqT6">Contact us here</a></p>
<p>Just want to try it out instead? <a href="/pricing">Register your GitHub Organisation and set-up a subscription</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Make your builds run faster with Caching for GitHub Actions]]></title>
            <link>https://actuated.dev/blog/caching-in-github-actions</link>
            <guid>https://actuated.dev/blog/caching-in-github-actions</guid>
            <pubDate>Fri, 10 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how we made a Golang project build 4x faster using GitHub's built-in caching mechanism.]]></description>
            <content:encoded><![CDATA[<p>GitHub provides a <a href="https://github.com/actions/cache">cache action</a> that allows caching dependencies and build outputs to improve workflow execution time.</p>
<p>A common use case would be to cache packages and dependencies from tools such as npm, pip, Gradle, ... . If you are using Go, caching go modules and the build cache can save you a significant amount of build time as we will see in the next section.</p>
<p>Caching can be configured manually, but a lot of setup actions already use the <a href="https://github.com/actions/cache">actions/cache</a> under the hood and provide a configuration option to enable caching.</p>
<p>We use the actions cache to speed up workflows for building the Actuated base images. As part of those workflows we build a kernel and then a rootfs. Since the kernel’s configuration is changed infrequently it makes sense to cache that output.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/build-time-comparison.png" alt="Build time comparison"></p>
<blockquote>
<p>Comparing workflow execution times with and without caching.</p>
</blockquote>
<p>Building the kernel takes around <code>1m20s</code> on our aarch-64 Actuated runner and <code>4m10s</code> for the x86-64 build so we get some significant time improvements by caching the kernel.</p>
<p>The output of the cache action can also be used to do something based on whether there was a cache hit or miss. We use this to skip the kernel publishing step when there was a cache hit.</p>
<pre><code class="hljs language-yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">if:</span> <span class="hljs-string">${{</span> <span class="hljs-string">steps.cache-kernel.outputs.cache-hit</span> <span class="hljs-type">!=</span> <span class="hljs-string">'true'</span> <span class="hljs-string">}}</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">Kernel</span>
  <span class="hljs-attr">run:</span> <span class="hljs-string">make</span> <span class="hljs-string">publish-kernel-x86-64</span>
</code></pre>
<h2>Caching Go dependency files and build outputs</h2>
<p>In this minimal example we are going to setup caching for Go dependency files and build outputs. As an example we will be building <a href="https://github.com/alexellis/registry-creds">alexellis/registry-creds</a>. This is a Kubernetes operator that can be used to replicate Kubernetes ImagePullSecrets to all namespaces.</p>
<p>It has the K8s API as a dependency which is quite large so we expect to save some time by cashing the Go mod download. By also caching the Go build cache it should be possible to speed up the workflow even more.</p>
<h3>Configure caching manually</h3>
<p>We will first create the workflow and run it without any caching.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">ci</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">build:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">repository:</span> <span class="hljs-string">"alexellis/registry-creds"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</span></code></pre>
<p>The <a href="https://github.com/actions/checkout">checkout action</a> is used to check out the registry-creds repo so the workflow can access it. The next step sets up Go using the <a href="https://github.com/actions/setup-go">setup-go action</a> and as a last step we run <code>go build</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/no-cache-workflow.png" alt="No cache workflow run"></p>
<p>When triggering this workflow we see that each run takes around <code>1m20s</code>.</p>
<p>Modify the workflow and add an additional step to configure the caches using the <a href="https://github.com/actions/cache">cache action</a>:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">steps:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-go@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">go-version:</span> <span class="hljs-string">~1.19</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">Golang</span> <span class="hljs-string">caches</span>
    <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/cache@v3</span>
    <span class="hljs-attr">with:</span>
      <span class="hljs-attr">path:</span> <span class="hljs-string">|
        ~/.cache/go-build
        ~/go/pkg/mod
</span>      <span class="hljs-attr">key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">runner.os</span> <span class="hljs-string">}}-golang-${{</span> <span class="hljs-string">hashFiles('**/go.sum')</span> <span class="hljs-string">}}</span>
      <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
        ${{ runner.os }}-golang-
</span></code></pre>
<p>The <code>path</code> parameter is used to set the paths on the runner to cache or restore. The <code>key</code> parameter sets the key used when saving the cache. A hash of the go.sum file is used as part of the cache key.</p>
<p>Optionally the restore-keys are used to find and restore a cache if there was no hit for the key. In this case we always restore the cache even if there was no specific hit for the go.sum file.</p>
<p>The first time this workflow is run the cache is not populated so we see a similar execution time as without any cache of around <code>1m20s</code>.</p>
<p><img src="/images/2023-02-10-caching-in-github-actions/workflow-cache-comparison.png" alt="Comparing workflow runs"></p>
<p>Running the workflow again we can see that it now completes in just <code>18s</code>.</p>
<h3>Use setup-go built-in caching</h3>
<p>The V3 edition of the <a href="https://github.com/actions/setup-go">setup-go</a> action has support for caching built-in. Under the hood it also uses the <a href="https://github.com/actions/cache">actions/cache</a> with a similar configuration as in the example above.</p>
<p>The advantage of using the built-in functionality is that it requires less configuration settings. Caching can be enabled by adding a single line to the workflow configuration:</p>
<pre><code class="hljs language-diff">name: ci

on: push

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          repository: "alexellis/registry-creds"
      - name: Setup Golang
        uses: actions/setup-go@v3
        with:
          go-version: ~1.19
<span class="hljs-addition">+         cache: true</span>
      - name: Build
        run: |
          CGO_ENABLED=0 GO111MODULE=on \
          go build -ldflags "-s -w -X main.Release=dev -X main.SHA=dev" -o controller
</code></pre>
<p>Triggering the workflow with the build-in cache yields similar time gains as with the manual cache configuration.</p>
<h2>Conclusion</h2>
<p>We walked you through a short example to show you how to set up caching for a Go project and managed to build the project 4x faster.</p>
<p>If you are building with Docker you can use <a href="https://docs.docker.com/build/ci/github-actions/examples/#cache">Docker layer caching</a> to make your builds faster. Buildkit automatically caches the build results and allows exporting the cache to an external location. It has support for <a href="https://docs.docker.com/build/cache/backends/">uploading the build cache to GitHub Actions cache</a></p>
<p>See also: <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows">GitHub: Caching dependencies in Workflows</a></p>
<p>Keep in mind that there are some limitations to the GitHub Actions cache. Cache entries that have not been accessed in over 7 days will be removed. There is also a limit on the total cache size of 10 GB per repository.</p>
<p>Some points to take away:</p>
<ul>
<li>Using the actions cache is not limited to GitHub hosted runners but can be used with self-hosted runners as well.</li>
<li>Workflows using the cache action can be converted to run on Actuated runners without any modifications.</li>
<li>Jobs on Actuated runners start in a clean VM each time. This means dependencies need to be downloaded and build artifacts or caches rebuilt each time. Caching these files in the actions cache can improve workflow execution time.</li>
</ul>
<blockquote>
<p>Want to learn more about Go and GitHub Actions?</p>
<p>Alex's eBook <a href="https://openfaas.gumroad.com/l/everyday-golang">Everyday Golang</a> has a chapter dedicated to building Go programs with Docker and GitHub Actions.</p>
</blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The efficient way to publish multi-arch containers from GitHub Actions]]></title>
            <link>https://actuated.dev/blog/multi-arch-docker-github-actions</link>
            <guid>https://actuated.dev/blog/multi-arch-docker-github-actions</guid>
            <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to publish container images for both Arm and Intel machines from GitHub Actions.]]></description>
            <content:encoded><![CDATA[<p>In 2017, I wrote an article on <a href="https://docs.docker.com/build/building/multi-stage/">multi-stage builds with Docker</a>, and it's now part of the Docker Documentation. In my opinion, multi-arch builds were the proceeding step in the evolution of container images.</p>
<h2>What's multi-arch and why should you care?</h2>
<p>If you want users to be able to use your containers on different types of computer, then you'll often need to build different versions of your binaries and containers.</p>
<p>The <a href="https://github.com/openfaas/faas-cli">faas-cli</a> tool is how users interact with <a href="https://github.com/openfaas/faas">OpenFaaS</a>.</p>
<p>It's distributed in binary format for users, with builds for Windows, MacOS and Linux.</p>
<ul>
<li><code>linux/amd64</code>, <code>linux/arm64</code>, <code>linux/arm/v7</code></li>
<li><code>darwin/amd64</code>, <code>darwin/arm64</code></li>
<li><code>windows/amd64</code></li>
</ul>
<p>But why are there six different binaries for three Operating Systems? With the advent of Raspberry Pi, M1 Macs (Apple Silicon) and AWS Graviton servers, we have had to start building binaries for more than just Intel systems.</p>
<p>If you're curious how to build multi-arch binaries with Go, you can check out the release process for the open source arkade tool here, which is a simpler example than faas-cli: <a href="https://github.com/alexellis/arkade/blob/master/Makefile">arkade Makefile</a> and <a href="https://github.com/alexellis/arkade/blob/master/.github/workflows/publish.yml">GitHub Actions publish job</a></p>
<p>So if we have to support at least six different binaries for Open Source CLIs, what about container images?</p>
<h2>Do we need multi-arch containers too?</h2>
<p>Until recently, it was common to hear people say: "I can't find any containers that work for Arm". This was because the majority of container images were built only for Intel. Docker Inc has done a sterling job of making their "official" images work on different platforms, that's why you can now run <code>docker run -t -i ubuntu /bin/bash</code> on a Raspberry Pi, M1 Mac and your regular PC.</p>
<p>Many open source projects have also caught on to the need for multi-arch images, but there are still a few like Bitnami, haven't yet seen value. I think that is OK, this kind work does take time and effort. Ultimately, it's up to the project maintainers to listen to their users and decide if they have enough interest to add support for Arm.</p>
<p>A multi-arch image is a container that will work on two or more different combinations of operating system and CPU architecture.</p>
<p>Typically, this would be:</p>
<ul>
<li><code>linux/amd64</code> - "normal" computers made by Intel or AMD</li>
<li><code>linux/arm64</code> - 64-bit Arm servers like <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-graviton-performance-testing/what-is-aws-graviton.html">AWS Graviton</a> or <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a></li>
<li><code>linux/arm/v7</code> - the 32-bit Raspberry Pi Operating System</li>
</ul>
<p>So multi-arch is really about catering for the needs of Arm users. Arm hardware platforms like the Ampere Altra come with 80 efficient CPU cores, have a very low TDP compared to traditional Intel hardware, and are available from various cloud providers.</p>
<h2>How do we build multi-arch containers work?</h2>
<p>There are a few tools and tricks that we can combine together to take a single Dockerfile and output an image that anyone can pull, which will be right for their machine.</p>
<p>Let's take the: <code>ghcr.io/inlets-operator:latest</code> image from <a href="https://inlets.dev/">inlets</a>.</p>
<p>When a user types in <code>docker pull</code>, or deploys a Pod to Kubernetes, their local containerd daemon will fetch the manifest file and inspect it to see what SHA reference to use for to download the required layers for the image.</p>
<p><img src="/images/2023-02-multi-arch/multi-arch.png" alt="How manifests work"></p>
<blockquote>
<p>How manifests work</p>
</blockquote>
<p>Let's look at a manifest file with the crane tool. I'm going to use <a href="https://arkade.dev">arkade</a> to install crane:</p>
<pre><code class="hljs language-bash">arkade get crane

crane manifest ghcr.io/inlets/inlets-operator:latest
</code></pre>
<p>You'll see a manifests array, with a platform section for each image:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.list.v2+json"</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">"manifests"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:bae8025e080d05f1db0e337daae54016ada179152e44613bf3f8c4243ad939df"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"amd64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">"mediaType"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"application/vnd.docker.distribution.manifest.v2+json"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"digest"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"sha256:3ddc045e2655f06653fc36ac88d1d85e0f077c111a3d1abf01d05e6bbc79c89f"</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">"platform"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"architecture"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"arm64"</span><span class="hljs-punctuation">,</span>
        <span class="hljs-attr">"os"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"linux"</span>
      <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<h2>How do we convert a Dockerfile to multi-arch?</h2>
<p>Instead of using the classic version of Docker, we can enable the buildx and Buildkit plugins which provide a way to build multi-arch images.</p>
<p>We'll continue with the Dockerfile from the open source inlets-operator project.</p>
<p>Within <a href="https://github.com/inlets/inlets-operator/blob/master/Dockerfile">the Dockerfile</a>, we need to make a couple of changes.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- FROM golang:1.18 as builder</span>
<span class="hljs-addition">+ FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder</span>

<span class="hljs-addition">+ ARG TARGETPLATFORM</span>
<span class="hljs-addition">+ ARG BUILDPLATFORM</span>
<span class="hljs-addition">+ ARG TARGETOS</span>
<span class="hljs-addition">+ ARG TARGETARCH</span>
</code></pre>
<p>The BUILDPLATFORM variable is the native architecture and platform of the machine performing the build, this is usually amd64.</p>
<p>The TARGETPLATFORM is important for the final step of the build, and will normally be injected based upon one each of the platforms you have specified for the build command.</p>
<p>For Go specifically, we also updated the <code>go build</code> command to tell Go to use cross-compilation based upon the TARGETOS and TARGETARCH environment variables, which are populated by Docker.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- go build -o inlets-operator</span>
<span class="hljs-addition">+ GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o inlets-operator</span>
</code></pre>
<p>Here's the full example:</p>
<pre><code>FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder

ARG TARGETPLATFORM
ARG BUILDPLATFORM
ARG TARGETOS
ARG TARGETARCH

ARG Version
ARG GitCommit

ENV CGO_ENABLED=0
ENV GO111MODULE=on

WORKDIR /go/src/github.com/inlets/inlets-operator

# Cache the download before continuing
COPY go.mod go.mod
COPY go.sum go.sum
RUN go mod download

COPY .  .

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go test -v ./...

RUN CGO_ENABLED=${CGO_ENABLED} GOOS=${TARGETOS} GOARCH=${TARGETARCH} \
  go build -ldflags "-s -w -X github.com/inlets/inlets-operator/pkg/version.Release=${Version} -X github.com/inlets/inlets-operator/pkg/version.SHA=${GitCommit}" \
  -a -installsuffix cgo -o /usr/bin/inlets-operator .

FROM --platform=${BUILDPLATFORM:-linux/amd64} gcr.io/distroless/static:nonroot

LABEL org.opencontainers.image.source=https://github.com/inlets/inlets-operator

WORKDIR /
COPY --from=builder /usr/bin/inlets-operator /
USER nonroot:nonroot

CMD ["/inlets-operator"]
</code></pre>
<h2>How to do you configure GitHub Actions to publish multi-arch images?</h2>
<p>Now that the Dockerfile has been configured, it's time to start working on the GitHub Action.</p>
<p>This example is taken from the Open Source <a href="https://github.com/inlets/inlets-operator">inlets-operator</a>. It builds a container image containing a Go binary and uses a Dockerfile in the root of the repository.</p>
<p>View <a href="https://github.com/inlets/inlets-operator/blob/master/.github/workflows/publish.yaml">publish.yaml</a>, adapted for actuated:</p>
<pre><code class="hljs language-diff">name: publish

on:
  push:
    tags:
      - '*'

jobs:
  publish:
<span class="hljs-addition">+    permissions:</span>
<span class="hljs-addition">+      packages: write</span>

<span class="hljs-deletion">-   runs-on: ubuntu-latest</span>
<span class="hljs-addition">+   runs-on: actuated</span>
    steps:
      - uses: actions/checkout@master
        with:
          fetch-depth: 1

<span class="hljs-addition">+     - name: Setup mirror</span>
<span class="hljs-addition">+       uses: self-actuated/hub-mirror@master</span>
      - name: Get TAG
        id: get_tag
        run: echo TAG=${GITHUB_REF#refs/tags/} >> $GITHUB_ENV
      - name: Get Repo Owner
        id: get_repo_owner
        run: echo "REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')" > $GITHUB_ENV

<span class="hljs-addition">+     - name: Set up QEMU</span>
<span class="hljs-addition">+       uses: docker/setup-qemu-action@v2</span>
<span class="hljs-addition">+     - name: Set up Docker Buildx</span>
<span class="hljs-addition">+       uses: docker/setup-buildx-action@v2</span>
      - name: Login to container Registry
        uses: docker/login-action@v2
        with:
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
          registry: ghcr.io

      - name: Release build
        id: release_build
        uses: docker/build-push-action@v4
        with:
          outputs: "type=registry,push=true"
          provenance: false
<span class="hljs-addition">+         platforms: linux/amd64,linux/arm/v6,linux/arm64</span>
          build-args: |
            Version=${{  env.TAG }}
            GitCommit=${{ github.sha }}
          tags: |
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ env.TAG }}
            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:latest
</code></pre>
<p>All of the images and corresponding manifest are published to GitHub's Container Registry (GHCR). The action itself is able to authenticate to GHCR using a built-in, short-lived token. This is dependent on the "permissions" section and "packages: write" being set.</p>
<p>You'll see that we added a <code>Setup mirror</code> step, this explained in the <a href="/examples/registry-mirror">Registry Mirror example</a> and is not required for Hosted Runners.</p>
<p>The <code>docker/setup-qemu-action@v2</code> step is responsible for setting up QEMU, which is used to emulate the different CPU architectures.</p>
<p>The <code>docker/build-push-action@v4</code> step is responsible for passing in a number of platform combinations such as: <code>linux/amd64</code> for cloud, <code>linux/arm64</code> for Arm servers and <code>linux/arm/v6</code> for Raspberry Pi.</p>
<h2>What if you're not using GitHub Actions?</h2>
<p>The various GitHub Actions published by the Docker team are a great way to get started, but if you look under the hood, they're just syntactic sugar for the Docker CLI.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> DOCKER_CLI_EXPERIMENTAL=enabled

<span class="hljs-comment"># Have Docker download the latest buildx plugin</span>
docker buildx install

<span class="hljs-comment"># Create a buildkit daemon with the name "multiarch"</span>
docker buildx create \
    --use \
    --name=multiarch \
    --node=multiarch

<span class="hljs-comment"># Install QEMU</span>
docker run --<span class="hljs-built_in">rm</span> --privileged \
    multiarch/qemu-user-static --reset -p <span class="hljs-built_in">yes</span>

<span class="hljs-comment"># Run a build for the different platforms</span>
docker buildx build \
    --platform=linux/arm64,linux/amd64 \
    --output=<span class="hljs-built_in">type</span>=registry,push=<span class="hljs-literal">true</span> --tag image:tag .
</code></pre>
<p>For OpenFaaS users, we do all of the above any time you type in <code>faas-cli publish</code> and the <code>faas-cli build</code> command just runs a regular Docker build, without any of the multi-arch steps.</p>
<p>If you're interested, you can checkout the code here: <a href="https://github.com/openfaas/faas-cli/blob/master/commands/publish.go">publish.go</a>.</p>
<h2>Putting it all together</h2>
<ul>
<li>CLIs are published for many different combinations of OS and CPU, but containers are usually only required for Linux with an amd64 or Arm CPU.</li>
<li>Multi-arch images work through a manifest, which then tells containerd which image is needs for the platform it is running on.</li>
<li>QEMU is a tool for emulating different CPU architectures, and is used to build the images for the different platforms.</li>
</ul>
<p>In our experience with OpenFaaS, inlets and actuated, once you have converted one or two projects to build multi-arch images, it becomes a lot easier to do it again, and make all software available for Arm servers.</p>
<p>You can learn more about <a href="https://docs.docker.com/build/building/multi-platform/">Multi-platform images</a> in the Docker Documentation.</p>
<p><em>Want more multi-arch examples?</em></p>
<p>OpenFaaS uses multi-arch Dockerfiles for all of its templates, and the examples are freely available on GitHub including Python, Node, Java and Go.</p>
<p>See also: <a href="https://github.com/openfaas/templates">OpenFaaS templates</a></p>
<p><em>A word of caution</em></p>
<p>QEMU can be incredibly slow at times when using a hosted runner, where a build takes takes 1-2 minutes can extend to over half an hour. If you do run into that, one option is to check out actuated or another solution, which can build directly on an Arm server with a securely isolated Virtual Machine.</p>
<p>In <a href="https://actuated.dev/blog/native-arm64-for-github-actions">How to make GitHub Actions 22x faster with bare-metal Arm</a>, we showed how we decreased the build time of an open-source Go project from 30.5 mins to 1.5 mins. If this is the direction you go in, you can use a <a href="https://docs.actuated.dev/examples/matrix/">matrix-build</a> instead of a QEMU-based multi-arch build.</p>
<p>See also: <a href="https://docs.actuated.dev/provision-server/#arm64-aka-aarch64">Recommended bare-metal Arm servers</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/sbom-in-github-actions</link>
            <guid>https://actuated.dev/blog/sbom-in-github-actions</guid>
            <pubDate>Wed, 25 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions in a few easy steps.]]></description>
            <content:encoded><![CDATA[<h2>What is a Software Bill of Materials (SBOM)?</h2>
<p>In <a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">April 2022 Justin Cormack, CTO of Docker announced</a> that Docker was adding support to generate a Software Bill of Materials (SBOM) for container images.</p>
<p>An SBOM is an inventory of the components that make up a software application. It is a list of the components that make up a software application including the version of each component. The version is important because it can be cross-reference with a vulnerability database to determine if the component has any known vulnerabilities.</p>
<p>Many organisations are also required to company with certain Open Source Software (OSS) licenses. So if SBOMs are included in the software they purchase or consume from vendors, then it can be used to determine if the software is compliant with their specific license requirements, lowering legal and compliance risk.</p>
<p>Docker's enhancements to Docker Desktop and their open source Buildkit tool were the result of a collaboration with Anchore, a company that provides a commercial SBOM solution.</p>
<h2>Check out an SBOM for yourself</h2>
<p>Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free.</p>
<ul>
<li><a href="https://github.com/anchore/syft">syft</a> - a command line tool that can be used to generate an SBOM for a container image.</li>
<li><a href="https://github.com/anchore/grype">grype</a> - a command line tool that can be used to scan an SBOM for vulnerabilities.</li>
</ul>
<p>OpenFaaS Community Edition (CE) is a popular open source serverless platform for Kubernetes. It's maintained by open source developers, and is free to use.</p>
<p>Let's pick a container image from the Community Edition of <a href="https://github.com/orgs/openfaasltd/packages">OpenFaaS</a> like the container image for the OpenFaaS gateway.</p>
<p>We can browse the GitHub UI to find the latest revision, or we can use Google's crane tool:</p>
<pre><code class="hljs language-bash">crane <span class="hljs-built_in">ls</span> ghcr.io/openfaas/gateway | <span class="hljs-built_in">tail</span> -n 5
0.26.0
8e1c34e222d6c194302c649270737c516fe33edf
0.26.1
c26ec5221e453071216f5e15c3409168446fd563
0.26.2
</code></pre>
<p>Now we can introduce one of those tags to syft:</p>
<pre><code class="hljs language-bash">syft ghcr.io/openfaas/gateway:0.26.2
 ✔ Pulled image            
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [39 packages]
NAME                                              VERSION                               TYPE      
alpine-baselayout                                 3.4.0-r0                              apk        
alpine-baselayout-data                            3.4.0-r0                              apk        
alpine-keys                                       2.4-r1                                apk        
apk-tools                                         2.12.10-r1                            apk        
busybox                                           1.35.0                                binary     
busybox                                           1.35.0-r29                            apk        
busybox-binsh                                     1.35.0-r29                            apk        
ca-certificates                                   20220614-r4                           apk        
ca-certificates-bundle                            20220614-r4                           apk        
github.com/beorn7/perks                           v1.0.1                                go-module  
github.com/cespare/xxhash/v2                      v2.1.2                                go-module  
github.com/docker/distribution                    v2.8.1+incompatible                   go-module  
github.com/gogo/protobuf                          v1.3.2                                go-module  
github.com/golang/protobuf                        v1.5.2                                go-module  
github.com/gorilla/mux                            v1.8.0                                go-module  
github.com/matttproud/golang_protobuf_extensions  v1.0.1                                go-module  
github.com/nats-io/nats.go                        v1.22.1                               go-module  
github.com/nats-io/nkeys                          v0.3.0                                go-module  
github.com/nats-io/nuid                           v1.0.1                                go-module  
github.com/nats-io/stan.go                        v0.10.4                               go-module  
github.com/openfaas/faas-provider                 v0.19.1                               go-module  
github.com/openfaas/faas/gateway                  (devel)                               go-module  
github.com/openfaas/nats-queue-worker             v0.0.0-20230117214128-3615ccb286cc    go-module  
github.com/prometheus/client_golang               v1.13.0                               go-module  
github.com/prometheus/client_model                v0.2.0                                go-module  
github.com/prometheus/common                      v0.37.0                               go-module  
github.com/prometheus/procfs                      v0.8.0                                go-module  
golang.org/x/crypto                               v0.5.0                                go-module  
golang.org/x/sync                                 v0.1.0                                go-module  
golang.org/x/sys                                  v0.4.1-0.20230105183443-b8be2fde2a9e  go-module  
google.golang.org/protobuf                        v1.28.1                               go-module  
libc-utils                                        0.7.2-r3                              apk        
libcrypto3                                        3.0.7-r2                              apk        
libssl3                                           3.0.7-r2                              apk        
musl                                              1.2.3-r4                              apk        
musl-utils                                        1.2.3-r4                              apk        
scanelf                                           1.3.5-r1                              apk        
ssl_client                                        1.35.0-r29                            apk        
zlib                                              1.2.13-r0                             apk  
</code></pre>
<p>These are all the components that syft found in the container image. We can see that it found 39 packages, including the OpenFaaS gateway itself.</p>
<p>Some of the packages are Go modules, others are packages that have been installed with <code>apk</code> (Alpine Linux's package manager).</p>
<h2>Checking for vulnerabilities</h2>
<p>Now that we have an SBOM, we can use grype to check for vulnerabilities.</p>
<pre><code class="hljs language-bash">grype ghcr.io/openfaas/gateway:0.26.2
 ✔ Vulnerability DB        [no update available]
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [39 packages]
 ✔ Scanned image           [2 vulnerabilities]
NAME                        INSTALLED  FIXED-IN  TYPE       VULNERABILITY   SEVERITY 
google.golang.org/protobuf  v1.28.1              go-module  CVE-2015-5237   High      
google.golang.org/protobuf  v1.28.1              go-module  CVE-2021-22570  Medium  
</code></pre>
<p>In this instance, we can see there are only two vulnerabilities, both of which are in the <code>google.golang.org/protobuf</code> Go module, and neither of them have been fixed yet.</p>
<ul>
<li>As the maintainer of OpenFaaS CE, I could try to eliminate the dependency from the original codebase, or wait for a workaround to be published by its vendor.</li>
<li>As a consumer of OpenFaaS CE my choices are similar, and it may be worth trying to look into the problem myself to see if the vulnerability is relevant to my use case.</li>
<li>Now, for OpenFaaS Pro, a commercial distribution of OpenFaaS, where source is not available, I'd need to contact the vendor OpenFaaS Ltd and see if they could help, or if they could provide a workaround. Perhaps there would even be a paid support relationship and SLA relating to fixing vulnerabilities of this kind?</li>
</ul>
<p>With this scenario, I wanted to show that different people care about the supply chain, and have different responsibilities for it.</p>
<h2>Generate an SBOM from within GitHub Actions</h2>
<p>The examples above were all run locally, but we can also generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.</p>
<p>Imagine you have the following Dockerfile:</p>
<pre><code>FROM alpine:3.17.0

RUN apk add --no-cache curl ca-certificates

CMD ["curl", "https://www.google.com"]
</code></pre>
<p>I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use <code>3.17.1</code> to fix a specific vulnerability.</p>
<p>Now a typical workflow for this Dockerfile would look like the below:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">build</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]
  <span class="hljs-attr">pull_request:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">checks:</span> <span class="hljs-string">write</span>
  <span class="hljs-attr">contents:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">publish:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">image</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/alexellis/gha-sbom:${{ github.sha }}
</span></code></pre>
<p>Upon each commit, an image is published to GitHub's Container Registry with the image name of: <code>ghcr.io/alexellis/gha-sbom:SHA</code>.</p>
<p>To generate an SBOM, we just need to update the <code>docker/build-push-action</code> to use the <code>sbom</code> flag:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Local</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">local_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v4</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">sbom:</span> <span class="hljs-literal">true</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
</code></pre>
<p>By checking the logs from the action, we can see that the image has been published with an SBOM:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment">#16 [linux/amd64] generating sbom using docker.io/docker/buildkit-syft-scanner:stable-1</span>
<span class="hljs-comment">#0 0.120 time="2023-01-25T15:35:19Z" level=info msg="starting syft scanner for buildkit v1.0.0"</span>
<span class="hljs-comment">#16 DONE 1.0s</span>
</code></pre>
<p>The SBOM can be viewed as before:</p>
<pre><code class="hljs language-bash">syft ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a

grype ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a
 ✔ Vulnerability DB        [updated]
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [21 packages]
 ✔ Scanned image           [2 vulnerabilities]
NAME        INSTALLED  FIXED-IN  TYPE  VULNERABILITY  SEVERITY 
libcrypto3  3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High      
libssl3     3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High  
</code></pre>
<p>The image: <code>alpine:3.17.0</code> contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed.</p>
<p>We can resolve the issue by changing the Dockerfile to use <code>alpine:3.17.1</code> instead, and re-running the build.</p>
<pre><code class="hljs language-bash">grype ghcr.io/alexellis/gha-sbom:63c6952d1ded1f53b1afa3f8addbba9efa37b52b
 ✔ Vulnerability DB        [no update available]
 ✔ Pulled image            
 ✔ Loaded image            
 ✔ Parsed image            
 ✔ Cataloged packages      [21 packages]
 ✔ Scanned image           [0 vulnerabilities]
No vulnerabilities found
</code></pre>
<h2>Wrapping up</h2>
<p>There is a lot written on the topic of supply chain security, so I wanted to give you a quick overview, and how to get started wth it.</p>
<p>We looked at Anchore's two open source tools: Syft and Grype, and how they can be used to generate an SBOM and scan for vulnerabilities.</p>
<p>We then produced an SBOM for a pre-existing Dockerfile and GitHub Action, introducing a vulnerability by using an older base image, and then fixing it by upgrading it. We did this by adding additional flags to the <code>docker/build-push-action</code>. We added the sbom flag, and set the provenance flag to false. Provenance is a separate but related topic, which is explained well in an article by Justin Chadwell of Docker (linked below).</p>
<p>I maintain an Open Source alternative to brew for developer-focused CLIs called <a href="https://arkade.dev/">arkade</a>. This already includes Google's crane project, and there's a <a href="https://github.com/alexellis/arkade/issues/839">Pull Request coming shortly to add Syft and Grype to the project</a>.</p>
<p>It can be a convenient way to install these tools on MacOS, Windows or Linux:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Available now</span>
arkade get crane syft

<span class="hljs-comment"># Coming shortly</span>
arkade get grype
</code></pre>
<p>In the beginning of the article we mentioned license compliance. SBOMs generated by syft do not seem to include license information, but in my experience, corporations which take this risk seriously tend to run their own scanning infrastructure <a href="https://www.synopsys.com/software-integrity/security-testing/software-composition-analysis.html">with commercial tools like Blackduck</a> or <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/21-08/prisma-cloud-compute-edition-admin/compliance/manage_compliance">Twistlock</a>.</p>
<p>Tools like Twistlock, and certain registries like <a href="https://jfrog.com/artifactory/">JFrog Artifactory</a> and the <a href="https://goharbor.io/">CNCF's Harbor</a>, can be configured to scan images. GitHub has a free, built-in service called Dependabot that won't just scan, but will also send Pull Requests to fix issues.</p>
<p>But with the SBOM approach, the responsibility is rebalanced, with the supplier taking on an active role in security. The consumer can then use the supplier's SBOMs, or run their own scanning infrastructure - or perhaps both.</p>
<p>See also:</p>
<ul>
<li><a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">Announcing Docker SBOM: A step towards more visibility into Docker images- Justin Cormack</a></li>
<li><a href="https://www.docker.com/blog/generate-sboms-with-buildkit/">Generating SBOMs for Your Image with BuildKit - Justin Chadwell</a></li>
<li><a href="https://github.com/anchore">Anchore on GitHub</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Is the GitHub Actions self-hosted runner safe for Open Source?]]></title>
            <link>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</link>
            <guid>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</guid>
            <pubDate>Fri, 20 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub warns against using self-hosted Actions runners for public repositories - but why? And are there alternatives?]]></description>
            <content:encoded><![CDATA[<p>First of all, why would someone working on an open source project need a self-hosted runner?</p>
<p>Having contributed to dozens of open source projects, and gotten to know many different maintainers, the primary reason tends to be out of necessity. They face an 18 minute build time upon every commit or Pull Request revision, and want to make the best of what little time they can give over to Open Source.</p>
<p>Having faster builds also lowers friction for contributors, and since many contributors are unpaid and rely on their own internal drive and enthusiasm, a fast build time can be the difference between them fixing a broken test or waiting another few days.</p>
<p>To sum up, there are probably just a few main reasons:</p>
<ol>
<li>Faster builds, higher concurrency, more disk space</li>
<li>Needing to build and test Arm binaries or containers on real hardware</li>
<li>Access to services on private networks</li>
</ol>
<p>The first point is probably one most people can relate to. Simply by provisioning an AMD bare-metal host, or a high spec VM with NVMe, you can probably shave minutes off a build.</p>
<p>For the second case, some projects like <a href="https://github.com/fluxcd/flagger">Flagger</a> from the CNCF felt their only option to support users deploying to AWS Graviton, was to seek sponsorship for a large Arm server and to install a self-hosted runner on it.</p>
<p>The third option is more nuanced, and specialist. This may or may not be something you can relate to, but it's worth mentioning. VPNs have very limited speed and there may be significant bandwidth costs to transfer data out of a region into GitHub's hosted runner environment. Self-hosted runners eliminate the cost and give full local link bandwidth, even as high as 10GbE. You just won't get anywhere near that with IPSec or Wireguard over the public Internet.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<h2>But are self-hosted runners safe?</h2>
<p>The GitHub team has a stark warning for those of us who are tempted to deploy a self-hosted runner and to connect it to a public repository.</p>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs. Some of the risks include:</p>
<ul>
<li>Malicious programs running on the machine.</li>
<li>Escaping the machine's runner sandbox.</li>
<li>Exposing access to the machine's network environment.</li>
<li>Persisting unwanted or dangerous data on the machine.</li>
</ul>
</blockquote>
<p>See also: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">Self-hosted runner security</a></p>
<p>Now you may be thinking "I won't approve pull requests from bad actors", but quite often the workflow goes this way: the contributor gets approval, then you don't need to approve subsequent pull requests after that.</p>
<p>An additional risk is if that user's account is compromised, then the attacker can submit a pull request with malicious code or malware. There is no way in GitHub to enforce Multi-Factor Authentication (MFA) for pull requests, even if you have it enabled on your Open Source Organisation.</p>
<p>Here are a few points to consider:</p>
<ul>
<li>Side effects build up with every build, making it less stable over time</li>
<li>You can't enforce MFA for pull requests - so malware can be installed directly on the host - whether intentionally or not</li>
<li>The GitHub docs warn that users can take over your account</li>
<li>Each runner requires maintenance and updates of the OS and all the required packages</li>
</ul>
<p>The chances are that if you're running the Flagger or Network Service Mesh project, you are shipping code that enterprise companies will deploy in production with sensitive customer data.</p>
<p>If you are not worried, try explaining the above to them, to see how they may see the risk differently.</p>
<h2>Doesn't Kubernetes fix all of this?</h2>
<p><a href="https://kubernetes.io/">Kubernetes</a> is a well known platform built for orchestrating containers. It's especially suited to running microservices, webpages and APIs, but has support for batch-style workloads like CI runners too.</p>
<p>You could make a container image and install the self-hosted runner binary within in, then deploy that as a Pod to a cluster. You could even scale it up with a few replicas.</p>
<p>If you are only building Java code, Python or Node.js, you may find this resolves many of the issues that we covered above, but it's hard to scale, and you still get side-effects as the environment is not immutable.</p>
<p>That's where the community project "actions-runtime-controller" or ARC comes in. It's a controller that launches a pool of Pods with the self-hosted runner.</p>
<blockquote>
<p>How much work does ARC need?</p>
<p>Some of the teams I have interviewed over the past 3 months told me that ARC took them a lot of time to set up and maintain, whilst others have told us it was a lot easier for them. It may depend on your use-case, and whether you're more of a personal user, or part of a team with 10-30 people committing code several times per day.
The first customer for actuated, which I'll mention later in the article was a team of ~ 20 people who were using ARC and had grew tired of the maintenance overhead and certain reliability issues.</p>
</blockquote>
<p>Unfortunately, by default ARC uses the same Pod many times as a persistent runner, so side effects still build up, malware can still be introduced and you have to maintain a Docker image with all the software needed for your builds.</p>
<p>You may be happy with those trade-offs, especially if you're only building private repositories.</p>
<p>But those trade-offs gets a lot worse if you use Docker or Kubernetes.</p>
<p>Out of the box, you simply cannot start a Docker container, build a container image or start a Kubernetes cluster.</p>
<p>And to do so, you'll need to resort to what can only be described as dangerous hacks:</p>
<ol>
<li>You expose the Docker socket from the host, and mount it into each Pod - any CI job can take over the host, game over.</li>
<li>You run in <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker in Docker (DIND)</a> mode. DIND requires a privileged Pod, which means that any CI job can take over the host, game over.</li>
</ol>
<p>There is some early work on running Docker In Docker in user-space mode, but this is slow, tricky to set up and complicated. By default, user-space mode uses a non-root account. So you can't install software packages or run commands like apt-get.</p>
<p>See also: <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Using Docker-in-Docker for your CI or testing environment? Think twice.</a></p>
<p>Have you heard of Kaniko?</p>
<p><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> is a tool for building container images from a Dockerfile, without the need for a Docker daemon. It's a great option, but it's not a replacement for running containers, it can only build them.</p>
<p>And when it builds them, in nearly every situation it will need root access in order to mount each layer to build up the image.</p>
<p>See also: <a href="https://suraj.io/post/root-in-container-root-on-host/">The easiest way to prove that root inside the container is also root on the host</a></p>
<p>And what about Kubernetes?</p>
<p>To run a KinD, Minikube or K3s cluster within your CI job, you're going to have to sort to one of the dangerous hacks we mentioned earlier which mean a bad actor could potentially take over the host.</p>
<p>Some of you may be running these Kubernetes Pods in your production cluster, whilst others have taken some due diligence and deployed a separate cluster just for these CI workloads. I think that's a slightly better option, but it's still not ideal and requires even more access control and maintenance.</p>
<p>Ultimately, there is a fine line between overconfidence and negligence. When building code on a public repository, we have to assume that the worst case scenario will happen one day. When using DinD or privileged containers, we're simply making that day come sooner.</p>
<p>Containers are great for running internal microservices and Kubernetes excels here, but there is a reason that AWS insists on hard multi-tenancy with Virtual Machines for their customers.</p>
<blockquote>
<p>See also: <a href="https://www.amazon.science/publications/firecracker-lightweight-virtualization-for-serverless-applications">Firecracker whitepaper</a></p>
</blockquote>
<h2>What's the alternative?</h2>
<p>When GitHub cautioned us against using self-hosted runners, on public repos, they also said:</p>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<p>So using GitHub's hosted runners are probably the most secure option for Open Source projects and for public repositories - if you are happy with the build speed, and don't need Arm runners.</p>
<p>But that's why I'm writing this post, sometimes we need faster builds, or access to specialist hardware like Arm servers.</p>
<p>The Kubernetes solution is fast, but it uses a Pod which runs many jobs, and in order to make it useful enough to run <code>docker run</code>, <code>docker build</code> or to start a Kubernetes cluster, we have to make our machines vulnerable.</p>
<p>With actuated, we set out to re-build the same user experience as GitHub's hosted runners, but without the downsides of self-hosted runners or using Kubernetes Pods for runners.</p>
<p>Actuated runs each build in a microVM on servers that you alone provision and control.</p>
<p>Its centralised control-plane schedules microVMs to each server using an immutable Operating System that is re-built with automation and kept up to date with the latest security patches.</p>
<p>Once the microVM has launched, it connects to GitHub, receives a job, runs to completion and is completely erased thereafter.</p>
<p>You get all of the upsides of self-hosted runners, with a user experience that is as close to GitHub's hosted runners as possible.</p>
<p>Pictured - an Arm Server with 270 GB of RAM and 80 cores - that's a lot of builds.</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62k4gXkAMHX-B?format=jpg&#x26;name=large" alt=""></a></p>
<p>You get to run the following, without worrying about security or side-effects:</p>
<ul>
<li>Docker (<code>docker run</code>) and <code>docker build</code></li>
<li>Kubernetes - Minikube, K3s, KinD</li>
<li><code>sudo</code> / root commands</li>
</ul>
<p>Need to test against a dozen different Kubernetes versions?</p>
<p>Not a problem:</p>
<p><img src="https://actuated.dev/images/k3sup.png" alt="Testing multiple Kubernetes versions"></p>
<p>What about running the same on Arm servers?</p>
<p>Just change <code>runs-on: actuated</code> to <code>runs-on: actuated-aarch64</code> and you're good to go. We test and maintain support for Docker and Kubernetes for both Intel and Arm CPU architectures.</p>
<p>Do you need insights for your Open Source Program Office (OSPO) or for the Technical Steering Committee (TSC)?</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62kSTXgAQLzUb?format=jpg&#x26;name=medium" alt=""></a></p>
<p>We know that no open source project has a single repository that represents all of its activity. Actuated provides insights across an organisation, including total build time and the time queued - which is a reflection of whether you could do with more or fewer build machines.</p>
<p>And we are only just getting started with compiling insights, there's a lot more to come.</p>
<h2>Get involved today</h2>
<p>We've already launched 10,000 VMs for customers jobs, and are now ready to open up the platform to the wider community. So if you'd like to try out what we're offering, we'd love to hear from you. As you offer feedback, you'll get hands on support from our engineering team and get to shape the product through collaboration.</p>
<p>So what does it cost? There is a subscription fee which includes - the control plane for your organisation, the agent software, maintenance of the OS images and our support via Slack. But all the plans are flat-rate, so it may even work out cheaper than paying GitHub for the bigger instances that they offer.</p>
<p>Professional Open Source developers like the ones you see at Red Hat, VMware, Google and IBM, that know how to work in community and understand cloud native are highly sought after and paid exceptionally well. So the open source project you work on has professional full-time engineers allocated to it by one or more companies, as is often the case, then using actuated could pay for itself in a short period of time.</p>
<p>If you represent an open source project that has no funding and is purely maintained by volunteers, what we have to offer may not be suited to your current position. And in that case, we'd recommend you stick with the slower GitHub Runners. Who knows? Perhaps one day GitHub may offer sponsored faster runners at no cost for certain projects?</p>
<p>And finally, what if your repositories are private? Well, we've made you aware of the trade-offs with a static self-hosted runner, or running builds within Kubernetes. It's up to you to decide what's best for your team, and your customers. Actuated works just as well with private repositories as it does with public ones.</p>
<p>See microVMs launching in ~ 1s during a matrix build for testing a Custom Resource Definition (CRD) on different Kubernetes versions:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want to know how actuated works? <a href="https://docs.actuated.dev/faq/">Read the FAQ for more technical details</a>.</p>
<ul>
<li><a href="/pricing">Find a server for your builds</a></li>
<li><a href="https://docs.actuated.dev/provision-server/">Register for actuated</a></li>
</ul>
<p>Follow us on Twitter - <a href="https://twitter.com/selfactuated">selfactuated</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to make GitHub Actions 22x faster with bare-metal Arm]]></title>
            <link>https://actuated.dev/blog/native-arm64-for-github-actions</link>
            <guid>https://actuated.dev/blog/native-arm64-for-github-actions</guid>
            <pubDate>Tue, 17 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub doesn't provide hosted Arm runners, so how can you use native Arm runners safely & securely?]]></description>
            <content:encoded><![CDATA[<p>GitHub Actions is a modern, fast and efficient way to build and test software, with free runners available. We use the free runners for various open source projects and are generally very pleased with them, after all, who can argue with good enough and free? But one of the main caveats is that GitHub's hosted runners don't yet support the Arm architecture.</p>
<p>So many people turn to software-based emulation using <a href="https://www.qemu.org/">QEMU</a>. QEMU is tricky to set up, and requires specific code and tricks if you want to use software in a standard way, without modifying it. But QEMU is great when it runs with hardware acceleration. Unfortunately, the hosted runners on GitHub do not have KVM available, so builds tend to be incredibly slow, and I mean so slow that it's going to distract you and your team from your work.</p>
<p>This was even more evident when <a href="https://twitter.com/fredbrancz">Frederic Branczyk</a> tweeted about his experience with QEMU on <a href="https://github.com/features/actions">GitHub Actions</a> for his open source observability project named <a href="https://github.com/parca-dev/parca">Parca</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does anyone have a <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> actions self-hosted runner manifest for me to throw at a <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> cluster? I&#39;m tired of waiting for emulated arm64 CI runs taking ages.</p>&mdash; Frederic 🧊 Branczyk @brancz@hachyderm.io (@fredbrancz) <a href="https://twitter.com/fredbrancz/status/1582779459379204096?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I checked out his build and expected "ages" to mean 3 minutes, in fact, it meant 33.5 minutes. I know because I forked his project and ran a test build.</p>
<p>After migrating it to actuated and one of our build agents, the time dropped to 1 minute and 26 seconds, a 22x improvement for zero effort.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>You can see the results here:</p>
<p><a href="https://twitter.com/alexellisuk/status/1583089248084729856/photo/1"><img src="https://pbs.twimg.com/media/FfhC5z1XkAAoYjn?format=jpg&#x26;name=large" alt="Results from the test, side-by-side"></a></p>
<p>As a general rule, the download speed is going to be roughly the same with a hosted runner, it may even be slightly faster due to the connection speed of Azure's network.</p>
<p>But the compilation times speak for themselves - in the Parca build, <code>go test</code> was being run with QEMU. Moving it to run on the ARM64 host directly, resulted in the marked increase in speed. In fact, the team had introduced lots of complicated code to try and set up a Docker container to use QEMU, all that could be stripped out, replacing it with a very standard looking test step:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">the</span> <span class="hljs-string">go</span> <span class="hljs-string">tests</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">go</span> <span class="hljs-string">test</span> <span class="hljs-string">./...</span>
</code></pre>
<h2>Can't I just install the self-hosted runner on an Arm VM?</h2>
<p>There are relatively cheap Arm VMs available from Oracle OCI, Google and Azure based upon the Ampere Altra CPU. AWS have their own Arm VMs available in the Graviton line.</p>
<p>So why shouldn't you just go ahead and install the runner and add them to your repos?</p>
<p>The moment you do that you run into three issues:</p>
<ul>
<li>You now have to maintain the software packages installed on that machine</li>
<li>If you use KinD or Docker, you're going to run into conflicts between builds</li>
<li>Out of the box scheduling is poor - by default it only runs one build at a time there</li>
</ul>
<p>Chasing your tail with package updates, faulty builds due to caching and conflicts is not fun, you may feel like you're saving money, but you are paying with your time and if you have a team, you're paying with their time too.</p>
<p>Most importantly, GitHub say that it cannot be used safely with a public repository. There's no security isolation, and state can be left over from one build to the next, including harmful code left intentionally by bad actors, or accidentally from malware.</p>
<h2>So how do we get to a safer, more efficient Arm runner?</h2>
<p>The answer is to get us as close as possible to a hosted runner, but with the benefits of a self-hosted runner.</p>
<p>That's where actuated comes in.</p>
<p>We run a SaaS that manages bare-metal for you, and talks to GitHub upon your behalf to schedule jobs efficiently.</p>
<ul>
<li>No need to maintain software, we do that for you with an automated OS image</li>
<li>We use microVMs to isolate builds from each other</li>
<li>Every build is immutable and uses a clean environment</li>
<li>We can schedule multiple builds at once without side-effects</li>
</ul>
<p>microVMs on Arm require a bare-metal server, and we have tested all the options available to us. Note that the Arm VMs discussed above do not currently support KVM or nested virtualisation.</p>
<ul>
<li>a1.metal on AWS - 16 cores / 32GB RAM - 300 USD / mo</li>
<li>c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM - 2.5 USD / hr</li>
<li><a href="https://www.hetzner.com/dedicated-rootserver/matrix-rx">RX-Line</a> from <a href="https://hetzner.com">Hetzner</a> with 128GB / 256GB RAM, NVMe &#x26; 80 cores for approx 200-250 EUR / mo.</li>
<li><a href="https://amzn.to/3WiSDE7">Mac Mini M1</a> - 8 cores / 16GB RAM - tested with Asahi Linux - one-time payment of ~ 1500 USD</li>
</ul>
<p>If you're already an AWS customer, the a1.metal is a good place to start. If you need expert support, networking and a high speed uplink, you can't beat Equinix Metal (we have access to hardware there and can help you get started) - you can even pay per minute and provision machines via API. The Mac Mini &#x3C;1 has a really fast NVMe and we're running one of these with Asahi Linux for our own Kernel builds for actuated. The RX Line from Hetzner has serious power and is really quite affordable, but just be aware that you're limited to a 1Gbps connection, a setup fee and monthly commitment, unless you pay significantly more.</p>
<p>I even tried Frederic's Parca job <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">on my 8GB Raspberry Pi with a USB NVMe</a>. Why even bother, do I hear you say? Well for a one-time payment of 80 USD, it was 26m30s quicker than a hosted runner with QEMU!</p>
<p><a href="https://alexellisuk.medium.com/upgrade-your-raspberry-pi-4-with-a-nvme-boot-drive-d9ab4e8aa3c2">Learn how to connect an NVMe over USB-C to your Raspberry Pi 4</a></p>
<h2>What does an Arm job look like?</h2>
<p>Since I first started trying to build code for Arm in 2015, I noticed a group of people who had a passion for this efficient CPU and platform. They would show up on GitHub issue trackers, ready to send patches, get access to hardware and test out new features on Arm chips. It was a tough time, and we should all be grateful for their efforts which go largely unrecognised.</p>
<blockquote>
<p>If you're looking to make your <a href="https://twitter.com/alexellisuk">software compatible with Arm</a>, feel free to reach out to me via Twitter.</p>
</blockquote>
<p>In 2020 when Apple released their M1 chip, Arm went mainstream, and projects that had been putting off Arm support like KinD and Minikube, finally had that extra push to get it done.</p>
<p>I've had several calls with teams who use Docker on their M1/M2 Macs exclusively, meaning they build only Arm binaries and use only Arm images from the Docker Hub. Some of them even ship to project using Arm images, but I think we're still a little behind the curve there.</p>
<p>That means Kubernetes - KinD/Minikube/K3s and Docker - including Buildkit, compose etc, all work out of the box.</p>
<p>I'm going to use the arkade CLI to download KinD and kubectl, however you can absolutely find the download links and do all this manually. I don't recommend it!</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">e2e-kind-test</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">start-kind:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">and</span> <span class="hljs-string">kubectl</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">kubectl:</span> <span class="hljs-string">latest</span>
          <span class="hljs-attr">kind:</span> <span class="hljs-string">latest</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">a</span> <span class="hljs-string">KinD</span> <span class="hljs-string">cluster</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          mkdir -p $HOME/.kube/
          kind create cluster --wait 300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Wait</span> <span class="hljs-string">until</span> <span class="hljs-string">CoreDNS</span> <span class="hljs-string">is</span> <span class="hljs-string">ready</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          kubectl rollout status deploy/coredns -n kube-system --timeout=300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">nodes</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">nodes</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">pods</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">pod</span> <span class="hljs-string">-A</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
</code></pre>
<p>That's our <code>x86_64</code> build, or Intel/AMD build that will run on a hosted runner, but will be kind of slow.</p>
<p>Let's convert it to run on an actuated ARM64 runner:</p>
<pre><code class="hljs language-diff">jobs:
  start-kind:
<span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: actuated-aarch64</span>
</code></pre>
<p>That's it, we've changed the runner type and we're ready to go.</p>
<p><img src="/images/2023-native-arm64-for-oss/in-progress-dashboard.png" alt="In progress build on the dashboard"></p>
<blockquote>
<p>An in progress build on the dashboard</p>
</blockquote>
<p>Behind the scenes, actuated, the SaaS schedules the build on a bare-metal ARM64 server, the boot up takes less than 1 second, and then the standard GitHub Actions Runner talks securely to GitHub to run the build. The build is isolated from other builds, and the runner is destroyed after the build is complete.</p>
<p><img src="/images/2023-native-arm64-for-oss/arm-kind.png" alt="Setting up an Arm KinD cluster took about 49s"></p>
<blockquote>
<p>Setting up an Arm KinD cluster took about 49s</p>
</blockquote>
<p>Setting up an Arm KinD cluster took about 49s, then it's over to you to test your Arm images, or binaries.</p>
<p>If I were setting up CI and needed to test software on both Arm and x86_64, then I'd probably create two separate builds, one for each architecture, with a <code>runs-on</code> label of <code>actuated</code> and <code>actuated-aarch64</code> respectively.</p>
<p>Do you need to test multiple versions of Kubernetes? Let's face it, it changes so often, that who doesn't need to do that. You can use the <code>matrix</code> feature to test multiple versions of Kubernetes on Arm and x86_64.</p>
<p>I show 5x clusters being launched in parallel in the video below:</p>
<p><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Demo - Actuated - secure, isolated CI for containers and Kubernetes</a></p>
<p>What about Docker?</p>
<p>Docker comes pre-installed in the actuated OS images, so you can simply use <code>docker build</code>, without any need to install extra tools like Buildx, or to have to worry about multi-arch Dockerfiles. Although these are always good to have, and are <a href="https://github.com/openfaas/golang-http-template/blob/master/template/golang-middleware/Dockerfile">available out of the box in OpenFaaS</a>, if you're curious what a multi-arch Dockerfile looks like.</p>
<h2>Wrapping up</h2>
<p>Building on bare-metal Arm hosts is more secure because side effects cannot be left over between builds, even if malware is installed by a bad actor. It's more efficient because you can run multiple builds at once, and you can use the latest software with our automated Operating System image. Enabling actuated on a build is as simple as changing the runner type.</p>
<p>And as you've seen from the example with the OSS Parca project, moving to a native Arm server can improve speed by 22x, shaving off a massive 34 minutes per build.</p>
<p>Who wouldn't want that?</p>
<p>Parca isn't a one-off, I was also told by <a href="https://twitter.com/cohix">Connor Hicks from Suborbital</a> that they have an Arm build that takes a good 45 minutes due to using QEMU.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<p>So if that sounds like where you are, reach out to us and we'll get you set up.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register to pilot actuated with us</a></li>
</ul>
<p>Additional links:</p>
<ul>
<li><a href="https://docs.actuated.dev/">Actuated docs</a></li>
<li><a href="https://docs.actuated.dev/faq">FAQ &#x26; comparison to other solutions</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blazing fast CI with MicroVMs]]></title>
            <link>https://actuated.dev/blog/blazing-fast-ci-with-microvms</link>
            <guid>https://actuated.dev/blog/blazing-fast-ci-with-microvms</guid>
            <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[I saw an opportunity to fix self-hosted runners for GitHub Actions. Actuated is now in pilot and aims to solve most if not all of the friction.]]></description>
            <content:encoded><![CDATA[<p>Around 6-8 months ago I started exploring MicroVMs out of curiosity. Around the same time, I saw an opportunity to <strong>fix</strong> self-hosted runners for GitHub Actions. <a href="https://docs.actuated.dev/">Actuated</a> is now in pilot and aims to solve <a href="https://twitter.com/alexellisuk/status/1573599285362532353?s=20&#x26;t=dFcd54c4KIynk6vIGTb7QA">most if not all of the friction</a>.</p>
<p>There's three parts to this post:</p>
<ol>
<li>A quick debrief on Firecracker and MicroVMs vs legacy solutions</li>
<li>Exploring friction with GitHub Actions from a hosted and self-hosted perspective</li>
<li>Blazing fast CI with Actuated, and additional materials for learning more about Firecracker</li>
</ol>
<blockquote>
<p>We're looking for customers who want to solve the problems explored in this post.
<a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot</a></p>
</blockquote>
<h2>1) A quick debrief on Firecracker 🔥</h2>
<blockquote>
<p>Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services.</p>
</blockquote>
<p>I learned about <a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> mostly by experimentation, building bigger and more useful prototypes. This helped me see what the experience was going to be like for users and the engineers working on a solution. I met others in the community and shared notes with them. Several people asked "Are microVMs the next thing that will replace containers?" I don't think they are, but they are an important tool where hard isolation is necessary.</p>
<p>Over time, one thing became obvious:</p>
<blockquote>
<p>MicroVMs fill a need that legacy VMs and containers can't.</p>
</blockquote>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker, you can replay my deep dive session with Richard Case, Principal Engineer (previously Weaveworks, now at SUSE).</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h2>2) So what's wrong with GitHub Actions?</h2>
<p>First let me say that I think GitHub Actions is a far better experience than Travis ever was, and we have moved all our CI for OpenFaaS, inlets and actuated to Actions for public and private repos. We've built up a good working knowledge in the community and the company.</p>
<p>I'll split this part into two halves.</p>
<h3>What's wrong with hosted runners?</h3>
<p><strong>Hosted runners are constrained</strong></p>
<p>Hosted runners are incredibly convenient, and for most of us, that's all we'll ever need, especially for public repositories with fast CI builds.</p>
<p>Friction starts when the 7GB of RAM and 2 cores allocated causes issues for us - like when we're launching a KinD cluster, or trying to run E2E tests and need more power. Running out of disk space is also a common problem when using Docker images.</p>
<p>GitHub recently launched new paid plans to get faster runners, however the costs add up, the more you use them.</p>
<p>What if you could pay a flat fee, or bring your own hardware?</p>
<p><strong>They cannot be used with public repos</strong></p>
<p>From GitHub.com:</p>
<blockquote>
<p>We recommend that you only use self-hosted runners with private repositories. This is because forks of your public repository can potentially run dangerous code on your self-hosted runner machine by creating a pull request that executes the code in a workflow.</p>
</blockquote>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs.</p>
</blockquote>
<p>Read more about the risks: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners">Self-hosted runner security</a></p>
<p>Despite a stern warning from GitHub, at least one notable CNCF project runs self-hosted ARM64 runners on public repositories.</p>
<p>On one hand, I don't blame that team, they have no other option if they want to do open source, it means a public repo, which means risking everything knowingly.</p>
<p>Is there another way we can help them?</p>
<p>I spoke to the GitHub Actions engineering team, who told me that using an ephemeral VM and an immutable OS image would solve the concerns.</p>
<p><strong>There's no access to ARM runners</strong></p>
<p>Building with QEMU is incredibly slow as Frederic Branczyk, Co-founder, Polar Signals found out when his Parca project was taking 33m5s to build.</p>
<p>I forked it and changed a line: <code>runs-on: actuated-aarch64</code> and reduced the total build time to 1m26s.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><strong>They limit maximum concurrency</strong></p>
<p>On the free plan, you can only launch 20 hosted runners at once, this increases as you pay GitHub more money.</p>
<p><strong>Builds on private repos are billed per minute</strong></p>
<p>I think this is a fair arrangement. GitHub donates Azure VMs to open source users or any public repo for that matter, and if you want to build closed-source software, you can do so by renting VMs per hour.</p>
<p>There's a free allowance for free users, then Pro users like myself get a few more build minutes included. However, These are on the standard, 2 Core 7GB RAM machines.</p>
<p>What if you didn't have to pay per minute of build time?</p>
<h3>What's wrong with self-hosted runners?</h3>
<p><strong>It's challenging to get all the packages right as per a hosted runner</strong></p>
<p>I spent several days running and re-running builds to get all the software required on a self-hosted runner for the private repos for <a href="https://www.openfaas.com/pricing/">OpenFaaS Pro</a>. Guess what?</p>
<p>I didn't want to touch that machine again afterwards, and even if I built up a list of apt packages, it'd be wrong in a few weeks. I then had a long period of tweaking the odd missing package and generating random container image names to prevent Docker and KinD from conflicting and causing side-effects.</p>
<p>What if we could get an image that had everything we needed and was always up to date, and we didn't have to maintain that?</p>
<p><strong>Self-hosted runners cause weird bugs due to caching</strong></p>
<p>If your job installs software like apt packages, the first run will be different from the second. The system is mutable, rather than immutable and the first problem I faced was things clashing like container names or KinD cluster names.</p>
<p><strong>You get limited to one job per machine at a time</strong></p>
<p>The default setup is for a self-hosted Actions Runner to only run one job at a time to avoid the issues I mentioned above.</p>
<p>What if you could schedule as many builds as made sense for the amount of RAM and core the host has?</p>
<p><strong>Docker isn't isolated at all</strong></p>
<p>If you install Docker, then the runner can take over that machine since Docker runs at root on the host. If you try user-namespaces, many things break in weird and frustrating aways like Kubernetes.</p>
<p>Container images and caches can cause conflicts between builds.</p>
<p><strong>Kubernetes isn't a safe alternative</strong></p>
<p>Adding a single large machine isn't a good option because of the dirty cache, weird stateful errors you can run into, and side-effects left over on the host.</p>
<p>So what do teams do?</p>
<p>They turn to a controller called <a href="https://github.com/actions/actions-runner-controller">Actions Runtime Controller (ARC)</a>.</p>
<p>ARC is non trivial to set up and requires you to create a GitHub App or PAT (please don't do that), then to provision, monitor, maintain and upgrade a bunch of infrastructure.</p>
<p>This controller starts a number of re-usable (not one-shot) Pods and has them register as a runner for your jobs. Unfortunately, they still need to use Docker or need to run Kubernetes which leads us to two awful options:</p>
<ol>
<li>Sharing a Docker Socket (easy to become root on the host)</li>
<li>Running Docker In Docker (requires a privileged container, root on the host)</li>
</ol>
<p>There is a third option which is to use a non-root container, but that means you can't use <code>sudo</code> in your builds. You've now crippled your CI.</p>
<p>What if you don't need to use Docker build/run, Kaniko or Kubernetes in CI at all? Well ARC may be a good solution for you, until the day you do need to ship a container image.</p>
<h2>3) Can we fix it? Yes we can.</h2>
<p><a href="https://docs.actuated.dev/">Actuated</a> ("cause (a machine or device) to operate.") is a semi-managed solution that we're building at OpenFaaS Ltd.</p>
<p><img src="https://docs.actuated.dev/images/conceptual-high-level.png" alt="A semi-managed solution, where you provide hosts and we do the rest."></p>
<blockquote>
<p>A semi-managed solution, where you provide hosts and we do the rest.</p>
</blockquote>
<p>You provide your own hosts to run jobs, we schedule to them and maintain a VM image with everything you need.</p>
<p>You install our GitHub App, then change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code> or <code>runs-on: actuated-aarch64</code> for ARM.</p>
<p>Then, provision one or more VMs with nested virtualisation enabled on GCP, DigitalOcean or Azure, or a bare-metal host, and <a href="https://docs.actuated.dev/add-agent/">install our agent</a>. That's it.</p>
<p>If you need ARM support for your project, the <a href="https://aws.amazon.com/ec2/instance-types/a1/">a1.metal from AWS</a> is ideal with 16 cores and 32GB RAM, or an <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a> machine like the c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM if you really need to push things. The 2020 M1 Mac Mini also works well with <a href="https://asahilinux.org/">Asahi Linux</a>, and can be maxed out at 16GB RAM / 8 Cores. <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">I even tried Frederic's Parca job on my Raspberry Pi</a> and it was 26m30s quicker than a hosted runner!</p>
<p>Whenever a build is triggered by a repo in your organisation, the control plane will schedule a microVM on one of your own servers, then GitHub takes over from there. When the GitHub runner exits, we forcibly delete the VM.</p>
<p>You get:</p>
<ul>
<li>A fresh, isolated VM for every build, no re-use at all</li>
<li>A fast boot time of ~ &#x3C;1-2s</li>
<li>An immutable image, which is updated regularly and built with automation</li>
<li>Docker preinstalled and running at boot-up</li>
<li>Efficient scheduling and packing of builds to your fleet of hosts</li>
</ul>
<p>It's capable of running Docker and Kubernetes (KinD, kubeadm, K3s) with full isolation. You'll find some <a href="https://docs.actuated.dev/">examples in the docs</a>, but anything that works on a hosted runner we expect to work with actuated also.</p>
<p>Here's what it looks like:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want the deeply technical information and comparisons? <a href="https://docs.actuated.dev/faq/">Check out the FAQ</a></p>
<p>You may also be interested in a debug experience that we're building for GitHub Actions. It can be used to launch a shell session over SSH with hosted and self-hosted runners: <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">Debug GitHub Actions with SSH and launch a cloud shell</a></p>
<h2>Wrapping up</h2>
<p>We're piloting actuated with customers today. If you're interested in faster, more isolated CI without compromising on security, we would like to hear from you.</p>
<p><strong>Register for the pilot</strong></p>
<p>We're looking for customers to participate in our pilot.</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot 📝</a></p>
<p>Actuated is live in pilot and we've already run thousands of VMs for our customers, but we're only just getting started here.</p>
<p><img src="https://blog.alexellis.io/content/images/2022/11/vm-launch.png" alt="VM launch events over the past several days"></p>
<blockquote>
<p>Pictured: VM launch events over the past several days</p>
</blockquote>
<p>Other links:</p>
<ul>
<li><a href="https://docs.actuated.dev/faq/">Read the FAQ</a></li>
<li><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Watch a short video demo</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>
<p><strong>What about GitLab?</strong></p>
<p>We're focusing on GitHub Actions users for the pilot, but have a prototype for GitLab. If you'd like to know more, reach out using the <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Apply for the pilot form</a>.</p>
<p><strong>Just want to play with Firecracker or learn more about microVMs vs legacy VMs and containers?</strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch A cracking time: Exploring Firecracker &#x26; MicroVMs</a></li>
<li><a href="https://github.com/alexellis/firecracker-init-lab">Try my firecracker lab on GitHub - alexellis/firecracker-init-lab</a></li>
</ul>
<h2>What are people saying about actuated?</h2>
<blockquote>
<p>"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff."</p>
<p>Addison van den Hoeven - DevOps Lead, Riskfuel</p>
</blockquote>
<blockquote>
<p>"Actuated looks super cool, interested to see where you take it!"</p>
<p>Guillermo Rauch, CEO Vercel</p>
</blockquote>
<blockquote>
<p>"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here."</p>
<p>Richard Case, Principal Engineer, SUSE</p>
</blockquote>
<blockquote>
<p>"Thank you. I think actuated is amazing."</p>
<p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p>
</blockquote>
<blockquote>
<p>"Nice work, security aspects alone with shared/stale envs on self-hosted runners."</p>
<p>Matt Johnson, Palo Alto Networks</p>
</blockquote>
<blockquote>
<p>"Is there a way to pay github for runners that suck less?"</p>
<p>Darren Shepherd, Acorn Labs</p>
</blockquote>
<blockquote>
<p>"Excited to try out actuated! We use custom actions runners and I think there's something here 🔥"</p>
<p>Nick Gerace, System Initiative</p>
</blockquote>
<blockquote>
<p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running Github Actions in isolated VMs in seconds (vs minutes)."</p>
<p>Rinat Abdullin, ML &#x26; Innovation at Trustbit</p>
</blockquote>
<blockquote>
<p>This is awesome!" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p>
<p>Frederic Branczyk, Co-founder, Polar Signals</p>
</blockquote>]]></content:encoded>
        </item>
    </channel>
</rss>