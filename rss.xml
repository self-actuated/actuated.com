<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Actuated - blog</title>
        <link>https://actuated.dev</link>
        <description>Keep your team productive &amp; focused with blazing fast CI</description>
        <lastBuildDate>Wed, 25 Jan 2023 19:46:47 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <image>
            <title>Actuated - blog</title>
            <url>https://actuated.dev/images/actuated.png</url>
            <link>https://actuated.dev</link>
        </image>
        <item>
            <title><![CDATA[How to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions]]></title>
            <link>https://actuated.dev/blog/sbom-in-github-actions</link>
            <guid>https://actuated.dev/blog/sbom-in-github-actions</guid>
            <pubDate>Wed, 25 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to add a Software Bill of Materials (SBOM) to your containers with GitHub Actions in a few easy steps.]]></description>
            <content:encoded><![CDATA[<h2>What is a Software Bill of Materials (SBOM)?</h2>
<p>In <a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">April 2022 Justin Cormack, CTO of Docker announced</a> that Docker was adding support to generate a Software Bill of Materials (SBOM) for container images.</p>
<p>An SBOM is an inventory of the components that make up a software application. It is a list of the components that make up a software application including the version of each component. The version is important because it can be cross-reference with a vulnerability database to determine if the component has any known vulnerabilities.</p>
<p>Many organisations are also required to company with certain Open Source Software (OSS) licenses. So if SBOMs are included in the software they purchase or consume from vendors, then it can be used to determine if the software is compliant with their specific license requirements, lowering legal and compliance risk.</p>
<p>Docker's enhancements to Docker Desktop and their open source Buildkit tool were the result of a collaboration with Anchore, a company that provides a commercial SBOM solution.</p>
<h2>Check out an SBOM for yourself</h2>
<p>Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free.</p>
<ul>
<li><a href="https://github.com/anchore/syft">syft</a> - a command line tool that can be used to generate an SBOM for a container image.</li>
<li><a href="https://github.com/anchore/grype">grype</a> - a command line tool that can be used to scan an SBOM for vulnerabilities.</li>
</ul>
<p>OpenFaaS Community Edition (CE) is a popular open source serverless platform for Kubernetes. It's maintained by open source developers, and is free to use.</p>
<p>Let's pick a container image from the Community Edition of <a href="https://github.com/orgs/openfaasltd/packages">OpenFaaS</a> like the container image for the OpenFaaS gateway.</p>
<p>We can browse the GitHub UI to find the latest revision, or we can use Google's crane tool:</p>
<pre><code class="hljs language-bash">crane <span class="hljs-built_in">ls</span> ghcr.io/openfaas/gateway | <span class="hljs-built_in">tail</span> -n 5
0.26.0
8e1c34e222d6c194302c649270737c516fe33edf
0.26.1
c26ec5221e453071216f5e15c3409168446fd563
0.26.2
</code></pre>
<p>Now we can introduce one of those tags to syft:</p>
<pre><code class="hljs language-bash">syft ghcr.io/openfaas/gateway:0.26.2
 âœ” Pulled image            
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [39 packages]
NAME                                              VERSION                               TYPE      
alpine-baselayout                                 3.4.0-r0                              apk        
alpine-baselayout-data                            3.4.0-r0                              apk        
alpine-keys                                       2.4-r1                                apk        
apk-tools                                         2.12.10-r1                            apk        
busybox                                           1.35.0                                binary     
busybox                                           1.35.0-r29                            apk        
busybox-binsh                                     1.35.0-r29                            apk        
ca-certificates                                   20220614-r4                           apk        
ca-certificates-bundle                            20220614-r4                           apk        
github.com/beorn7/perks                           v1.0.1                                go-module  
github.com/cespare/xxhash/v2                      v2.1.2                                go-module  
github.com/docker/distribution                    v2.8.1+incompatible                   go-module  
github.com/gogo/protobuf                          v1.3.2                                go-module  
github.com/golang/protobuf                        v1.5.2                                go-module  
github.com/gorilla/mux                            v1.8.0                                go-module  
github.com/matttproud/golang_protobuf_extensions  v1.0.1                                go-module  
github.com/nats-io/nats.go                        v1.22.1                               go-module  
github.com/nats-io/nkeys                          v0.3.0                                go-module  
github.com/nats-io/nuid                           v1.0.1                                go-module  
github.com/nats-io/stan.go                        v0.10.4                               go-module  
github.com/openfaas/faas-provider                 v0.19.1                               go-module  
github.com/openfaas/faas/gateway                  (devel)                               go-module  
github.com/openfaas/nats-queue-worker             v0.0.0-20230117214128-3615ccb286cc    go-module  
github.com/prometheus/client_golang               v1.13.0                               go-module  
github.com/prometheus/client_model                v0.2.0                                go-module  
github.com/prometheus/common                      v0.37.0                               go-module  
github.com/prometheus/procfs                      v0.8.0                                go-module  
golang.org/x/crypto                               v0.5.0                                go-module  
golang.org/x/sync                                 v0.1.0                                go-module  
golang.org/x/sys                                  v0.4.1-0.20230105183443-b8be2fde2a9e  go-module  
google.golang.org/protobuf                        v1.28.1                               go-module  
libc-utils                                        0.7.2-r3                              apk        
libcrypto3                                        3.0.7-r2                              apk        
libssl3                                           3.0.7-r2                              apk        
musl                                              1.2.3-r4                              apk        
musl-utils                                        1.2.3-r4                              apk        
scanelf                                           1.3.5-r1                              apk        
ssl_client                                        1.35.0-r29                            apk        
zlib                                              1.2.13-r0                             apk  
</code></pre>
<p>These are all the components that syft found in the container image. We can see that it found 39 packages, including the OpenFaaS gateway itself.</p>
<p>Some of the packages are Go modules, others are packages that have been installed with <code>apk</code> (Alpine Linux's package manager).</p>
<h2>Checking for vulnerabilities</h2>
<p>Now that we have an SBOM, we can use grype to check for vulnerabilities.</p>
<pre><code class="hljs language-bash">grype ghcr.io/openfaas/gateway:0.26.2
 âœ” Vulnerability DB        [no update available]
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [39 packages]
 âœ” Scanned image           [2 vulnerabilities]
NAME                        INSTALLED  FIXED-IN  TYPE       VULNERABILITY   SEVERITY 
google.golang.org/protobuf  v1.28.1              go-module  CVE-2015-5237   High      
google.golang.org/protobuf  v1.28.1              go-module  CVE-2021-22570  Medium  
</code></pre>
<p>In this instance, we can see there are only two vulnerabilities, both of which are in the <code>google.golang.org/protobuf</code> Go module, and neither of them have been fixed yet.</p>
<ul>
<li>As the maintainer of OpenFaaS CE, I could try to eliminate the dependency from the original codebase, or wait for a workaround to be published by its vendor.</li>
<li>As a consumer of OpenFaaS CE my choices are similar, and it may be worth trying to look into the problem myself to see if the vulnerability is relevant to my use case.</li>
<li>Now, for OpenFaaS Pro, a commercial distribution of OpenFaaS, where source is not available, I'd need to contact the vendor OpenFaaS Ltd and see if they could help, or if they could provide a workaround. Perhaps there would even be a paid support relationship and SLA relating to fixing vulnerabilities of this kind?</li>
</ul>
<p>With this scenario, I wanted to show that different people care about the supply chain, and have different responsibilities for it.</p>
<h2>Generate an SBOM from within GitHub Actions</h2>
<p>The examples above were all run locally, but we can also generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.</p>
<p>Imagine you have the following Dockerfile:</p>
<pre><code>FROM alpine:3.17.0

RUN apk add --no-cache curl ca-certificates

CMD ["curl", "https://www.google.com"]
</code></pre>
<p>I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use <code>3.17.1</code> to fix a specific vulnerability.</p>
<p>Now a typical workflow for this Dockerfile would look like the below:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">build</span>

<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]
  <span class="hljs-attr">pull_request:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">master</span>, <span class="hljs-string">main</span> ]

<span class="hljs-attr">permissions:</span>
  <span class="hljs-attr">actions:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">checks:</span> <span class="hljs-string">write</span>
  <span class="hljs-attr">contents:</span> <span class="hljs-string">read</span>
  <span class="hljs-attr">packages:</span> <span class="hljs-string">write</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">publish:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Buildx</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/setup-buildx-action@v2</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Login</span> <span class="hljs-string">to</span> <span class="hljs-string">Docker</span> <span class="hljs-string">Registry</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/login-action@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">username:</span> <span class="hljs-string">${{</span> <span class="hljs-string">github.repository_owner</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">password:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.GITHUB_TOKEN</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">registry:</span> <span class="hljs-string">ghcr.io</span>

      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Publish</span> <span class="hljs-string">image</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">build-args:</span> <span class="hljs-string">|
            GitCommit=${{ github.sha }}
</span>          <span class="hljs-attr">outputs:</span> <span class="hljs-string">"type=registry,push=true"</span>
          <span class="hljs-attr">tags:</span> <span class="hljs-string">|
            ghcr.io/alexellis/gha-sbom:${{ github.sha }}
</span></code></pre>
<p>Upon each commit, an image is published to GitHub's Container Registry with the image name of: <code>ghcr.io/alexellis/gha-sbom:SHA</code>.</p>
<p>To generate an SBOM, we just need to update the <code>docker/build-push-action</code> to use the <code>sbom</code> flag:</p>
<pre><code class="hljs language-yaml">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Local</span> <span class="hljs-string">build</span>
        <span class="hljs-attr">id:</span> <span class="hljs-string">local_build</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">docker/build-push-action@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">sbom:</span> <span class="hljs-literal">true</span>
          <span class="hljs-attr">provenance:</span> <span class="hljs-literal">false</span>
</code></pre>
<p>By checking the logs from the action, we can see that the image has been published with an SBOM:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment">#16 [linux/amd64] generating sbom using docker.io/docker/buildkit-syft-scanner:stable-1</span>
<span class="hljs-comment">#0 0.120 time="2023-01-25T15:35:19Z" level=info msg="starting syft scanner for buildkit v1.0.0"</span>
<span class="hljs-comment">#16 DONE 1.0s</span>
</code></pre>
<p>The SBOM can be viewed as before:</p>
<pre><code class="hljs language-bash">syft ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a

grype ghcr.io/alexellis/gha-sbom:46bc16cb4033364233fad3caf8f3a255b5b4d10d@sha256:7229e15004d8899f5446a40ebdd072db6ff9c651311d86e0c8fd8f999a32a61a
 âœ” Vulnerability DB        [updated]
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [21 packages]
 âœ” Scanned image           [2 vulnerabilities]
NAME        INSTALLED  FIXED-IN  TYPE  VULNERABILITY  SEVERITY 
libcrypto3  3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High      
libssl3     3.0.7-r0   3.0.7-r2  apk   CVE-2022-3996  High  
</code></pre>
<p>The image: <code>alpine:3.17.0</code> contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed.</p>
<p>We can resolve the issue by changing the Dockerfile to use <code>alpine:3.17.1</code> instead, and re-running the build.</p>
<pre><code class="hljs language-bash">grype ghcr.io/alexellis/gha-sbom:63c6952d1ded1f53b1afa3f8addbba9efa37b52b
 âœ” Vulnerability DB        [no update available]
 âœ” Pulled image            
 âœ” Loaded image            
 âœ” Parsed image            
 âœ” Cataloged packages      [21 packages]
 âœ” Scanned image           [0 vulnerabilities]
No vulnerabilities found
</code></pre>
<h2>Wrapping up</h2>
<p>There is a lot written on the topic of supply chain security, so I wanted to give you a quick overview, and how to get started wth it.</p>
<p>We looked at Anchore's two open source tools: Syft and Grype, and how they can be used to generate an SBOM and scan for vulnerabilities.</p>
<p>We then produced an SBOM for a pre-existing Dockerfile and GitHub Action, introducing a vulnerability by using an older base image, and then fixing it by upgrading it. We did this by adding additional flags to the <code>docker/build-push-action</code>. We added the sbom flag, and set the provenance flag to false. Provenance is a separate but related topic, which is explained well in an article by Justin Chadwell of Docker (linked below).</p>
<p>I maintain an Open Source alternative to brew for developer-focused CLIs called <a href="https://arkade.dev/">arkade</a>. This already includes Google's crane project, and there's a <a href="https://github.com/alexellis/arkade/issues/839">Pull Request coming shortly to add Syft and Grype to the project</a>.</p>
<p>It can be a convenient way to install these tools on MacOS, Windows or Linux:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Available now</span>
arkade get crane syft

<span class="hljs-comment"># Coming shortly</span>
arkade get grype
</code></pre>
<p>In the beginning of the article we mentioned license compliance. SBOMs generated by syft do not seem to include license information, but in my experience, corporations which take this risk seriously tend to run their own scanning infrastructure <a href="https://www.synopsys.com/software-integrity/security-testing/software-composition-analysis.html">with commercial tools like Blackduck</a> or <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/21-08/prisma-cloud-compute-edition-admin/compliance/manage_compliance">Twistlock</a>.</p>
<p>Tools like Twistlock, and certain registries like <a href="https://jfrog.com/artifactory/">JFrog Artifactory</a> and the <a href="https://goharbor.io/">CNCF's Harbor</a>, can be configured to scan images. GitHub has a free, built-in service called Dependabot that won't just scan, but will also send Pull Requests to fix issues.</p>
<p>But with the SBOM approach, the responsibility is rebalanced, with the supplier taking on an active role in security. The consumer can then use the supplier's SBOMs, or run their own scanning infrastructure - or perhaps both.</p>
<p>See also:</p>
<ul>
<li><a href="https://www.docker.com/blog/announcing-docker-sbom-a-step-towards-more-visibility-into-docker-images/">Announcing Docker SBOM: A step towards more visibility into Docker images- Justin Cormack</a></li>
<li><a href="https://www.docker.com/blog/generate-sboms-with-buildkit/">Generating SBOMs for Your Image with BuildKit - Justin Chadwell</a></li>
<li><a href="https://github.com/anchore">Anchore on GitHub</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Is the GitHub Actions self-hosted runner safe for Open Source?]]></title>
            <link>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</link>
            <guid>https://actuated.dev/blog/is-the-self-hosted-runner-safe-github-actions</guid>
            <pubDate>Fri, 20 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub warns against using self-hosted Actions runners for public repositories - but why? And are there alternatives?]]></description>
            <content:encoded><![CDATA[<p>First of all, why would someone working on an open source project need a self-hosted runner?</p>
<p>Having contributed to dozens of open source projects, and gotten to know many different maintainers, the primary reason tends to be out of necessity. They face an 18 minute build time upon every commit or Pull Request revision, and want to make the best of what little time they can give over to Open Source.</p>
<p>Having faster builds also lowers friction for contributors, and since many contributors are unpaid and rely on their own internal drive and enthusiasm, a fast build time can be the difference between them fixing a broken test or waiting another few days.</p>
<p>To sum up, there are probably just a few main reasons:</p>
<ol>
<li>Faster builds, higher concurrency, more disk space</li>
<li>Needing to build and test Arm binaries or containers on real hardware</li>
<li>Access to services on private networks</li>
</ol>
<p>The first point is probably one most people can relate to. Simply by provisioning an AMD bare-metal host, or a high spec VM with NVMe, you can probably shave minutes off a build.</p>
<p>For the second case, some projects like <a href="https://github.com/fluxcd/flagger">Flagger</a> from the CNCF felt their only option to support users deploying to AWS Graviton, was to seek sponsorship for a large Arm server and to install a self-hosted runner on it.</p>
<p>The third option is more nuanced, and specialist. This may or may not be something you can relate to, but it's worth mentioning. VPNs have very limited speed and there may be significant bandwidth costs to transfer data out of a region into GitHub's hosted runner environment. Self-hosted runners eliminate the cost and give full local link bandwidth, even as high as 10GbE. You just won't get anywhere near that with IPSec or Wireguard over the public Internet.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<h2>But are self-hosted runners safe?</h2>
<p>The GitHub team has a stark warning for those of us who are tempted to deploy a self-hosted runner and to connect it to a public repository.</p>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs. Some of the risks include:</p>
<ul>
<li>Malicious programs running on the machine.</li>
<li>Escaping the machine's runner sandbox.</li>
<li>Exposing access to the machine's network environment.</li>
<li>Persisting unwanted or dangerous data on the machine.</li>
</ul>
</blockquote>
<p>See also: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners#self-hosted-runner-security">Self-hosted runner security</a></p>
<p>Now you may be thinking "I won't approve pull requests from bad actors", but quite often the workflow goes this way: the contributor gets approval, then you don't need to approve subsequent pull requests after that.</p>
<p>An additional risk is if that user's account is compromised, then the attacker can submit a pull request with malicious code or malware. There is no way in GitHub to enforce Multi-Factor Authentication (MFA) for pull requests, even if you have it enabled on your Open Source Organisation.</p>
<p>Here are a few points to consider:</p>
<ul>
<li>Side effects build up with every build, making it less stable over time</li>
<li>You can't enforce MFA for pull requests - so malware can be installed directly on the host - whether intentionally or not</li>
<li>The GitHub docs warn that users can take over your account</li>
<li>Each runner requires maintenance and updates of the OS and all the required packages</li>
</ul>
<p>The chances are that if you're running the Flagger or Network Service Mesh project, you are shipping code that enterprise companies will deploy in production with sensitive customer data.</p>
<p>If you are not worried, try explaining the above to them, to see how they may see the risk differently.</p>
<h2>Doesn't Kubernetes fix all of this?</h2>
<p><a href="https://kubernetes.io/">Kubernetes</a> is a well known platform built for orchestrating containers. It's especially suited to running microservices, webpages and APIs, but has support for batch-style workloads like CI runners too.</p>
<p>You could make a container image and install the self-hosted runner binary within in, then deploy that as a Pod to a cluster. You could even scale it up with a few replicas.</p>
<p>If you are only building Java code, Python or Node.js, you may find this resolves many of the issues that we covered above, but it's hard to scale, and you still get side-effects as the environment is not immutable.</p>
<p>That's where the community project "actions-runtime-controller" or ARC comes in. It's a controller that launches a pool of Pods with the self-hosted runner.</p>
<blockquote>
<p>How much work does ARC need?</p>
<p>Some of the teams I have interviewed over the past 3 months told me that ARC took them a lot of time to set up and maintain, whilst others have told us it was a lot easier for them. It may depend on your use-case, and whether you're more of a personal user, or part of a team with 10-30 people committing code several times per day.
The first customer for actuated, which I'll mention later in the article was a team of ~ 20 people who were using ARC and had grew tired of the maintenance overhead and certain reliability issues.</p>
</blockquote>
<p>Unfortunately, by default ARC uses the same Pod many times as a persistent runner, so side effects still build up, malware can still be introduced and you have to maintain a Docker image with all the software needed for your builds.</p>
<p>You may be happy with those trade-offs, especially if you're only building private repositories.</p>
<p>But those trade-offs gets a lot worse if you use Docker or Kubernetes.</p>
<p>Out of the box, you simply cannot start a Docker container, build a container image or start a Kubernetes cluster.</p>
<p>And to do so, you'll need to resort to what can only be described as dangerous hacks:</p>
<ol>
<li>You expose the Docker socket from the host, and mount it into each Pod - any CI job can take over the host, game over.</li>
<li>You run in <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker in Docker (DIND)</a> mode. DIND requires a privileged Pod, which means that any CI job can take over the host, game over.</li>
</ol>
<p>There is some early work on running Docker In Docker in user-space mode, but this is slow, tricky to set up and complicated. By default, user-space mode uses a non-root account. So you can't install software packages or run commands like apt-get.</p>
<p>See also: <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Using Docker-in-Docker for your CI or testing environment? Think twice.</a></p>
<p>Have you heard of Kaniko?</p>
<p><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> is a tool for building container images from a Dockerfile, without the need for a Docker daemon. It's a great option, but it's not a replacement for running containers, it can only build them.</p>
<p>And when it builds them, in nearly every situation it will need root access in order to mount each layer to build up the image.</p>
<p>See also: <a href="https://suraj.io/post/root-in-container-root-on-host/">The easiest way to prove that root inside the container is also root on the host</a></p>
<p>And what about Kubernetes?</p>
<p>To run a KinD, Minikube or K3s cluster within your CI job, you're going to have to sort to one of the dangerous hacks we mentioned earlier which mean a bad actor could potentially take over the host.</p>
<p>Some of you may be running these Kubernetes Pods in your production cluster, whilst others have taken some due diligence and deployed a separate cluster just for these CI workloads. I think that's a slightly better option, but it's still not ideal and requires even more access control and maintenance.</p>
<p>Ultimately, there is a fine line between overconfidence and negligence. When building code on a public repository, we have to assume that the worst case scenario will happen one day. When using DinD or privileged containers, we're simply making that day come sooner.</p>
<p>Containers are great for running internal microservices and Kubernetes excels here, but there is a reason that AWS insists on hard multi-tenancy with Virtual Machines for their customers.</p>
<blockquote>
<p>See also: <a href="https://www.amazon.science/publications/firecracker-lightweight-virtualization-for-serverless-applications">Firecracker whitepaper</a></p>
</blockquote>
<h2>What's the alternative?</h2>
<p>When GitHub cautioned us against using self-hosted runners, on public repos, they also said:</p>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<p>So using GitHub's hosted runners are probably the most secure option for Open Source projects and for public repositories - if you are happy with the build speed, and don't need Arm runners.</p>
<p>But that's why I'm writing this post, sometimes we need faster builds, or access to specialist hardware like Arm servers.</p>
<p>The Kubernetes solution is fast, but it uses a Pod which runs many jobs, and in order to make it useful enough to run <code>docker run</code>, <code>docker build</code> or to start a Kubernetes cluster, we have to make our machines vulnerable.</p>
<p>With actuated, we set out to re-build the same user experience as GitHub's hosted runners, but without the downsides of self-hosted runners or using Kubernetes Pods for runners.</p>
<p>Actuated runs each build in a microVM on servers that you alone provision and control.</p>
<p>Its centralised control-plane schedules microVMs to each server using an immutable Operating System that is re-built with automation and kept up to date with the latest security patches.</p>
<p>Once the microVM has launched, it connects to GitHub, receives a job, runs to completion and is completely erased thereafter.</p>
<p>You get all of the upsides of self-hosted runners, with a user experience that is as close to GitHub's hosted runners as possible.</p>
<p>Pictured - an Arm Server with 270 GB of RAM and 80 cores - that's a lot of builds.</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62k4gXkAMHX-B?format=jpg&#x26;name=large" alt=""></a></p>
<p>You get to run the following, without worrying about security or side-effects:</p>
<ul>
<li>Docker (<code>docker run</code>) and <code>docker build</code></li>
<li>Kubernetes - Minikube, K3s, KinD</li>
<li><code>sudo</code> / root commands</li>
</ul>
<p>Need to test against a dozen different Kubernetes versions?</p>
<p>Not a problem:</p>
<p><img src="https://actuated.dev/images/k3sup.png" alt="Testing multiple Kubernetes versions"></p>
<p>What about running the same on Arm servers?</p>
<p>Just change <code>runs-on: actuated</code> to <code>runs-on: actuated-aarch64</code> and you're good to go. We test and maintain support for Docker and Kubernetes for both Intel and Arm CPU architectures.</p>
<p>Do you need insights for your Open Source Program Office (OSPO) or for the Technical Steering Committee (TSC)?</p>
<p><a href="https://twitter.com/alexellisuk/status/1616430466042560514/"><img src="https://pbs.twimg.com/media/Fm62kSTXgAQLzUb?format=jpg&#x26;name=medium" alt=""></a></p>
<p>We know that no open source project has a single repository that represents all of its activity. Actuated provides insights across an organisation, including total build time and the time queued - which is a reflection of whether you could do with more or fewer build machines.</p>
<p>And we are only just getting started with compiling insights, there's a lot more to come.</p>
<h2>Get involved today</h2>
<p>We've already launched 10,000 VMs for customers jobs, and are now ready to open up the platform to the wider community. So if you'd like to try out what we're offering, we'd love to hear from you. As you offer feedback, you'll get hands on support from our engineering team and get to shape the product through collaboration.</p>
<p>So what does it cost? There is a subscription fee which includes - the control plane for your organisation, the agent software, maintenance of the OS images and our support via Slack. But all the plans are flat-rate, so it may even work out cheaper than paying GitHub for the bigger instances that they offer.</p>
<p>Professional Open Source developers like the ones you see at Red Hat, VMware, Google and IBM, that know how to work in community and understand cloud native are highly sought after and paid exceptionally well. So the open source project you work on has professional full-time engineers allocated to it by one or more companies, as is often the case, then using actuated could pay for itself in a short period of time.</p>
<p>If you represent an open source project that has no funding and is purely maintained by volunteers, what we have to offer may not be suited to your current position. And in that case, we'd recommend you stick with the slower GitHub Runners. Who knows? Perhaps one day GitHub may offer sponsored faster runners at no cost for certain projects?</p>
<p>And finally, what if your repositories are private? Well, we've made you aware of the trade-offs with a static self-hosted runner, or running builds within Kubernetes. It's up to you to decide what's best for your team, and your customers. Actuated works just as well with private repositories as it does with public ones.</p>
<p>See microVMs launching in ~ 1s during a matrix build for testing a Custom Resource Definition (CRD) on different Kubernetes versions:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want to know how actuated works? <a href="https://docs.actuated.dev/faq/">Read the FAQ for more technical details</a>.</p>
<ul>
<li><a href="https://docs.actuated.dev/register/">Find a server for your builds</a></li>
<li><a href="https://docs.actuated.dev/provision-server/">Register for actuated</a></li>
</ul>
<p>Follow us on Twitter - <a href="https://twitter.com/selfactuated">selfactuated</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to make GitHub Actions 22x faster with bare-metal Arm]]></title>
            <link>https://actuated.dev/blog/native-arm64-for-github-actions</link>
            <guid>https://actuated.dev/blog/native-arm64-for-github-actions</guid>
            <pubDate>Tue, 17 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub doesn't provide hosted Arm runners, so how can you use native Arm runners safely & securely?]]></description>
            <content:encoded><![CDATA[<p>GitHub Actions is a modern, fast and efficient way to build and test software, with free runners available. We use the free runners for various open source projects and are generally very pleased with them, after all, who can argue with good enough and free? But one of the main caveats is that GitHub's hosted runners don't yet support the Arm architecture.</p>
<p>So many people turn to software-based emulation using <a href="https://www.qemu.org/">QEMU</a>. QEMU is tricky to set up, and requires specific code and tricks if you want to use software in a standard way, without modifying it. But QEMU is great when it runs with hardware acceleration. Unfortunately, the hosted runners on GitHub do not have KVM available, so builds tend to be incredibly slow, and I mean so slow that it's going to distract you and your team from your work.</p>
<p>This was even more evident when <a href="https://twitter.com/fredbrancz">Frederic Branczyk</a> tweeted about his experience with QEMU on <a href="https://github.com/features/actions">GitHub Actions</a> for his open source observability project named <a href="https://github.com/parca-dev/parca">Parca</a>.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does anyone have a <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> actions self-hosted runner manifest for me to throw at a <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> cluster? I&#39;m tired of waiting for emulated arm64 CI runs taking ages.</p>&mdash; Frederic ðŸ§Š Branczyk @brancz@hachyderm.io (@fredbrancz) <a href="https://twitter.com/fredbrancz/status/1582779459379204096?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I checked out his build and expected "ages" to mean 3 minutes, in fact, it meant 33.5 minutes. I know because I forked his project and ran a test build.</p>
<p>After migrating it to actuated and one of our build agents, the time dropped to 1 minute and 26 seconds, a 22x improvement for zero effort.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>You can see the results here:</p>
<p><a href="https://twitter.com/alexellisuk/status/1583089248084729856/photo/1"><img src="https://pbs.twimg.com/media/FfhC5z1XkAAoYjn?format=jpg&#x26;name=large" alt="Results from the test, side-by-side"></a></p>
<p>As a general rule, the download speed is going to be roughly the same with a hosted runner, it may even be slightly faster due to the connection speed of Azure's network.</p>
<p>But the compilation times speak for themselves - in the Parca build, <code>go test</code> was being run with QEMU. Moving it to run on the ARM64 host directly, resulted in the marked increase in speed. In fact, the team had introduced lots of complicated code to try and set up a Docker container to use QEMU, all that could be stripped out, replacing it with a very standard looking test step:</p>
<pre><code class="hljs language-yaml">  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">the</span> <span class="hljs-string">go</span> <span class="hljs-string">tests</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">go</span> <span class="hljs-string">test</span> <span class="hljs-string">./...</span>
</code></pre>
<h2>Can't I just install the self-hosted runner on an Arm VM?</h2>
<p>There are relatively cheap Arm VMs available from Oracle OCI, Google and Azure based upon the Ampere Altra CPU. AWS have their own Arm VMs available in the Graviton line.</p>
<p>So why shouldn't you just go ahead and install the runner and add them to your repos?</p>
<p>The moment you do that you run into three issues:</p>
<ul>
<li>You now have to maintain the software packages installed on that machine</li>
<li>If you use KinD or Docker, you're going to run into conflicts between builds</li>
<li>Out of the box scheduling is poor - by default it only runs one build at a time there</li>
</ul>
<p>Chasing your tail with package updates, faulty builds due to caching and conflicts is not fun, you may feel like you're saving money, but you are paying with your time and if you have a team, you're paying with their time too.</p>
<p>Most importantly, GitHub say that it cannot be used safely with a public repository. There's no security isolation, and state can be left over from one build to the next, including harmful code left intentionally by bad actors, or accidentally from malware.</p>
<h2>So how do we get to a safer, more efficient Arm runner?</h2>
<p>The answer is to get us as close as possible to a hosted runner, but with the benefits of a self-hosted runner.</p>
<p>That's where actuated comes in.</p>
<p>We run a SaaS that manages bare-metal for you, and talks to GitHub upon your behalf to schedule jobs efficiently.</p>
<ul>
<li>No need to maintain software, we do that for you with an automated OS image</li>
<li>We use microVMs to isolate builds from each other</li>
<li>Every build is immutable and uses a clean environment</li>
<li>We can schedule multiple builds at once without side-effects</li>
</ul>
<p>microVMs on Arm require a bare-metal server, and we have tested all the options available to us. Note that the Arm VMs discussed above do not currently support KVM or nested virtualisation.</p>
<ul>
<li>a1.metal on AWS - 16 cores / 32GB RAM - 300 USD / mo</li>
<li>c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM - 2.5 USD / hr</li>
<li><a href="https://www.hetzner.com/dedicated-rootserver/matrix-rx">RX-Line</a> from <a href="https://hetzner.com">Hetzner</a> with 128GB / 256GB RAM, NVMe &#x26; 80 cores for approx 200-250 EUR / mo.</li>
<li><a href="https://amzn.to/3WiSDE7">Mac Mini M1</a> - 8 cores / 16GB RAM - tested with Asahi Linux - one-time payment of ~ 1500 USD</li>
</ul>
<p>If you're already an AWS customer, the a1.metal is a good place to start. If you need expert support, networking and a high speed uplink, you can't beat Equinix Metal (we have access to hardware there and can help you get started) - you can even pay per minute and provision machines via API. The Mac Mini &#x3C;1 has a really fast NVMe and we're running one of these with Asahi Linux for our own Kernel builds for actuated. The RX Line from Hetzner has serious power and is really quite affordable, but just be aware that you're limited to a 1Gbps connection, a setup fee and monthly commitment, unless you pay significantly more.</p>
<p>I even tried Frederic's Parca job <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">on my 8GB Raspberry Pi with a USB NVMe</a>. Why even bother, do I hear you say? Well for a one-time payment of 80 USD, it was 26m30s quicker than a hosted runner with QEMU!</p>
<p><a href="https://alexellisuk.medium.com/upgrade-your-raspberry-pi-4-with-a-nvme-boot-drive-d9ab4e8aa3c2">Learn how to connect an NVMe over USB-C to your Raspberry Pi 4</a></p>
<h2>What does an Arm job look like?</h2>
<p>Since I first started trying to build code for Arm in 2015, I noticed a group of people who had a passion for this efficient CPU and platform. They would show up on GitHub issue trackers, ready to send patches, get access to hardware and test out new features on Arm chips. It was a tough time, and we should all be grateful for their efforts which go largely unrecognised.</p>
<blockquote>
<p>If you're looking to make your <a href="https://twitter.com/alexellisuk">software compatible with Arm</a>, feel free to reach out to me via Twitter.</p>
</blockquote>
<p>In 2020 when Apple released their M1 chip, Arm went mainstream, and projects that had been putting off Arm support like KinD and Minikube, finally had that extra push to get it done.</p>
<p>I've had several calls with teams who use Docker on their M1/M2 Macs exclusively, meaning they build only Arm binaries and use only Arm images from the Docker Hub. Some of them even ship to project using Arm images, but I think we're still a little behind the curve there.</p>
<p>That means Kubernetes - KinD/Minikube/K3s and Docker - including Buildkit, compose etc, all work out of the box.</p>
<p>I'm going to use the arkade CLI to download KinD and kubectl, however you can absolutely find the download links and do all this manually. I don't recommend it!</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">e2e-kind-test</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">push</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">start-kind:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">fetch-depth:</span> <span class="hljs-number">1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">arkade</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/setup-arkade@v1</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">get</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">and</span> <span class="hljs-string">kubectl</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">alexellis/arkade-get@master</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">kubectl:</span> <span class="hljs-string">latest</span>
          <span class="hljs-attr">kind:</span> <span class="hljs-string">latest</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Create</span> <span class="hljs-string">a</span> <span class="hljs-string">KinD</span> <span class="hljs-string">cluster</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          mkdir -p $HOME/.kube/
          kind create cluster --wait 300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Wait</span> <span class="hljs-string">until</span> <span class="hljs-string">CoreDNS</span> <span class="hljs-string">is</span> <span class="hljs-string">ready</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|
          kubectl rollout status deploy/coredns -n kube-system --timeout=300s
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">nodes</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">nodes</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Explore</span> <span class="hljs-string">pods</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">get</span> <span class="hljs-string">pod</span> <span class="hljs-string">-A</span> <span class="hljs-string">-o</span> <span class="hljs-string">wide</span>
</code></pre>
<p>That's our <code>x86_64</code> build, or Intel/AMD build that will run on a hosted runner, but will be kind of slow.</p>
<p>Let's convert it to run on an actuated ARM64 runner:</p>
<pre><code class="hljs language-diff">jobs:
  start-kind:
<span class="hljs-deletion">-    runs-on: ubuntu-latest</span>
<span class="hljs-addition">+    runs-on: actuated-aarch64</span>
</code></pre>
<p>That's it, we've changed the runner type and we're ready to go.</p>
<p><img src="/images/2023-native-arm64-for-oss/in-progress-dashboard.png" alt="In progress build on the dashboard"></p>
<blockquote>
<p>An in progress build on the dashboard</p>
</blockquote>
<p>Behind the scenes, actuated, the SaaS schedules the build on a bare-metal ARM64 server, the boot up takes less than 1 second, and then the standard GitHub Actions Runner talks securely to GitHub to run the build. The build is isolated from other builds, and the runner is destroyed after the build is complete.</p>
<p><img src="/images/2023-native-arm64-for-oss/arm-kind.png" alt="Setting up an Arm KinD cluster took about 49s"></p>
<blockquote>
<p>Setting up an Arm KinD cluster took about 49s</p>
</blockquote>
<p>Setting up an Arm KinD cluster took about 49s, then it's over to you to test your Arm images, or binaries.</p>
<p>If I were setting up CI and needed to test software on both Arm and x86_64, then I'd probably create two separate builds, one for each architecture, with a <code>runs-on</code> label of <code>actuated</code> and <code>actuated-aarch64</code> respectively.</p>
<p>Do you need to test multiple versions of Kubernetes? Let's face it, it changes so often, that who doesn't need to do that. You can use the <code>matrix</code> feature to test multiple versions of Kubernetes on Arm and x86_64.</p>
<p>I show 5x clusters being launched in parallel in the video below:</p>
<p><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Demo - Actuated - secure, isolated CI for containers and Kubernetes</a></p>
<p>What about Docker?</p>
<p>Docker comes pre-installed in the actuated OS images, so you can simply use <code>docker build</code>, without any need to install extra tools like Buildx, or to have to worry about multi-arch Dockerfiles. Although these are always good to have, and are <a href="https://github.com/openfaas/golang-http-template/blob/master/template/golang-middleware/Dockerfile">available out of the box in OpenFaaS</a>, if you're curious what a multi-arch Dockerfile looks like.</p>
<h2>Wrapping up</h2>
<p>Building on bare-metal Arm hosts is more secure because side effects cannot be left over between builds, even if malware is installed by a bad actor. It's more efficient because you can run multiple builds at once, and you can use the latest software with our automated Operating System image. Enabling actuated on a build is as simple as changing the runner type.</p>
<p>And as you've seen from the example with the OSS Parca project, moving to a native Arm server can improve speed by 22x, shaving off a massive 34 minutes per build.</p>
<p>Who wouldn't want that?</p>
<p>Parca isn't a one-off, I was also told by <a href="https://twitter.com/cohix">Connor Hicks from Suborbital</a> that they have an Arm build that takes a good 45 minutes due to using QEMU.</p>
<p>Just a couple of days ago <a href="https://twitter.com/edwarnicke?lang=en">Ed Warnicke, Distinguished Engineer at Cisco</a> reached out to us to pilot actuated. Why?</p>
<p>Ed, who had <a href="https://networkservicemesh.io/">Network Service Mesh</a> in mind said:</p>
<blockquote>
<p>I'd kill for proper Arm support. I'd love to be able to build our many containers for Arm natively, and run our KIND based testing on Arm natively.
We want to build for Arm - Arm builds is what brought us to actuated</p>
</blockquote>
<p>So if that sounds like where you are, reach out to us and we'll get you set up.</p>
<ul>
<li><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register to pilot actuated with us</a></li>
</ul>
<p>Additional links:</p>
<ul>
<li><a href="https://docs.actuated.dev/">Actuated docs</a></li>
<li><a href="https://docs.actuated.dev/faq">FAQ &#x26; comparison to other solutions</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blazing fast CI with MicroVMs]]></title>
            <link>https://actuated.dev/blog/blazing-fast-ci-with-microvms</link>
            <guid>https://actuated.dev/blog/blazing-fast-ci-with-microvms</guid>
            <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[I saw an opportunity to fix self-hosted runners for GitHub Actions. Actuated is now in pilot and aims to solve most if not all of the friction.]]></description>
            <content:encoded><![CDATA[<p>Around 6-8 months ago I started exploring MicroVMs out of curiosity. Around the same time, I saw an opportunity to <strong>fix</strong> self-hosted runners for GitHub Actions. <a href="https://docs.actuated.dev/">Actuated</a> is now in pilot and aims to solve <a href="https://twitter.com/alexellisuk/status/1573599285362532353?s=20&#x26;t=dFcd54c4KIynk6vIGTb7QA">most if not all of the friction</a>.</p>
<p>There's three parts to this post:</p>
<ol>
<li>A quick debrief on Firecracker and MicroVMs vs legacy solutions</li>
<li>Exploring friction with GitHub Actions from a hosted and self-hosted perspective</li>
<li>Blazing fast CI with Actuated, and additional materials for learning more about Firecracker</li>
</ol>
<blockquote>
<p>We're looking for customers who want to solve the problems explored in this post.
<a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot</a></p>
</blockquote>
<h2>1) A quick debrief on Firecracker ðŸ”¥</h2>
<blockquote>
<p>Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services.</p>
</blockquote>
<p>I learned about <a href="https://github.com/firecracker-microvm/firecracker">Firecracker</a> mostly by experimentation, building bigger and more useful prototypes. This helped me see what the experience was going to be like for users and the engineers working on a solution. I met others in the community and shared notes with them. Several people asked "Are microVMs the next thing that will replace containers?" I don't think they are, but they are an important tool where hard isolation is necessary.</p>
<p>Over time, one thing became obvious:</p>
<blockquote>
<p>MicroVMs fill a need that legacy VMs and containers can't.</p>
</blockquote>
<p>If you'd like to know more about how Firecracker works and how it compares to traditional VMs and Docker, you can replay my deep dive session with Richard Case, Principal Engineer (previously Weaveworks, now at SUSE).</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CYCsa5e2vqg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<blockquote>
<p>Join Alex and Richard Case for a cracking time. The pair share what's got them so excited about Firecracker, the kinds of use-cases they see for microVMs, fundamentals of Linux Operating Systems and plenty of demos.</p>
</blockquote>
<h2>2) So what's wrong with GitHub Actions?</h2>
<p>First let me say that I think GitHub Actions is a far better experience than Travis ever was, and we have moved all our CI for OpenFaaS, inlets and actuated to Actions for public and private repos. We've built up a good working knowledge in the community and the company.</p>
<p>I'll split this part into two halves.</p>
<h3>What's wrong with hosted runners?</h3>
<p><strong>Hosted runners are constrained</strong></p>
<p>Hosted runners are incredibly convenient, and for most of us, that's all we'll ever need, especially for public repositories with fast CI builds.</p>
<p>Friction starts when the 7GB of RAM and 2 cores allocated causes issues for us - like when we're launching a KinD cluster, or trying to run E2E tests and need more power. Running out of disk space is also a common problem when using Docker images.</p>
<p>GitHub recently launched new paid plans to get faster runners, however the costs add up, the more you use them.</p>
<p>What if you could pay a flat fee, or bring your own hardware?</p>
<p><strong>They cannot be used with public repos</strong></p>
<p>From GitHub.com:</p>
<blockquote>
<p>We recommend that you only use self-hosted runners with private repositories. This is because forks of your public repository can potentially run dangerous code on your self-hosted runner machine by creating a pull request that executes the code in a workflow.</p>
</blockquote>
<blockquote>
<p>This is not an issue with GitHub-hosted runners because each GitHub-hosted runner is always a clean isolated virtual machine, and it is destroyed at the end of the job execution.</p>
</blockquote>
<blockquote>
<p>Untrusted workflows running on your self-hosted runner pose significant security risks for your machine and network environment, especially if your machine persists its environment between jobs.</p>
</blockquote>
<p>Read more about the risks: <a href="https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners">Self-hosted runner security</a></p>
<p>Despite a stern warning from GitHub, at least one notable CNCF project runs self-hosted ARM64 runners on public repositories.</p>
<p>On one hand, I don't blame that team, they have no other option if they want to do open source, it means a public repo, which means risking everything knowingly.</p>
<p>Is there another way we can help them?</p>
<p>I spoke to the GitHub Actions engineering team, who told me that using an ephemeral VM and an immutable OS image would solve the concerns.</p>
<p><strong>There's no access to ARM runners</strong></p>
<p>Building with QEMU is incredibly slow as Frederic Branczyk, Co-founder, Polar Signals found out when his Parca project was taking 33m5s to build.</p>
<p>I forked it and changed a line: <code>runs-on: actuated-aarch64</code> and reduced the total build time to 1m26s.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This morning <a href="https://twitter.com/fredbrancz?ref_src=twsrc%5Etfw">@fredbrancz</a> said that his ARM64 build was taking 33 minutes using QEMU in a GitHub Action and a hosted runner.<br><br>I ran it on <a href="https://twitter.com/selfactuated?ref_src=twsrc%5Etfw">@selfactuated</a> using an ARM64 machine and a microVM.<br><br>That took the time down to 1m 26s!! About a 22x speed increase. <a href="https://t.co/zwF3j08vEV">https://t.co/zwF3j08vEV</a> <a href="https://t.co/ps21An7B9B">pic.twitter.com/ps21An7B9B</a></p>&mdash; Alex Ellis (@alexellisuk) <a href="https://twitter.com/alexellisuk/status/1583089248084729856?ref_src=twsrc%5Etfw">October 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><strong>They limit maximum concurrency</strong></p>
<p>On the free plan, you can only launch 20 hosted runners at once, this increases as you pay GitHub more money.</p>
<p><strong>Builds on private repos are billed per minute</strong></p>
<p>I think this is a fair arrangement. GitHub donates Azure VMs to open source users or any public repo for that matter, and if you want to build closed-source software, you can do so by renting VMs per hour.</p>
<p>There's a free allowance for free users, then Pro users like myself get a few more build minutes included. However, These are on the standard, 2 Core 7GB RAM machines.</p>
<p>What if you didn't have to pay per minute of build time?</p>
<h3>What's wrong with self-hosted runners?</h3>
<p><strong>It's challenging to get all the packages right as per a hosted runner</strong></p>
<p>I spent several days running and re-running builds to get all the software required on a self-hosted runner for the private repos for <a href="https://www.openfaas.com/pricing/">OpenFaaS Pro</a>. Guess what?</p>
<p>I didn't want to touch that machine again afterwards, and even if I built up a list of apt packages, it'd be wrong in a few weeks. I then had a long period of tweaking the odd missing package and generating random container image names to prevent Docker and KinD from conflicting and causing side-effects.</p>
<p>What if we could get an image that had everything we needed and was always up to date, and we didn't have to maintain that?</p>
<p><strong>Self-hosted runners cause weird bugs due to caching</strong></p>
<p>If your job installs software like apt packages, the first run will be different from the second. The system is mutable, rather than immutable and the first problem I faced was things clashing like container names or KinD cluster names.</p>
<p><strong>You get limited to one job per machine at a time</strong></p>
<p>The default setup is for a self-hosted Actions Runner to only run one job at a time to avoid the issues I mentioned above.</p>
<p>What if you could schedule as many builds as made sense for the amount of RAM and core the host has?</p>
<p><strong>Docker isn't isolated at all</strong></p>
<p>If you install Docker, then the runner can take over that machine since Docker runs at root on the host. If you try user-namespaces, many things break in weird and frustrating aways like Kubernetes.</p>
<p>Container images and caches can cause conflicts between builds.</p>
<p><strong>Kubernetes isn't a safe alternative</strong></p>
<p>Adding a single large machine isn't a good option because of the dirty cache, weird stateful errors you can run into, and side-effects left over on the host.</p>
<p>So what do teams do?</p>
<p>They turn to a controller called <a href="https://github.com/actions/actions-runner-controller">Actions Runtime Controller (ARC)</a>.</p>
<p>ARC is non trivial to set up and requires you to create a GitHub App or PAT (please don't do that), then to provision, monitor, maintain and upgrade a bunch of infrastructure.</p>
<p>This controller starts a number of re-usable (not one-shot) Pods and has them register as a runner for your jobs. Unfortunately, they still need to use Docker or need to run Kubernetes which leads us to two awful options:</p>
<ol>
<li>Sharing a Docker Socket (easy to become root on the host)</li>
<li>Running Docker In Docker (requires a privileged container, root on the host)</li>
</ol>
<p>There is a third option which is to use a non-root container, but that means you can't use <code>sudo</code> in your builds. You've now crippled your CI.</p>
<p>What if you don't need to use Docker build/run, Kaniko or Kubernetes in CI at all? Well ARC may be a good solution for you, until the day you do need to ship a container image.</p>
<h2>3) Can we fix it? Yes we can.</h2>
<p><a href="https://docs.actuated.dev/">Actuated</a> ("cause (a machine or device) to operate.") is a semi-managed solution that we're building at OpenFaaS Ltd.</p>
<p><img src="https://docs.actuated.dev/images/conceptual-high-level.png" alt="A semi-managed solution, where you provide hosts and we do the rest."></p>
<blockquote>
<p>A semi-managed solution, where you provide hosts and we do the rest.</p>
</blockquote>
<p>You provide your own hosts to run jobs, we schedule to them and maintain a VM image with everything you need.</p>
<p>You install our GitHub App, then change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code> or <code>runs-on: actuated-aarch64</code> for ARM.</p>
<p>Then, provision one or more VMs with nested virtualisation enabled on GCP, DigitalOcean or Azure, or a bare-metal host, and <a href="https://docs.actuated.dev/add-agent/">install our agent</a>. That's it.</p>
<p>If you need ARM support for your project, the <a href="https://aws.amazon.com/ec2/instance-types/a1/">a1.metal from AWS</a> is ideal with 16 cores and 32GB RAM, or an <a href="https://amperecomputing.com/processors/ampere-altra/">Ampere Altra</a> machine like the c3.large.arm64 from <a href="https://metal.equinix.com/product/servers/c3-large-arm64/">Equinix Metal</a> with 80 Cores and 256GB RAM if you really need to push things. The 2020 M1 Mac Mini also works well with <a href="https://asahilinux.org/">Asahi Linux</a>, and can be maxed out at 16GB RAM / 8 Cores. <a href="https://twitter.com/alexellisuk/status/1585228202087415808?s=20&#x26;t=kW-cfn44pQTzUsRiMw32kQ">I even tried Frederic's Parca job on my Raspberry Pi</a> and it was 26m30s quicker than a hosted runner!</p>
<p>Whenever a build is triggered by a repo in your organisation, the control plane will schedule a microVM on one of your own servers, then GitHub takes over from there. When the GitHub runner exits, we forcibly delete the VM.</p>
<p>You get:</p>
<ul>
<li>A fresh, isolated VM for every build, no re-use at all</li>
<li>A fast boot time of ~ &#x3C;1-2s</li>
<li>An immutable image, which is updated regularly and built with automation</li>
<li>Docker preinstalled and running at boot-up</li>
<li>Efficient scheduling and packing of builds to your fleet of hosts</li>
</ul>
<p>It's capable of running Docker and Kubernetes (KinD, kubeadm, K3s) with full isolation. You'll find some <a href="https://docs.actuated.dev/">examples in the docs</a>, but anything that works on a hosted runner we expect to work with actuated also.</p>
<p>Here's what it looks like:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2o28iUC-J1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Want the deeply technical information and comparisons? <a href="https://docs.actuated.dev/faq/">Check out the FAQ</a></p>
<p>You may also be interested in a debug experience that we're building for GitHub Actions. It can be used to launch a shell session over SSH with hosted and self-hosted runners: <a href="https://www.youtube.com/watch?v=l9VuQZ4a5pc">Debug GitHub Actions with SSH and launch a cloud shell</a></p>
<h2>Wrapping up</h2>
<p>We're piloting actuated with customers today. If you're interested in faster, more isolated CI without compromising on security, we would like to hear from you.</p>
<p><strong>Register for the pilot</strong></p>
<p>We're looking for customers to participate in our pilot.</p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Register for the pilot ðŸ“</a></p>
<p>Actuated is live in pilot and we've already run thousands of VMs for our customers, but we're only just getting started here.</p>
<p><img src="https://blog.alexellis.io/content/images/2022/11/vm-launch.png" alt="VM launch events over the past several days"></p>
<blockquote>
<p>Pictured: VM launch events over the past several days</p>
</blockquote>
<p>Other links:</p>
<ul>
<li><a href="https://docs.actuated.dev/faq/">Read the FAQ</a></li>
<li><a href="https://www.youtube.com/watch?v=2o28iUC-J1w">Watch a short video demo</a></li>
<li><a href="https://twitter.com/selfactuated">Follow actuated on Twitter</a></li>
</ul>
<p><strong>What about GitLab?</strong></p>
<p>We're focusing on GitHub Actions users for the pilot, but have a prototype for GitLab. If you'd like to know more, reach out using the <a href="https://docs.google.com/forms/d/e/1FAIpQLScA12IGyVFrZtSAp2Oj24OdaSMloqARSwoxx3AZbQbs0wpGww/viewform">Apply for the pilot form</a>.</p>
<p><strong>Just want to play with Firecracker or learn more about microVMs vs legacy VMs and containers?</strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=CYCsa5e2vqg">Watch A cracking time: Exploring Firecracker &#x26; MicroVMs</a></li>
<li><a href="https://github.com/alexellis/firecracker-init-lab">Try my firecracker lab on GitHub - alexellis/firecracker-init-lab</a></li>
</ul>
<h2>What are people saying about actuated?</h2>
<blockquote>
<p>"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff."</p>
<p>Addison van den Hoeven - DevOps Lead, Riskfuel</p>
</blockquote>
<blockquote>
<p>"Actuated looks super cool, interested to see where you take it!"</p>
<p>Guillermo Rauch, CEO Vercel</p>
</blockquote>
<blockquote>
<p>"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here."</p>
<p>Richard Case, Principal Engineer, SUSE</p>
</blockquote>
<blockquote>
<p>"Thank you. I think actuated is amazing."</p>
<p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p>
</blockquote>
<blockquote>
<p>"Nice work, security aspects alone with shared/stale envs on self-hosted runners."</p>
<p>Matt Johnson, Palo Alto Networks</p>
</blockquote>
<blockquote>
<p>"Is there a way to pay github for runners that suck less?"</p>
<p>Darren Shepherd, Acorn Labs</p>
</blockquote>
<blockquote>
<p>"Excited to try out actuated! We use custom actions runners and I think there's something here ðŸ”¥"</p>
<p>Nick Gerace, System Initiative</p>
</blockquote>
<blockquote>
<p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running Github Actions in isolated VMs in seconds (vs minutes)."</p>
<p>Rinat Abdullin, ML &#x26; Innovation at Trustbit</p>
</blockquote>
<blockquote>
<p>This is awesome!" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p>
<p>Frederic Branczyk, Co-founder, Polar Signals</p>
</blockquote>]]></content:encoded>
        </item>
    </channel>
</rss>